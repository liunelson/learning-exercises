{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Barnes-Hut t-Distributed Stochastic Neighbour Embedding (t-SNE)\n","\n"," Reference: [L. van der Maaten, \"Accelerating t-SNE using Tree-Based Algorithms,\" J. Mach. Learn. Res., (2014)](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf).\n","\n"," ## t-SNE\n","\n"," t-SNE is an algorithm that maps data points in a high-dimensional space\n"," ($\\mathcal{D} = \\{\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\}, \\: \\boldsymbol{x}_i \\in \\mathbb{R}^N$)\n"," to a 2-/3-dimensional embedding space ($\\mathcal{E} = \\{\\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n \\}, \\: \\boldsymbol{y}_i \\in \\mathbb{R}^S$)\n"," while preserving as much as possible their local structure.\n","\n"," The affinity in the original high-D space, $p_{j|i}$, is represented by Gaussian joint probabilities:\n","\n"," $\\begin{align} p_{j|i} &= \\frac{ \\mathcal{N}(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j); 0, \\sigma_i^2)}{\\sum\\limits_{k \\neq i} \\mathcal{N}(d(\\boldsymbol{x}_i, \\boldsymbol{x}_k); 0, \\sigma_i^2)} \\: (1 - \\delta_{ij}) \\\\ p_{ij} &= \\frac{p_{i|j} + p_{j|i}}{2 n} \\\\ \\mathcal{N}(z; 0, \\sigma^2) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{z^2}{2 \\sigma^2} \\right) \\end{align}$\n","\n"," where $\\sigma_i^2$ is width of the Gaussian kernel such that\n"," the *perplexity* at $\\boldsymbol{x}_i$, $2^{H(P_i)}$,\n"," is equal to some given value $u$,\n"," where $H(P_i) = -\\sum\\limits_j p_{j|i} \\log_2 p_{j|i}$ is the Shannon entropy.\n","\n"," The affinity in the embedding space, $q_{j|i}$, is taken to follow the normalized Student-t distribution (with degree of freedom $n = 1$):\n","\n"," $\\begin{align} q_{ij} &= \\frac{ \\mathcal{T}(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j); \\nu = 1) }{\\sum\\limits_{k \\neq l} \\mathcal{T}(d(\\boldsymbol{x}_k, \\boldsymbol{x}_l); \\nu = 1)} \\: (1 - \\delta_{ij}) \\\\ \\mathcal{T}(z; \\nu = 1) &= \\frac{1}{\\pi} \\left(1 + z^2 \\right)^{-1} \\end{align}$\n","\n"," The metric or distance function $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ could be the $L^2$ (Euclidean) norm $|| \\boldsymbol{x}_i - \\boldsymbol{x}_j ||_2$.\n","\n"," The cost function $\\mathcal{C}(\\mathcal{E})$ is the Kullback-Leibler divergence between the two distributions:\n","\n"," $\\begin{align}  \\mathcal{C}(\\mathcal{E}) = D_\\textrm{KL}(P \\; || \\; Q) = \\sum\\limits_{i, j \\neq i} p_{ij} \\: \\log_2 \\left( \\frac{p_{ij}}{q_{ij}} \\right) \\end{align} $\n","\n"," Although this function is non-convex, it can be minimized as usual by the gradient descent algorithm:\n","\n"," $\\begin{align} \\frac{\\partial \\mathcal{C}}{\\partial \\boldsymbol{y}_i} &= 4 \\sum\\limits_{i, j \\neq i} \\left( p_{ij} - q_{ij} \\right) q_{ij} \\; A \\left(\\boldsymbol{y}_i - \\boldsymbol{y}_j \\right) \\\\ \\boldsymbol{y}_i^{(m + 1)} &= \\boldsymbol{y}_i^{(m)} - \\eta \\frac{\\partial \\mathcal{C}}{\\partial \\boldsymbol{y}_i} \\end{align}$\n","\n"," where $A = \\sum\\limits_{k \\neq l} \\mathcal{T}(d(\\boldsymbol{x}_k, \\boldsymbol{x}_l); \\nu = 1)$ is a normalization term.\n","\n"," The input affinities $p_{ij}$ can be approximated by noting its sparsity and restricting the denominator summation to $\\mathcal{N}_i$,\n"," the set of $\\lfloor 3 u \\rfloor$ nearest neighbours to $\\boldsymbol{x}_i$.\n","\n"," $p_{ij} = \\begin{cases} \\frac{ \\mathcal{N}(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j); 0, \\sigma_i^2)}{\\sum\\limits_{k \\in \\mathcal{N}_i} \\mathcal{N}(d(\\boldsymbol{x}_i, \\boldsymbol{x}_k); 0, \\sigma_i^2)} & \\textrm{if } j \\in \\mathcal{N}_i \\\\ 0 & \\textrm{otherwise} \\end{cases}$\n","\n"," $\\mathcal{N}_i$ can be found by:\n","\n"," 1. building a *vantage-point tree* from $\\mathcal{D}$ and $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$;\n"," 2. performing a nearest-neighbour search.\n","\n","\n","\n"," ## Barnes-Hut Algorithm\n","\n"," Note that the gradient of the cost function can be interpreted as the total force in a classical $n$-body problem:\n","\n"," $\\begin{align} \\frac{\\partial \\mathcal{C}}{\\partial \\boldsymbol{y}_i} &= \\left( 4 \\sum\\limits_{i, j \\neq i} p_{ij} q_{ij} \\; A \\left(\\boldsymbol{y}_i - \\boldsymbol{y}_j \\right) \\right) + \\left( - 4 \\sum\\limits_{i, j \\neq i} q_{ij}^2 \\; A \\left(\\boldsymbol{y}_i - \\boldsymbol{y}_j \\right) \\right) \\\\ &= F_\\textrm{att} + F_\\textrm{rep} \\end{align}$\n","\n"," where $F_\\textrm{att}, F_\\textrm{rep}$ are spring-like attractive and repulsive forces.\n","\n"," The term $q_{ij} \\; A = \\left(1 + ||\\boldsymbol{y}_i - \\boldsymbol{y}_j||^2 \\right)^{-1}$ can be computed $\\mathcal{O}(1)$.\n","\n"," The term $F_\\textrm{att}$ can be computed efficiently by considering summing over only the non-zero elements\n"," of the sparse distribution $P$ that was constructed using $\\mathcal{N}_i$.\n","\n"," Minimization of this quantity can be achieved using the Barnes-Hut algorithm:\n","\n"," 1. construct an *octree* from the embedding $\\mathcal{E} = \\{ \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n \\}$;\n"," 2. traverse the octree using a depth-first search;\n"," 3. at each node, decide if the corresponding cell could be taken altogether as the contribution of all its children to $F_\\textrm{att}, F_\\textrm{rep}$;\n","    - $\\frac{r_\\textrm{cell}}{|| \\boldsymbol{y}_i - \\boldsymbol{y}_\\textrm{cell} ||} < \\theta$ for some given $\\theta \\sim 0.5$;\n","    - if true, $F_\\textrm{rep} \\; A \\approx - 4 N_\\textrm{cell} \\; q_{i, \\textrm{cell}}^2 \\; A^2 (\\boldsymbol{y}_i - \\boldsymbol{y}_\\textrm{cell})$;\n","    - $r_\\textrm{cell}$, side length of the cell; $N_\\textrm{cell}$, the number of points $\\boldsymbol{y}_i$ in the cell;\n","    - $\\boldsymbol{y}_\\textrm{cell}$, centre of mass of the cell; $q_{i, \\textrm{cell}} \\; A = \\left(1 + ||\\boldsymbol{y}_i - \\boldsymbol{y}_\\textrm{cell}||^2 \\right)^{-1}$\n","    - estimate $A = \\sum\\limits_{i \\neq j} \\left(1 + ||\\boldsymbol{y}_i - \\boldsymbol{y}_j||^2 \\right)^{-1}$ in the same way;\n","    - thus $F_\\textrm{rep} = \\frac{F_\\textrm{rep} \\; A}{A}$."],"metadata":{}},{"source":["# Let's start with building a quadtree/octree.\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}