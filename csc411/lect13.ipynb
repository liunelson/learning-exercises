{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes from [CSC 411](http://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 13 - Probabilistic Models (Part I)\n","\n"," ### Maximum Likelihood (ML) Estimation\n","\n"," Likelihood function $L(\\theta)$\n"," is the probability of the observed data,\n"," as a function of some model parameter $\\theta$.\n","\n"," Log-likelihood function: $\\ell(\\theta) = \\log L(\\theta)$\n","\n"," Example:\n"," - consider a observed sequence of $N$ temperature values $\\{x_n\\}$\n"," that behave as a random variable $X$;\n"," - $X$ follows a Gaussian distribution $\\mathcal{N}(x; \\mu,\\sigma^2)$\n"," - $\\mu, \\sigma^2$ are unknown;\n"," - the log-likelihood is\n"," $\\ell(\\mu, \\sigma) = \\log \\left( \\prod\\limits_{n=1}^N \\mathcal{N}(x_n; \\mu, \\sigma) \\right)$;\n"," - then $\\left. \\frac{\\partial \\ell}{\\mathrm{\\partial}\\mu} \\right|_\\hat{\\mu} = 0 \\: \\Rightarrow \\hat{\\mu} = \\bar{x}$\n"," - and $\\left. \\frac{\\partial \\ell}{\\mathrm{\\partial}\\sigma} \\right|_\\hat{\\sigma} = 0 \\: \\Rightarrow \\hat{\\sigma} = \\frac{1}{N} \\sum\\limits_{n=1}^N (x_n - \\mu)^2$\n","\n","\n"," If there is no closed-form solution associated with some complicated PDF,\n"," then use gradient descent.\n","\n"," Note that we've been doing ML estimation all along:\n"," - e.g. linear regression with squared-error loss,\n"," $\\begin{align} p(t|y) &= \\mathcal{N}(t;y,\\sigma^2) \\\\ -\\log\\left(p(t|y)\\right) &= \\frac{1}{2 \\sigma^2}(y-t)^2 + \\textrm{const} \\end{align}$\n"," - e.g. logistic regression with cross-entropy loss,\n"," $\\begin{align} p(t|y) &= y \\\\ -\\log\\left(p(t|y)\\right) &= - \\left(t \\log(y) + (1-t)\\log(1-y)\\right) \\end{align}$\n","\n","\n"," Two approaches to classification:\n"," 1. *discriminative* classifiers estimate parameters of decision boundary\n"," directly from labeled examples;\n","    - learn $p(y|\\mathbf{x})$ directly (logistic regression models);\n","    - learn mappings fro inputs to classes (decision trees);\n"," 2. *generative* (*Bayes*) classifiers model the distribution characteristic of the class;\n","    - build a model of $p(\\mathbf{x}|y)$;\n","    - apply Bayes' Rule.\n","\n","\n"," ### Naive Bayes\n","\n"," Bayes classifer:\n"," - aim: classify emails into either `spam` ($c = 1$) or `not-spam` ($c = 0$);\n"," - using *bag-of-words* features, get binary vector $\\mathbf{x} = [x_1 \\; \\ldots \\; x_K]^\\mathsf{T}$ for each email;\n"," - compute class probabilities using Bayes' Rule,\n"," $p(c|\\mathbf{x}) = \\frac{p(\\mathbf{x}|c) p(c)}{p(\\mathbf{x})}$;\n"," - i.e. $\\textrm{posterior} = \\frac{\\textrm{class likelihood} \\times \\textrm{prior}}{\\textrm{evidence} }$;\n"," - compute $p(\\mathbf{x}) = p(\\mathbf{x}|c = 0) p(c = 0) + p(\\mathbf{x}|c = 1) p(c = 1)$;\n"," - to known $p(\\mathbf{x}|c)$ and $p(c)$,\n"," requires defining a joint distribution $p(c,x_1, \\ldots,x_K)$\n"," - but this requires $2^{K+1}$ entries!\n"," - impose a structure as constraints.\n","\n","\n"," *Naive Bayes* assumes that the word features $x_k$\n"," are *conditionally independent* given the class $c$;\n"," thus, $p(c,x_1,\\ldots,x_K) = p(c) p(x_1|c) \\ldots p(x_K|c)$.\n","\n"," Compact representation of the joint distribution:\n"," - prior probability of class, $p(c = 1) = \\theta_C$;\n"," - conditional probability of word feature given class, $p(x_k = 1|c) = \\theta_{k,c}$\n"," - $2K + 1$ parameters total (instead of $2^{K+1}$).\n","\n","\n"," This model can be represented as an *directed graph* (*Bayesian network*):\n"," $c$ is the root node and $x_k$ are the leaf nodes.\n","\n"," The parameters can be learned efficiently since the log-likelihood\n"," decomposes into independent terms:\n","\n"," $\\begin{align} \\ell(\\boldsymbol{\\theta}) &= \\log \\left(\\prod\\limits_{n=1}^N p(c^{(n)}, \\mathbf{x}^{(n)}) \\right) \\\\ &= \\ldots \\\\ &= \\sum\\limits_{n=1}^N \\log \\left( p(c^{(n)}) \\right) + \\sum\\limits_{k=1}^K \\sum\\limits_{n=1}^N \\log \\left( p(x_k^{(n)}|c^{(n)})) \\right) \\end{align}$\n","\n"," Maximize $\\sum_n \\log \\left(p(x_k^{(n)} | c^{(n)}) \\right)$:\n"," - let $\\theta_{a,b} = p(x_k = a|c = b)$; $\\theta_{1,b} = 1 - \\theta_{0,b}$;\n"," - then\n"," $\\log \\left( p(x_k^{(n)}|c^{(n)})) \\right) = c^{(n)} x_k^{(n)} \\log(\\theta_{1,1}) + c^{(n)} (1 - x_k^{(n)}) \\log(1 - \\theta_{1,1}) + (1 - c^{(n)}) x_k^{(n)} \\log(\\theta_{1,0}) + (1 - c^{(n)}) (1 - x_k^{(n)}) \\log(1 - \\theta_{1,0})$\n"," - obtain the ML estimates by setting the derivatives to zero;\n"," - thus $\\theta_{1,1} = \\frac{N_{1,1}}{N_{1,1} + N_{0,1}}$ and $\\theta_{1,0} = \\frac{N_{1,0}}{N_{1,0} + N_{0,0}}$;\n","\n","\n"," Predict the class by performing an *inference* using Bayes' Rule:\n"," $p(c|\\mathbf{x}) = \\frac{p(c) p(\\mathbf{x}|c)}{\\sum_{c^\\prime} p(c^\\prime) p(\\mathbf{x}|c^\\prime)} \\propto p(c) \\prod_k p(x_k|c)$\n","\n"," Naive Bayes is a very inexpensive algorithm:\n"," - training time:\n","    1. compute co-occurence counts of each feature with labels;\n","    2. one pass only to do ML estimation of parameters;\n"," - test time: apply Bayes' Rule;\n"," - not very accurate though due to *naive* assumption."],"metadata":{}},{"source":["  \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ## Lecture 14 - Probabilistic Models (Part II)\n","\n"," Problem with ML estimation: what if too little data?\n","\n"," Example: count coin flips but too few to even get a single `tail` event.\n","\n"," Such *data sparsity* causes $\\ell(\\theta)$ to be $-\\infty$.\n","\n"," In ML estimation, the observations are treated random variables but\n"," the parameters are not.\n","\n"," In *Bayesian parameter estimation*, the latter are random variables too:\n"," - prior distribution $p(\\boldsymbol{theta})$ encodes beliefs about parameters before observation;\n"," - likelihood is as before.\n"," - we update our beliefs about the parameters by computing the posterior distribution.\n","\n","\n"," Recall the Bernoulli-type coin flip example;\n"," - the likelihood is $L(\\theta) = p(\\mathcal{D}) = \\theta^{N_H} (1 - \\theta)^{N_T}$;\n"," - specify the prior $p(\\theta)$;\n"," - choose an *uninformative prior*:\n","    1. could be an uniform distribution;\n","    2. assuming 50% is more probable, we choose a *beta distribution*\n","       $p(\\theta;a,b) = \\mathrm{Beta}(\\theta;a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\theta^{a-1} (1 - \\theta)^{b-1}$\n"," - compute the posterior distribution,\n","   $\\begin{align} p(\\theta|\\mathcal{D}) & \\propto p(\\theta) p(\\mathcal{D}|\\theta) \\\\ & \\propto \\theta^{a - 1 + N_H} (1 - \\theta)^{b - 1 + N_T} \\\\ &= \\mathrm{Beta}(\\theta; N_H + a; N_T + b)\\end{align}$\n"," - convenient that the posterior is just another Beta distribution;\n"," - thus, $\\mathrm{E}[\\theta|\\mathcal{D}] = \\frac{N_H + a}{N_H + N_T + a + b}$;\n"," - $a, b$ are known as *pseudo-counts* as a result.\n","\n","\n"," As $N_H, N_T$ become large, the data/likelihood overwhelms the prior.\n","\n"," The posterior itself is used to compute the *posterior predictive distribution*:\n","\n"," $p(\\mathcal{D}^\\prime|\\mathcal{D}) = \\int p(\\theta|\\mathcal{D}) p(\\mathcal{D}^\\prime|\\theta) \\mathrm{d}\\theta$.\n","\n"," In the above example:\n"," $\\begin{align} p(x^\\prime = \\mathrm{H}|\\mathcal{D}) = \\int p(\\theta|\\mathcal{D}) p(x^\\prime = \\mathrm{H}|\\theta) \\mathrm{d}\\theta \\\\ &= \\int \\mathrm{Beta}(\\theta; N_H + a, N_T + b) \\theta \\mathrm{d}\\theta \\\\ &= \\ldots \\\\ &= \\frac{N_H + a}{N_H + N_T + a + b} \\end{align}$.\n","\n"," Comparing ML estimation with Bayesian estimation:\n"," - Bayesian estimation can handle data sparsity;\n"," - former is an optimization problem\n"," while later is an integration problem;\n"," - former is easier due to gradient descent;\n","\n","\n"," ### Maximum A-Posteriori (MAP) Estimation\n","\n"," MAP estimation: find the most likely parameter settings under the posterior.\n","\n"," This converts the Bayesian parameter estimation problem into an optimization one:\n","\n"," $\\begin{align} \\hat{\\theta}_\\textrm{MAP} &= \\arg \\max\\limits_\\theta p(\\theta|\\mathcal{D}) \\\\ &= \\arg \\max\\limits_\\theta p(\\theta,\\mathcal{D}) \\\\ &= \\arg \\max\\limits_\\theta \\log p(\\theta) + \\log p(\\mathcal{D}|\\theta) \\end{align}$\n","\n"," In the coin-flip example:\n"," - the joint probability is\n"," $\\log p(\\theta, \\mathcal{D}) = \\log p(\\theta) + \\log p(\\mathcal{D}|\\theta) = \\textrm{const} + \\log \\mathrm{Beta}(\\theta; N_H+a, N_T+b)$\n"," - maximize by finding critical point,\n"," $\\frac{\\mathrm{d}}{\\mathrm{d} \\theta} \\log(\\theta, \\mathcal{D}) = 0$;\n"," - solving for $\\theta$, $\\hat{\\theta}_\\textrm{MAP} = \\frac{N_H + a - 1}{N_H + N_t + a + b - 2}$.\n","\n","\n"," | | Formula | $(N_H = 2, N_T = 0)$ |  $(N_H = 55, N_T = 45)$ |\n"," |:---:|:---:|:---:|:---:|\n"," |$\\hat{\\theta}_\\textrm{ML}$| $\\frac{N_H}{N_H + N_T}$ | 1 | 0.55 |\n"," |$\\hat{\\theta}_\\textrm{BE}$| $\\frac{N_H + a}{N_H + N_T + a + b}$ | 0.67 | 0.548 |\n"," |$\\hat{\\theta}_\\textrm{MAP}$| $\\frac{N_H + a - 1}{N_H + N_T + a + b - 2}$ | 0.75 | 0.549 |"],"metadata":{}},{"cell_type":"markdown","source":[" ### Gaussian Discriminant Analysis (GDA)\n","\n"," In a generative model, we don't try to separate the classes; try to model\n"," we try to model the class distribution $p(\\mathbf{x}|t = k)$\n"," (which could be very complex).\n","\n"," Recall Bayes classifier:\n"," $h(\\mathbf{x}) = \\arg \\max p(t = k|\\mathbf{x}) = \\arg \\max \\frac{p(\\mathbf{x}|t = k) p(t = k)}{p(\\mathbf{x})}$.\n","\n"," Consider a continuous $\\mathbf{x}$\n"," and model $p(\\mathbf{x}|t = k)$ as a multivariate Gaussian distribution.\n","\n"," For $\\mathbf{x} \\in \\mathbb{R}^d$,\n"," *Gaussian discriminant analysis* (or *Gaussian Bayes classifier* (GBC)) assumes:\n","\n"," $p(\\mathbf{x}|t = k) = \\frac{1}{(2 \\pi)^{d/2} \\sqrt{|\\Sigma_k|}} \\exp \\left( -\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_k)^\\mathsf{T} \\Sigma_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k) \\right)$\n","\n"," Each class $k$ has an associated mean vector $\\mathbf{\\mu}_k$\n"," and covariance matrix $\\Sigma_k$ (with $\\mathcal{O}(d^2)$ parameters).\n","\n"," Example:\n"," $\\mathbf{X}$ is a $N \\times d$ matrix,\n"," $N$ observations/instances/examples of\n"," some value with $d$ inputs/features/attributes.\n","\n"," $\\Sigma = \\mathrm{cov}[\\mathbf{x}] = \\mathrm{E}[(\\mathbf{x} - \\mathbf{\\mu})^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\mu})]$\n","\n"," Note that $(\\mathbf{x} - \\mathbf{\\mu}_k)^\\mathsf{T} \\Sigma_k^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)$\n"," is the *Mahalanobis distance*\n"," (a measure of the distance from $\\mathbf{x} to $\\mathbf{\\mu}_k$ in terms of $\\Sigma_k$)\n","\n"," The log of the class posterior is then:\n"," $\\begin{align} \\log p(t_k|\\mathbf{x}) &= \\log p(\\mathbf{x}|t_k) + \\log p(t_k) - \\log p(\\mathbf{x}) \\\\ &= \\end{align}$\n","\n"," Not very good if class-conditional data is not multivariate Gaussian.\n",""],"metadata":{}},{"source":["  "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ## Lecture 15 - k-Means\n","\n"," Use unsupervised learning when\n"," one wants to infer the *latent* (unobserved) causal structure\n"," underlying the data.\n","\n"," Consider the *k-means clustering* algorithm and then\n"," reformulate it as a *latent variable model*\n"," and apply the *expectation-maximization* (EM) algorithm.\n","\n"," Clusters:\n"," groups of data points/examples which are similar within\n"," but dissimilar without, i.e. *multimodal* distribution.\n","\n"," Clustering: group unlabelled data points into clusters\n"," (NP-hard problem).\n","\n"," Assume:\n"," - $N$ data points $\\mathbf{x}_n \\in \\mathcal{R}^d$;\n"," - $\\mathbf{x}_n$ belongs to $K$ classes;\n"," - similarity measure $\\rightarrow$ Euclidean distance;\n","\n","\n"," Objective:\n"," - Find clusters centres $\\mathbf{m}$ and assignments $\\mathbf{r}$\n"," to minimize the sum of squared distances of data points $\\{\\mathbf{x}^{(n)}\\}$\n"," to their assigned clusters centres;\n"," - $\\min\\limits_{\\{\\mathbf{m}\\}, \\{\\mathbf{r}\\}} J(\\{\\mathbf{m}\\}, \\{\\mathbf{r}\\})$;\n"," - $J(\\{\\mathbf{m}\\}, \\{\\mathbf{r}\\}) = \\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^K r_k^{(n)} ||\\mathbf{m}_k - \\mathbf{x}^{(n)} ||^2$;\n"," - such that $\\sum\\limits_{k=1}^K r_k^{(n)} = 1 \\: \\forall \\: n$;\n"," - where $r_k^{(n)} \\in \\{0, 1\\} \\: \\forall \\: k,n$;\n"," - where $r_k^{(n)} = 1$ means that $\\mathbf{x}^{(n)}$ is assigned to cluster $k$.\n","\n","\n"," Optimization method:\n"," - *block/coordinate descent*;\n"," - successively minimize along different coordinates ($\\{\\mathbf{m}\\}$ vs $\\{\\mathbf{r}\\}$)\n","\n","\n"," Algorithm:\n","\n"," 1. initialization: set $K$ cluster centres $\\mathbf{m}_k$ to random values;\n"," 2. assignment: assign each of $N$ data points $\\mathbf{x}^{(n)}$ to nearest cluster\n","    - $\\hat{k}^{(n)} = \\arg \\min\\limits_k d(\\mathbf{m}_k, \\mathbf{x}^{(n)})$\n","    - e.g. $L^2$ distance\n","    - define *responsibilities* (1-hot encoding), $r_k^{(n)} = 1 \\leftrightarrow \\hat{k}^{(n)} = k$\n"," 3. refitting: adjust model parameter (cluster centres) to match sample mean of data points\n","    - $\\mathbf{m}_k = \\frac{\\sum_n r_k^{(n)} \\mathbf{x}^{(n)}}{\\sum_n r_k^{(n)}}$\n","\n","\n"," Example of use:\n"," - vector quantization ($\\mathbf{x} = [l, a, b]$);\n"," - image segmentation ($\\mathbf{x} = [x, y, l, a, b]$);\n","\n","\n"," Test for convergence:\n"," - check when objective function $J$ reaches minimum;\n"," - since $J$ is non-convex, no guarantee of convergence to global minimum;\n"," - solution: many random starting $\\mathbf{m}_k$\n"," or split-merge (split large clusters, merge nearby ones).\n","\n","\n"," Soft k-means:\n"," - allow *soft* (non-integer) responsibilities;\n"," - $r_k^{(n)} = \\sigma \\left(\\beta \\; d(\\mathbf{m}_{k^\\prime}, \\mathbf{x}^{(n)}) \\right) = \\frac{\\exp \\left(\\beta \\; d(\\mathbf{m}_k, \\mathbf{x}^{(n)}) \\right)}{\\sum_{k^\\prime}\\exp \\left(\\beta \\; d(\\mathbf{m}_{k^\\prime}, \\mathbf{x}^{(n)}) \\right)}$;\n"," - weighted refit, $\\mathbf{m}_k = \\frac{\\sum\\limits_n r_k^{(n)} \\mathbf{x}^{(n)}}{\\sum\\limits_n r_k^{(n)}}$\n","\n","\n"," Problems:\n"," - How to set $\\beta$?\n"," - Elongated clusters?\n"," - Clusters of different weight and size?\n",""],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}