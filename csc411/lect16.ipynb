{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Notes from [CSC 411](http://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/)\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lecture 16 - Expectation-Maximization (Part I-II)\n",
    "\n",
    " Reformulate clustering in terms of a *generative* model\n",
    " by focusing on deriving the probability distribution\n",
    " that could generate the observed data\n",
    " (instead of focusing on decision boundaries).\n",
    "\n",
    "\n",
    " Define the joint distribution as\n",
    " $p(\\mathbf{x}, z) = p(\\mathbf{x}| z) p(z)$\n",
    " where $z$ are the class labels.\n",
    "\n",
    " Since $z$ is a priori unknown in unsupervised learning, we write\n",
    " $p(\\mathbf{x}) = \\sum\\limits_z p(\\mathbf{x}, z) = \\sum\\limits_z p(\\mathbf{x}| z) p(z)$.\n",
    "\n",
    " This is a *mixture model*.\n",
    "\n",
    " Example: *Gaussian mixture model* (GMM)\n",
    " - a GMM represent a distribution as $p(\\mathbf{x}) = \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}| \\mu_k, \\Sigma_k)$;\n",
    " - $\\pi_k$ are *mixing coefficients* such that $\\sum_k \\pi_k = 1$ and $\\pi_k \\geq 0 \\: \\forall \\: k$;\n",
    " - GMM is a density estimator;\n",
    " - GMMS are *universal approximators of densities* (with enough Gaussians);\n",
    "\n",
    "\n",
    " Fitting GMMs by maximum likelihood algorithm:\n",
    " - define the log-likelihood $\\ell(\\boldsymbol{\\pi}, \\mu, \\Sigma) = \\ln p(\\mathbf(X)|\\mu,\\mu,\\Sigma) = \\sum\\limits_{n=1}^N \\ln \\left( \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}^{(n)}|\\mu_k,\\Sigma_k ) \\right)$\n",
    " - maximize it with respect to $\\Theta = {\\pi_k, \\mu_k, \\Sigma_k}$;\n",
    "\n",
    "\n",
    " From above, $z$ is a hidden/latent variable:\n",
    " - hidden/latent variables are model variables which are always unobserved;\n",
    " - let $z \\sim \\textrm{Categorial}(\\boldsymbol{\\pi})$;\n",
    " - then $\\begin{align} p(\\mathbf{x}) &= \\sum\\limits_{k=1}^K p(\\mathbf{x}, z = k) \\\\ &= \\sum\\limits_{k=1}^K p(z = k) p(\\mathbf{x}|z = k) \\end{align}$\n",
    " - where $p(z = k) = \\pi_k$ and $p(\\mathbf{x}|z=k) = \\mathcal(N)(\\mathbf{x}|\\mu_k,\\Sigma_k)$.\n",
    "\n",
    "\n",
    " If we knew the $z^{(n)}$ associated with each $\\mathbf{x}^{(n)}$,\n",
    " this ML problem could be solved easily:\n",
    " - $\\ell(\\boldsymbol{\\pi}, \\mu, \\Sigma) = \\sum\\limits_{n=1}^N \\left( \\ln p(\\mathbf{x}^{(n)}|z^{(n)}; \\mu, \\Sigma) + \\ln p(z^{(n)}|\\boldsymbol{\\pi}) \\right)$\n",
    " - recall Gaussian Bayes classifiers;\n",
    " - solution: $\\begin{align} \\mu_k &= \\frac{\\sum\\limits_{n=1}^N 1_{z^{(n)} = k} \\; \\mathbf{x}^{(n)}}{\\sum\\limits_{n=1}^N 1_{z^{(n)} = k}} \\\\ \\mu_k &= \\frac{\\sum\\limits_{n=1}^N 1_{z^{(n)} = k} \\; (\\mathbf{x}^{(n)} - \\mu_k) (\\mathbf{x}^{(n)} - \\mu_k)^\\mathsf{T} }{\\sum\\limits_{n=1}^N 1_{z^{(n)} = k}} \\\\ \\pi_k &= \\frac{1}{N} \\sum\\limits_{n=1}^N 1_{z^{(n)} = k} \\end{align}$\n",
    "\n",
    "\n",
    " Using the *expectation-maximization* algorithm:\n",
    " - *E-step*: compute the posterior probability over $z$ given current model (contribution of each Gaussian to each data point);\n",
    " - *M-step*: change the parameters of each Gaussian to maximize the probability that it generates its assigned data points.\n",
    "\n",
    "\n",
    " Recall the k-means clustering algorithm.\n",
    " - the assignment step $\\leftrightarrow$ the E-step;\n",
    " - the refitting step $\\leftrightarrow$ the M-step.\n",
    "\n",
    "\n",
    " The EM algorithm is a powerful method for finding ML solutions for such latent-variable models.\n",
    "\n",
    " Derivation:\n",
    " - write the log-likelihood function $\\ell(\\mathbf{X},\\Theta) = \\sum\\limits_n \\ln (P(\\mathbf{x}^{(n)});\\Theta) = \\sum\\limits_n \\ln \\left( \\sum\\limits_k P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta) \\right)$;\n",
    " - introduce a new distribution $q$, $\\ell(\\mathbf{X},\\Theta) = \\sum\\limits_n \\ln \\left( \\sum\\limits_k q_k \\frac{P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta)}{q_k} \\right)$;\n",
    " - from Jensen's inequality for concave functions like $\\ln$,\n",
    " $f(\\mathrm{E}[x]) = f\\left(\\sum\\limits_i p_i x_i \\right) \\geq \\sum\\limits_i p_i f(x_i) = \\mathrm{E}[f(x)]$;\n",
    " - then $\\sum\\limits_n \\ln \\left( \\sum\\limits_k q_k \\frac{P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta)}{q_k} \\right) = \\sum\\limits_n \\sum\\limits_k q_k \\ln \\left( \\frac{P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta)}{q_k} \\right) $;\n",
    " - this moved the summation outside of the $\\ln$;\n",
    " - maximizing this lower bound to force $\\ell$ to increase;\n",
    " - how to pick $q_k$?\n",
    " - suppose $q_k = p(z^{(n)} = k|x^{(n)},\\Theta^\\textrm{old})$;\n",
    " - then optimize $Q(\\Theta) = \\sum\\limits_n \\sum\\limits_k p(z^{(n)} = k|x^{(n)},\\Theta^\\textrm{old}) \\ln P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta)$;\n",
    " - this is just the expectation over the distribution $P$, $Q(\\Theta) = \\mathrm{E}[\\ln P(\\mathbf{x}^{(n)}), z^{(n)} = k;\\Theta)]$;\n",
    " - maximizing $Q(\\Theta)$ now gives a better lower bound for the log-likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " General EM algorithm:\n",
    "\n",
    " 1. initialize $\\Theta^\\textrm{old}$;\n",
    " 2. E-step, evaluate $p(\\mathbf{Z}|\\mathbf{X},\\Theta^\\textrm{old})$;\n",
    " and compute $Q(\\Theta,\\Theta^\\textrm{old}) = \\sum\\limits_z p(\\mathbf{Z}|\\mathbf{X},\\Theta^\\textrm{old}) \\ln p(\\mathbf{Z}|\\mathbf{X},\\Theta)$;\n",
    " 3. M-step, maximize: $\\Theta^\\textrm{new} = \\arg \\max\\limits_\\Theta Q(\\Theta,\\Theta^\\textrm{old})$;\n",
    " 4. evaluate log-likelihood and check for convergence; if not converged, set $\\Theta^\\textrm{old} = \\Theta^\\textrm{new}$ and repeat step 2.\n",
    "\n",
    "\n",
    " As applied to GMM:\n",
    " - initialize the parameters $\\mu_k, \\Sigma_k, \\pi_k$ (with k-means);\n",
    " - conditional probability of $\\mathbf{z}$ given $\\mathbf{x}$ by Bayes' Rule:\n",
    " $\\begin{align} \\gamma_k &= p(z = k|\\mathbf{x}) \\\\ &= \\frac{p(z = k) p(\\mathbf{x}|z = k)}{p(\\mathbf{x})} \\\\ &= \\frac{p(z = k) p(\\mathbf{x}|z = k)}{\\sum\\limits_{j=1}^K p(z = j) p(\\mathbf{x}| z = j)} \\\\ &= \\frac{\\pi_k \\; \\mathcal{N}(\\mathbf{x}|\\mu_k,\\Sigma_k)}{\\sum\\limits_{j=1}^K \\pi_j \\; \\mathcal{N}(\\mathbf{x}|\\mu_j, \\Sigma_j)} \\end{align}$\n",
    " - $\\gamma_k$ is like the responsibility of cluster $k$ towards $\\mathbf{x}$;\n",
    " - compute the expected log-likelihood:\n",
    " $\\mathrm{E}_{P(z^{(n)}|\\mathbf{x}^{(n)})} \\left[ \\sum\\limits_{n=1}^N \\ln P(\\mathbf{x}^{(n)}, z^{(n)} | \\Theta) \\right] = \\ldots = \\sum\\limits_{k=1}^K \\sum\\limits_{n=1}^N \\gamma_k^{(n)} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}^{(n)}; \\mu_k, \\Sigma_k) \\right) $;\n",
    " - optimize these $k$ Gaussians with weights $\\gamma_k^{(n)}$;\n",
    " - re-estimated parameters:\n",
    " $\\begin{align} \\mu_k &= \\frac{1}{N_k} \\sum\\limits_{n=1}^N \\gamma_k^{(n)} \\mathbf{x}^{(n)} \\\\ \\Sigma_k &= \\frac{1}{N_k} \\sum\\limits_{n=1}^N \\gamma_k^{(n)} (\\mathbf{x}^{(n)} - \\mu_k) (\\mathbf{x}^{(n)} - \\mu_k)^\\mathsf{T} \\\\ \\pi_k &= \\frac{N_k}{N} \\\\ N_k &= \\sum\\limits_{n=1}^N \\gamma_k^{(n)}\\end{align}$\n",
    " - evaluate the log-likelihood:\n",
    " $\\ell(\\mathbf{X}, \\Theta) = \\ln p(\\mathbf{X}|\\pi,\\mu,\\Sigma) = \\sum\\limits_{n=1}^N \\ln \\left( \\sum\\limits_{k=1}^K \\pi_k \\; \\mathcal{N}(\\mathbf{x^{(n)}| \\mu_k, \\Sigma_k }) \\right)$.\n",
    "\n",
    "\n",
    " EM + Gaussian mixture model is just like soft k-means clustering\n",
    " with fixed priors and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lecture 18 - Matrix Factorization\n",
    "\n",
    " Recall PCA:\n",
    " - it is a matrix factorization problem;\n",
    " - can be extended to matrix completion (data matrix not fully observed);\n",
    " - each $N$ $D \\times 1$ input vector $\\mathbf{x}^{(i)}$ is approximated as\n",
    " $\\mathbf{U} \\mathbf{z}$;\n",
    " - $\\mathbf{U}$ is $D \\times K$ orthogonal basis matrix and $z$ is the $K \\times N$ code vector;\n",
    " - square error: $|| \\mathbf{X} - \\mathbf{U}\\mathbf{Z}||_\\textrm{F}^2$;\n",
    " - Frobenius norm, $||\\mathbf{A}||_\\textrm{F}^2 = \\sum_{i,j} a_{i,j}^2$;\n",
    " - result: $\\mathbf{X} \\approx \\mathbf{U} \\mathbf{z}$;\n",
    " - i.e. optimal low-rank matrix factorization.\n",
    "\n",
    "\n",
    " *Singular value decomposition* (SVD):\n",
    " - $\\mathbf{X} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\mathsf{T}$;\n",
    " - $\\mathbf{U}, \\mathbf{V}$ are unitary matrices (orthonormal columns);\n",
    " - $\\mathbf{S} = \\mathrm{diag}(s_i)$;\n",
    " - the first $k$ SVD vectors correspond to the first $k$ PCA components ($\\mathbf{Z} = \\mathbf{S} \\mathbf{V}^\\mathsf{T}$);\n",
    "\n",
    "\n",
    " Suppose $\\mathbf{X}$ is only partially observed;\n",
    " how to fill in blanks?\n",
    " Application: recommender systems like Youtube and Netflix.\n",
    "\n",
    " Algorithms:\n",
    " - alternating least square (ALS) method;\n",
    " - gradient descent;\n",
    " - non-negative matrix factorization;\n",
    " - low-rank matrix completion;\n",
    " - etc.\n",
    "\n",
    "\n",
    " Alternating least square (ALS) method:\n",
    " - assume $\\mathbf{X}$ is low rank;\n",
    " - using squared-error loss;\n",
    " - do $\\min\\limits_{\\mathbf{U},\\mathbf{Z}} \\frac{1}{2} \\sum\\limits_{x_{i,j} \\textrm{observed}} (x_{i,j} - \\mathbf{u}_i^\\mathsf{T} \\mathbf{z}_j)^2$;\n",
    " - this objective function is non-convex and this problem is NP-hard;\n",
    " - it is convex though as a function of $\\mathbf{U}$ or $\\mathbf{Z}$ individually;\n",
    " - solution: fix $\\mathbf{U}$ and optimize $\\mathbf{Z}$, vice versa and repeat until convergence$;\n",
    " - algorithm:\n",
    "    1. initialize $\\mathbf{U}, \\mathbf{V}$ randomly;\n",
    "    2. for every $i = 1, \\ldots, D$,\n",
    "       do $\\mathbf{u}_i = \\left( \\sum\\limits_{j \\; : \\; x_{ij} \\neq 0} z_{j} \\mathbf{z}_j^\\mathsf{T} \\right)^{-1} \\sum\\limits_{j \\; : \\; x_{ij} \\neq 0} x_{ij} \\mathbf{z}_j $\n",
    "    3. for every $j = 1, \\ldots, N$,\n",
    "       do $\\mathbf{z}_i = \\left( \\sum\\limits_{i \\; : \\; x_{ij} \\neq 0} u_{i} \\mathbf{u}_i^\\mathsf{T} \\right)^{-1} \\sum\\limits_{j \\; : \\; x_{ij} \\neq 0} x_{ij} \\mathbf{u}_i $\n",
    "    4. repeat until convergence.\n",
    "\n",
    "\n",
    " k-means as matrix factorization:\n",
    " - define matrix $\\mathbf{R}$ where its rows are the indicator vectors $\\mathbf{r}$;\n",
    " - define matrix $\\mathbf{M}$ where it rows are the cluster centres $\\mu_k$;\n",
    " - reconstruction of the data is thus $\\mathbf{X} \\approx \\mathbf{R} \\mathbf{M}$;\n",
    " - k-means distortion function in matrix form:\n",
    "   $\\sum\\limits_{n=1}^N \\sum\\limits_{k=1}^K r_k^{(n)} || \\mathbf{m}_k - \\mathbf{x}^{(n)} ||^2 = || \\mathbf{X} - \\mathbf{R} \\mathbf{M}||_\\textrm{F}^2$;\n",
    "\n",
    "\n",
    " *Co-clustering*:\n",
    " - co-clustering clusters both the rows and columns of the data matrix $\\mathbf{X}$;\n",
    " - indicator matrix for rows $\\times$ matrix of means for each block $\\times$ indicator matrix for columns;\n",
    "\n",
    "\n",
    " *Sparse coding*:\n",
    " - represent natural data (images and sounds) $\\mathbf{x}$\n",
    " using a dictionary of basis functions $\\{\\mathbf{a}_k\\}_{k=1}^K$;\n",
    " - $\\mathbf{x} \\approx = \\sum\\limits_{k=1}^K s_k \\mathbf{a}_k = \\mathbf{A} \\mathbf{s}$;\n",
    " - Since only a few basis functions are used, $\\mathbf{s}$ is a sparse vector;\n",
    " - choose $\\mathbf{s}$ with cost function $\\min\\limits_\\mathbf{s} \\left( || \\mathbf{x} - \\mathbf{A}\\mathbf{s}||^2 + \\beta ||\\mathbf{s}||_1 \\right)$;\n",
    " - learn dictionary by optimizing both $\\mathbf{A}$ and $\\{\\mathbf{s}_k\\}_{n=1}^N$\n",
    " - i.e. $\\min\\limits_{\\mathbf{A},\\{\\mathbf{s}_n\\}} \\left(|| \\mathbf{X} - \\mathbf{A}\\mathbf{S}||_\\textrm{F}^2 + \\beta ||\\mathbf{s}_n||_1 \\right)$;\n",
    " - subject to $||\\mathbf{a}_k||^2 \\leq 1 \\: \\forall \\: k$;\n",
    " - fit using ALS;\n",
    " - learned dictionary efficiently tiles all degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lecture 19 - Bayesian Linear Regression\n",
    "\n",
    " Both parametric and non-parametric models for regression and classification\n",
    " have been covered:\n",
    " - parametric, e.g. linear and logistic regression, neural networks, SVM, naive Bayes, GDA;\n",
    " - non-parametric, e.g. k-nearest neighbour (KNN)\n",
    "\n",
    "\n",
    " Next: Bayes linear regression (parametric model).\n",
    "\n",
    " Recall linear regression:\n",
    " - given training set of inputs and targets\n",
    " $\\{(\\mathbf{x}^{(n)}, t^{(n)})\\}_{n=1}^N$;\n",
    " - linear model, $y = \\mathbf{w}^\\mathsf{T} \\boldsymbol{\\psi}(mathbf{x})$;\n",
    " - squared-error loss function,\n",
    " $\\mathcal{L}(y, t) = \\frac{1}{2} (t - y)^2$;\n",
    " - $L^2$ regularization, $\\mathcal{R}(\\mathbf{w}) = \\frac{1}{2} \\lambda || \\mathbf{w}||^2$;\n",
    " - analytical solution (set gradient to $0$): $\\mathbf{w} = ( \\mathbf{\\Psi}^\\mathsf{T} \\mathbf{\\Psi} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Psi}^\\mathsf{T} \\mathbf{t}$;\n",
    " - approximate solution (gradient descent): $\\mathbf{w} \\leftarrow (1 - \\alpha \\lambda) \\mathbf{w} - \\alpha \\mathbf{\\Psi}^\\mathsf{T} (\\mathbf{y} - \\mathbf{t})$.\n",
    "\n",
    "\n",
    " Extensions:\n",
    " - assuming Gaussian noise,\n",
    " we get maximum likelihood method under this model;\n",
    " - i.e. $\\begin{align} \\mathbf{t} | \\mathbf{x} & \\sim \\mathcal{N}(\\mathbf{y}(\\mathbf{x}), \\sigma^2) \\\\ \\log \\prod_{n=1}^N p(t^{(n)}|\\mathbf{x}^{(n)}; \\mathbf{w},b)^{1/N} & = \\ldots = \\mathrm{const} - \\frac{1}{2 N \\sigma^2} \\sum\\limits_{n=1}^N (t^{(n)} - \\mathbf{y}(\\mathbf{x}))^2 \\end{align}$;\n",
    " - $L^2$ regularizer can be viewed as MAP inference with Gaussian prior\n",
    " - MAP inference:\n",
    " $\\begin{align} \\arg\\max\\limits_\\mathbf{w} \\log p(\\mathbf{w}|\\mathcal{D}) &= \\arg\\max\\limits_\\mathbf{w} \\left( \\log p(\\mathbf{w}) + \\log p(\\mathcal{D}|\\mathbf{w}) \\right) \\\\ \\log p(\\mathcal{D}|\\mathbf{w}) &= \\textrm{as above} \\end{align}$;\n",
    " - Gaussian prior:\n",
    " $\\begin{align} \\mathbf{w} &\\sim \\mathcal{N}(\\mathbf{m},\\mathbf{S}) \\\\ \\log p(\\mathbf{w}) &= \\ldots \\\\ &= -\\frac{1}{2 \\eta} ||\\mathbf{w}||^2 + \\textrm{const}\\end{align}$\n",
    "\n",
    "\n",
    " Full Bayesian inference:\n",
    " - make prediction by averaging over all likely explanations\n",
    " under the posterior distribution;\n",
    " - compute posterior using Bayes' Rule:\n",
    " $p(\\mathbf{w}|\\mathcal{D}) \\propto p(\\mathbf{w}) p(\\mathcal{D}|\\mathbf{w})$;\n",
    " - make prediction using the posterior predictive distribution:\n",
    " $p(t|\\mathbf{x},\\mathcal{D}) = \\int p(\\mathbf{w}|\\mathcal{D}) p(t|\\mathbf{x},\\mathbf{w}) \\mathrm{d}\\mathbf{w}$.\n",
    "\n",
    "\n",
    " Bayesian linear regression:\n",
    " - make predictions using all possible weights, weighted by their posterior probability;\n",
    " - prior distribution: $\\mathbf{w} \\sim \\mathcal{N}(0, \\mathbf{S})$;\n",
    " - likelihood: $t | \\mathbf{x},\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{y}(\\mathbf{x}), \\sigma^2)$;\n",
    " - posterior distribution:\n",
    " $\\begin{align} \\log p(\\mathbf{w}|\\mathcal{D}) &= \\log p(\\mathbf{w} + \\log p(\\mathcal{D}|\\mathbf{w}) + \\mathrm{const} \\\\ &= \\ldots \\\\ &= \\log \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) \\end{align}$;\n",
    " - i.e. a multivariate Gaussian distribution,\n",
    " $\\begin{align} \\boldsymbol{\\mu} &= \\frac{1}{\\sigma^2} \\mathbf{\\Sigma} \\mathbf{\\Psi}^\\mathsf{T} \\mathbf{t} \\\\ \\mathbf{\\Sigma}^{-1} &= \\frac{1}{\\sigma^2} \\mathbf{\\Psi}^\\mathsf{T} \\mathbf{\\Psi} + \\mathbf{S}^{-1} \\end{align}$;\n",
    " - posterior predictive distribution:\n",
    " $p(t|\\mathbf{x},\\mathcal{D}) = \\int p(\\mathbf{w}|\\mathcal{D}) p(t|\\mathbf{x},\\mathbf{w}) \\mathrm{d}\\mathbf{w} = \\int \\mathcal{N}(t; \\mathbf{y}(\\mathbf{x}), \\sigma) \\mathcal{N}(\\mathbf{w}; \\mathbf{\\mu},\\mathbf{\\Sigma}) \\mathrm{d}\\mathbf{w}$;\n",
    " - i.e. a Gaussian with parameters\n",
    " $\\mu_\\textrm{pred} = \\mu^\\mathsf{T} \\boldsymbol{\\psi}(\\mathbf{x})$\n",
    " and $\\sigma_\\textrm{pred}^2 = \\boldsymbol{\\psi}(\\mathbf{x})^\\mathsf{T} \\mathbf{\\Sigma} \\boldsymbol{\\psi}(\\mathbf{x}) + \\sigma^2$.\n",
    "\n",
    "\n",
    " As more data points $(\\mathbf{x}, t)$ are observed,\n",
    " the posterior predictive distribution narrows.\n",
    "\n",
    " Example use:\n",
    " - *decision theory*:\n",
    " choose a single prediction $y$ to minimize the expected squared-error loss;\n",
    " - $\\arg \\min\\limits_y \\mathrm{E}_{p(t|\\mathbf{x},\\mathcal{D})}[(y - t)^2] = \\mathrm{E}_{p(t|\\mathbf{x},\\mathcal{D})}[t]$;\n",
    "\n",
    "\n",
    "\n",
    " *Black-box optimization*:\n",
    " - minimize a function with just queries of function values;\n",
    " - i.e. no gradient;\n",
    " - each query could be expensive so few as possible;\n",
    " - e.g. minimize validation error of an ML algorithm with respect to its hyperparameters.\n",
    " - *Bayesian optimization*:\n",
    "    - approximate the function with simpler functions (*surrogate function*);\n",
    "    - condition on the few data points $\\in \\mathcal{D}$ to infer posterior using Bayesian linear regression;\n",
    "    - define an *acquisition function* to choose the next point to query;\n",
    "    - desired properties: high for good and uncertain points, low for known points;\n",
    "    - candidates:\n",
    "        1. *probability of improvement* (PI), $\\textrm{PI} = \\mathrm{Pr}(f(\\theta) < \\gamma - \\epsilon)$;\n",
    "        2. *expected improvement* (EI), $\\textrm{EI} = \\mathrm{E}[\\max(\\gamma - f(\\theta), 0)]$.\n",
    "    - maximize the acquisition function using gradient descent;\n",
    "    - use random restarts to avoid local maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lecture 20 - Gaussian Processes\n",
    "\n",
    " Gaussian processes:\n",
    " - generalization of Bayesian linear regression;\n",
    " - distributions over functions.\n",
    "\n",
    "\n",
    " A Bayesian linear regression model defines a distribution over functions:\n",
    " - $f(\\mathbf{x}) = \\mathbf{w}^\\mathsf{T} \\mathbf{\\psi}(\\mathbf{x})$;\n",
    " - $\\mathbf{w}$ sampled from the prior $\\mathcal{N}(\\mathbf{\\mu}_\\mathbf{w}, \\mathbf{\\Sigma}_\\mathbf{w})$;\n",
    " - let $\\mathbf{f} = (f_1, \\ldots, f_N) = (f(\\mathbf{x}_1), \\ldots, f(\\mathbf{x}_N))$;\n",
    " - by linear transformation of Gaussian random variables, $\\mathbf{f}$ is a Gaussian too, with:\n",
    "    - $\\mathrm{E}[f_n] = \\mathbf{\\mu}_\\mathbf{w} \\mathbf{\\psi}(\\mathbf{x})$;\n",
    "    - $\\mathrm{cov}[f_m, f_n] = \\mathbf{\\psi}(\\mathbf{x}_m) \\mathbf{\\Sigma}_\\mathbf{w} \\mathbf{\\psi}(\\mathbf{x}_n)$;\n",
    " - $\\mathbf{f} \\sim \\mathcal{N}(\\mathbf{\\mu}_\\mathbf{f}, \\mathbf{\\Sigma}_\\mathbf{f})$;\n",
    "    - $\\mathbf{\\mu}_\\mathbf{f} = \\mathrm{E}[\\mathbf{f}] = \\mathbf{\\Psi} \\mathbf{\\mu}_\\mathbf{w}$;\n",
    "    - $\\mathbf{\\Sigma}_\\mathbf{f} = \\mathrm{cov}[\\mathbf{f}] = \\mathbf{\\Psi} \\mathbf{\\Sigma}_\\mathbf{w} \\mathbf{\\Psi}^\\mathsf{T}$;\n",
    " - assume noisy Gaussian observations, $y_n \\sim \\mathcal{N}(f_n, \\sigma^2)$;\n",
    " - i.e. $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{\\mu}_\\mathbf{y}, \\mathbf{\\Sigma}_\\mathbf{y})$;\n",
    "    - $\\mathbf{\\mu}_\\mathbf{y} = \\mathbf{\\mu}_\\mathbf{f}$;\n",
    "    - $\\mathbf{\\Sigma}_\\mathbf{y}) = \\mathbf{\\Sigma}_\\mathbf{f}) + \\sigma^2 \\mathbf{I}$;\n",
    " - let $\\mathbf{y}, \\mathbf{y}^\\prime$ be the training and test data;\n",
    "    - both are jointly Gaussian;\n",
    "    - $\\mathbf{y}^\\prime | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{\\mu}_{\\mathbf{y}^\\prime | \\mathbf{y}}, \\mathbf{\\Sigma}_{\\mathbf{y}^\\prime | \\mathbf{y}})$;\n",
    "    - $\\mathbf{\\mu}_{\\mathbf{y}^\\prime | \\mathbf{y}} = \\mathbf{\\mu}_\\mathbf{y} + \\mathbf{\\Sigma}_{\\mathbf{y}^\\prime \\mathbf{y}} \\mathbf{\\Sigma}_{\\mathbf{y} \\mathbf{y}}^{-1} (\\mathbf{y} - \\mathbf{\\mu}_\\mathbf{y})$;\n",
    "    - $\\mathbf{\\Sigma}_{\\mathbf{y}^\\prime | \\mathbf{y}} = \\mathbf{\\Sigma}_{\\mathbf{y}^\\prime \\mathbf{y}^\\prime} + \\mathbf{\\Sigma}_{\\mathbf{y}^\\prime \\mathbf{y}} \\mathbf{\\Sigma}_{\\mathbf{y} \\mathbf{y}}^{-1} \\mathbf{\\Sigma}_{\\mathbf{y} \\mathbf{y}^\\prime}$;\n",
    " - then the marginal likelihood is the PDF of a Gaussian, $p(\\mathbf{y}|\\mathbf{X}) = \\mathcal{N}(\\mathbf{y}; \\mathbf{\\mu}_\\mathbf{y}, \\mathbf{\\Sigma}_\\mathbf{y})$;\n",
    " - after defining $\\mathbf{\\mu}_\\mathbf{f}, \\mathbf{\\Sigma}_\\mathbf{f}$, we can forget about $\\mathbf{w}$.\n",
    "\n",
    "\n",
    " Gaussian processes:\n",
    " - specify\n",
    "    - a mean function $\\mathrm{E}[f(\\mathbf{x}_n)] = \\mu(\\mathbf{x}_n)$;\n",
    "    - a convariance function (called *kernel function*) $\\mathrm{cov}[f(\\mathbf{x}_m), f(\\mathbf{x}_n)] = k(\\mathbf{x}_m, \\mathbf{x}_n)$;\n",
    " - let $\\mathbf{K}_\\mathbf{X}$ denote the kernel matrix for points $\\mathbf{X}$ (also called the *Gram matrix*);\n",
    " - i.e. $(\\mathbf{K}_\\mathbf{X})_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$;\n",
    " - require that $\\mathbf{K}_\\mathbf{X}$ be positive semidefinite for any $\\mathbf{X}$;\n",
    " - $\\mu, k$ can be arbitrary...\n",
    " - this defines a distribution over *function values*;\n",
    " - can be extended to a distribution over *functions* using the *Kolmogorov Extension Theorem*;\n",
    " - such distribution over functions is called a *Gaussian process*;\n",
    "\n",
    "\n",
    " *Kernel Trick*:\n",
    " - many algorithms can be written in terms of dot products between feature vectors\n",
    "   $\\langle \\mathbf{x}, \\mathbf{x}^\\prime \\rangle = \\mathbf{\\psi}(\\mathbf{x})^\\mathsf{T} \\mathbf{\\psi}(\\mathbf{x}^\\prime)$;\n",
    " - a *kernel* implements an inner product between feature vectors;\n",
    " - e.g. feature vector $\\phi(\\mathbf{x}) = [1 \\: \\sqrt{2}x_1 \\: \\ldots \\: \\sqrt{2}x_d \\: \\sqrt{2}x_1 x_2 \\: \\ldots \\: \\sqrt{2} x_{d-1}x_d \\: x_1^2 \\ldots x_d^2]$\n",
    " - the *quadratic kernel* can compute the inner product in linear time;\n",
    "    - $\\begin{align} k(\\mathbf{x}, \\mathbf{x}^\\prime) &= \\langle \\mathbf{x}, \\mathbf{x}^\\prime \\rangle \\\\ &= 1 + \\sum\\limits_{i=1}^d 2 x_i x_i^\\prime + \\sum\\limits_{i,j=1}^d x_i x_j x_i^\\prime x_j^\\prime \\\\ &= \\left( 1 + \\langle \\mathbf{x},\\mathbf{x}^\\prime \\rangle \\right)^2 \\end{align}$;\n",
    " - many algorithms can be written in terms of kernels (*kernelized*);\n",
    " - useful composition rules:\n",
    "    - $k(\\mathbf{x},\\mathbf{x}^\\prime) = \\alpha$ is a kernel;\n",
    "    - if $k_1, k_2$ are kernels and $a, b \\geq 0$, then $a k_1 + b k_2$ is a kernel too;\n",
    "    - $k(\\mathbf{x},\\mathbf{x}^\\prime) = k_1(\\mathbf{x},\\mathbf{x}^\\prime) k_2(\\mathbf{x},\\mathbf{x}^\\prime)$ is also kernel;\n",
    " - before artificial neural networks, kernel SVM is the best at classification;\n",
    "\n",
    "\n",
    " Computational cost of the kernel trick:\n",
    " - allows the use of very high-dim. feature spaces but at a cost;\n",
    " - Bayesian linear regression: need to invert a $d \\times d$ matrix;\n",
    " - GP regression: need to invert a $N \\times N$ matrix;\n",
    " - $\\mathcal{O}(N^3)$ cost is typical of kernel methods;\n",
    "\n",
    "\n",
    " GP kernels:\n",
    " - define kernel function by giving a set of basis functions\n",
    " and put a Gaussian prior on $\\mathbf{w}$;\n",
    " - e.g. *squared-exponential* or *Gaussian* or *radial basis function* (RBF) kernel\n",
    "    - $k(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma^2 \\exp \\left(- \\frac{|| \\mathbf{x}_i - \\mathbf{x}_j ||^2}{2 \\ell^2} \\right)$\n",
    " - this is a *kernel family* with hyperparameters $\\sigma, \\ell$;\n",
    " - $\\sigma^2$ is the output variance;\n",
    " - $\\ell$ is the lengthscale;\n",
    " - choice of these hyperparameters heavily affects the predictions;\n",
    " - tune them by e.g. maximizing marginal likelihood;\n",
    " - this kernel is *stationary* since it depends only on $\\mathbf{x}_i - \\mathbf{x}_j$;\n",
    " - most kernels are stationary;"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
