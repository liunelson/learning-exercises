{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes from [CSC 411](http://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 21 - Reinforcement Learning (Part I-II)\n","\n"," Learning problems:\n","\n"," 1. supervised:\n","    given inputs and some known outputs, predict remaining outputs;\n"," 2. reinforcement:\n","    given inputs and no correction, choose outputs (actions) to maximize reward;\n"," 3. unsupervised:\n","    given only inputs, predict outputs (e.g. clustering).\n","\n","\n"," Challenges of reinforcement learning (RL):\n"," - continuous stream of inputs;\n"," - effects of an action depend on the state of the agent;\n"," - reward depends on the state and actions;\n"," - only know reward given specific action, not generally;\n"," - possible delay between action and reward;\n"," - e.g. tic-tac-toe.\n","\n","\n"," Markov decision process (MDP):\n"," - framework to describe RL;\n"," - defined by the tuple $(\\mathcal{S, A, P, R}, \\gamma)$;\n","    - $\\mathcal{S}$, state space;\n","    - $\\mathcal{A} = \\{a_1, \\ldots, a_{|\\mathcal{A}|}\\}$, action space;\n","    - $\\mathcal{P}$, transition probability;\n","    - $\\mathcal{R}$, immediate reward distribution;\n","    - $\\gamma \\in [0, 1)$, discount factor\n"," - the agent has a state $S \\in \\mathcal{S}$ within the environment;\n"," - at every time step $t$,\n","    - the agent in state $S_t$;\n","    - taking action $A_t$ moves it to a new state $S_{t+1} \\sim \\mathcal{P}(\\cdot|S_t, A_t)$;\n","    - some reward $R_{t+1} \\sim \\mathcal{R(\\cdot|S_t, A_t, S_{t+1})}$ is received;\n"," - action selection mechanism is described by a *policy* $\\pi$;\n","    - $\\pi$ maps states to actions;\n","    - deterministic: $A_t = \\pi(S_t)$\n","    - stochastic: $A_t \\sim \\pi(\\cdot|S_t)$\n"," - goal: find $\\pi$ such that the agent's long-term reward is maximized;\n"," - long-term reward:\n","    - total reward, $R_0 + R_1 + R_2 + \\ldots$;\n","    - discounted reward, $R_0 + \\gamma R_1 + \\gamma^2 R_2 + \\ldots$;\n","    - discount factor $\\gamma$ determines how near- or far-sighted the agent is;\n"," - transition probability, $\\mathcal{P}(S_{t+1} = s^\\prime|S_t = s, A_t = a)$\n"," - Markov property: the future depends on the past only through the current state;\n"," - *(state-)value function* is the expected discounted reward;\n","    - for evaluating desirability of states);\n","    - $V^{(\\pi)}(s) = \\mathrm{E}_\\pi \\left[ \\sum\\limits_{t \\geq 0} \\gamma^t R_t \\: | \\: S_0 = s \\right]$\n","    - (state-value function for policy $\\pi$ given starting state $s$)\n"," - *action-value function*\n","    - $Q^{(\\pi)}(s, a) = \\mathrm{E}_\\pi \\left[ \\sum\\limits_{t \\geq 0} \\gamma^t R_t \\: | \\: S_0 = s, A_0 = a \\right]$\n"," - optimal value function:\n","   $Q^\\ast (s, a) = \\sup\\limits_\\pi Q^{(\\pi)}(s, a)$\n"," - optimal policy:\n","   $\\pi^\\ast(s) \\leftarrow \\arg \\max\\limits_{a} Q^\\ast(s, a)$;\n","\n","\n"," Example: tic-tac-toe\n"," - the game is described as\n","    - state: positions of $X$'s and $O$'s on the board;\n","    - action: location of new $X, O$'s;\n","    - policy: mapping states to actions;\n","    - reward: win, lose, tie;\n","    - value function: predict future reward based on current state;\n"," - since the state space is small, value function can be just a table;\n","\n","\n"," *Bellman Equation*:\n"," - $\\begin{align} Q^{(\\pi)}(s, a) &= \\mathrm{E}_\\pi \\left[ \\sum\\limits_{t \\geq 0} \\gamma^t R_t \\: | \\: S_0 = s, A_0 = a \\right] \\\\ &= \\mathrm{E}\\left[ R(S_0,A_0) + \\gamma Q^{(\\pi)}(S_1, \\pi(S_1)) \\: | \\: S_0 = s, A_0 = a \\right] \\\\ &= r(s, a) + \\gamma \\int_\\mathcal{S} \\mathcal{P}(\\mathrm{d}s^\\prime | s, a) Q^{(\\pi)}(s^\\prime, \\pi(s^\\prime)) \\\\ &= (T^{(\\pi)} Q^{(\\pi)})(s, a) \\end{align}$\n"," - $T$ is the *Bellman operator*;\n"," - $r(s,a) = \\mathrm{E}[\\mathcal{R}(\\cdot|s, a)]$;\n"," - the *Bellman optimality operator*, $(T^{\\ast} Q)(s, a) = r(s, a) + \\gamma \\int_\\mathcal{S} \\mathcal{P}(\\mathrm{d}s^\\prime | s, a) \\max\\limits_{a^\\prime \\in \\mathcal{A}} Q^{(\\pi)}(s^\\prime, \\pi(s^\\prime))$;\n"," - note that\n","     - $\\begin{align} Q^{(\\pi)} &= T^{(\\pi)}  Q^{(\\pi)} \\\\ Q^\\ast &= T^{\\ast}  Q^{\\ast} \\end{align}$\n","     - these are fixed-point equations;\n","     - they have unique solutions;\n","\n","\n"," Policy evaluation:\n"," - given $\\pi$, find $V^{(\\pi)}$ or $Q^{(\\pi)}$;\n"," - assuming $\\mathcal{P}$ and $r(s,a)$ are known;\n"," - if the state-action space $\\mathcal{S} \\times \\mathcal{A}$ is small enough,\n","   then solve the linear system of equations\n","   $Q(s, a) = r(s,a) + \\gamma \\sum\\limits_{s^\\prime \\in \\mathcal{S}} \\mathcal{P}(s^\\prime|s,a) Q(s^\\prime, \\pi(s^\\prime)) \\: \\forall \\: (s,a) \\in \\mathcal{S} \\times \\mathcal{A}$\n"," - *planning* problem: find optimal $\\pi$;\n"," - *value iteration* (VI): $Q_{k+1} \\leftarrow T^\\ast Q_k$;\n","\n","\n"," $\\ldots$"],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 23 - Algorithmic Fairness\n","\n"," Notation:\n"," - $X$: input to classifier;\n"," - $S$: sensitive feature (e.g. age, sex, etc.);\n"," - $Z$: latent representation;\n"," - $Y$: prediction;\n"," - $T$: true label;\n","\n","\n"," Criteria for fair classification:\n"," - demographic parity: $Y \\perp S$;\n"," - equalized odds: $Y \\perp S | T$;\n"," - equal opportunity: $Y \\perp S | T = t$;\n"," - equal (weak) calibration: $T \\perp S | Y$;\n"," - equal (strong) calibration: $T \\perp S | Y$ and $Y = \\mathrm{Pr}(T = 1)$;\n"," - fair subgroup accuracy: $\\mathbb{1}[T = Y] \\perp S$;\n"," - $\\perp$ denotes stochastic independence;\n"," - many are incompatible;\n","\n","\n"," Fair representation:\n"," - goal: find fair representation $Z$ that removes any info about sensitive feature $S$;\n"," - desired traits:\n","     1. retain info about $X$ (high mutual info between $X$ and $Z$);\n","     2. obfuscate $S$ (low mutual info between $S$ and $Z$);\n","     3. allow high classification accuracy (high mutual info between $T$ and $Z$);"],"metadata":{}},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}