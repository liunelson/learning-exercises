{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes from [CSC 411](http://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 5 - Ensembles (Part II)\n","\n"," A *weak* learner is a learning algorithm that outputs\n"," a hypothesis (e.g. a classifier) that performs only slightly better\n"," than by chance.\n","\n"," Examples: decision stumps (trees with only one split).\n","\n"," *Boosting*:\n"," train (weak) classifiers sequentially,\n"," each time focusing on training data points\n"," that were previously misclassified.\n","\n"," *Adaptive boosting* (AdaBoost):\n"," - at each iteratiion, assigning larger weights $w_i$\n"," to data points $x_i$ that were mis-classified;\n"," - ensemble classifier $H$ is the weighted sum\n"," of the weak classifiers $h_j$;\n"," - bias is reduced by making subsequent classifiers\n"," focus on their predecessors' mistakes;\n"," - Steps:\n","   1. given training dataset of size $N$ with $x_i \\in \\mathbb{R}^d$,\n"," $y_i \\in \\{-1, 1\\}$ (i.e. two classes);\n","    2. initialize weights as $w_i = \\frac{1}{N}$;\n","    3. for iteration $m$,\n","       $\\begin{align} \\epsilon_m &= \\frac{\\sum\\limits_{i=1}^N w_i 1_{h(x_i) \\neq y_i}}{\\sum\\limits_{i=1}^N w_i} \\\\ \\alpha_m &= \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right) \\\\ w_{i,m+1} &= w_{i,m} \\exp \\left(-\\alpha_m y_i h_m(x_i) \\right) \\end{align}$;\n","    4. $H(x) = \\mathrm{sgn} \\left(\\sum_{m=1}^M a_m h_m(x) \\right)$.\n","\n","\n"," AdaBoost can be interpreted as a stage-wise estimation procedure\n"," for an additive logistic regression model\n"," wherein the minimized loss function is $L(y, h(x)) = \\mathrm{E}[\\mathrm{e}^{-y h(x)}]$.\n","\n"," Assuming each weak learning has error $\\epsilon_m \\leq \\frac{1}{2} - \\gamma \\; \\forall \\; m$,\n"," the training error of $H(x)$ is $L_N(H) = \\frac{1}{N} \\sum\\limits_{i=1}^N 1_{H(x_i) \\neq y_i} \\leq \\mathrm{e}^{-2 \\gamma^2 M}$"],"metadata":{}},{"source":["  "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ## Lecture 6 - Linear Regression\n","\n","\n","\n",""],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}