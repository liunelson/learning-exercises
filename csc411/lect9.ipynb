{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes from [CSC 411](http://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 9 - Support Vector Machines and Boosting\n","\n"," Consider the binary classification problem again,\n"," $\\begin{align} z &= w^\\mathsf{T} x + b \\\\ y &= \\mathrm{sgn}(z) \\end{align}$.\n","\n"," Previously, this was solved using\n"," logistic regression with a cross-entropy loss function.\n","\n"," Alternative:\n"," - geometric solution;\n"," - desired decision boundary is a line (i.e. a $K-1$ hyperplane)\n"," separating the two classes;\n"," - defined by $f(x) = w^\\mathsf{T} x + b$ for some $(w, b)$;\n"," - the *optimal seperating hyperplane*\n"," is the one that maximizes the distance to the closest point\n"," from either class (maximizing the *margin* of the classifier);\n"," - recall the signed point-plane distance formula,\n"," $d = \\frac{w^\\mathsf{T}x^\\prime + b}{||w||_2}$;\n"," - then we get the optimization problem\n"," - $\\min ||w||_2^2$ such that $t_n (w^\\mathsf{T}x_n + b) \\leq 1$\n"," for all training examples $i = 1,\\ldots,N$;\n"," - the important training examples are those\n"," with margin $||w||_2$ (called *support vectors*);\n"," - this algorithm is called *support vector machine* (SVM)\n"," - similar algorithms are called *max-margin* or *large-margin*;\n","\n","\n"," If the data is not separable:\n"," - some points could be allowed to be misclassified\n"," using *slack variables* $\\xi_n \\geq 0$;\n"," - *soft-margin* constraint:\n"," $t_n(w^\\mathsf{T} x_n + b) \\geq 1 - \\xi_n$\n"," -  *soft-margin* objective:\n"," $\\min\\limits_{w,b,\\xi} \\frac{1}{2} ||w||_2^2 + \\gamma \\sum\\limits_{n=1}^N \\xi_n$\n"," - new hyperparameter $\\gamma$ for trading off margin with slack;\n","\n","\n"," Note that the soft-margin SVM can be written\n"," to be a linear classifier with *hinge loss*\n"," and a $L_2$ regularizer:\n"," - write $y_n(w, b) = w^\\mathsf{T}x + b$;\n"," - rewrite constraint as $\\xi_n \\geq 1 - t_n y_n(w, b)$;\n"," - then the slack penalty can be written as\n"," $\\sum\\limits_{n=1}^N \\xi_n = \\sum\\limits_{n=1}^N \\max\\{0, 1 - t_n y_n(w,b)\\}$;\n"," - replace $\\max\\{0,y\\}$ by $(y)_+$;\n"," - thus the optimization problem becomes\n"," $ \\min\\limits_{w,b} \\sum\\limits_{n=1}^N \\left(1 - t_n y_n(w,b)\\right)_+ + \\frac{1}{2 \\gamma} ||w||_2^2$\n"," - this is just a linear classifier\n"," - with a loss function $\\mathcal{L}(y,t) = (1-ty)_+$ (i.e. *hinge loss*)\n"," and a $L_2$-norm regularizer;\n"," - to find $w$, use gradient descent;\n","\n","\n"," Recall AdaBoost to reinterpret it in terms of loss functions:\n"," - consider a class $\\mathcal{H}$ of hypothesis;\n"," - each hypothesis is $h_i: x \\rightarrow \\{-1,+1\\}$\n"," (a weak learner or a *base*);\n"," - an additive model with $m$ terms is given by $H_m(x) = \\sum\\limits_{i=1}^m \\alpha_i h_i(x)$;\n"," - a way to fit additive models is *stagewise training*:\n","    1. initialize $H_0(x) = 0$;\n","    2. for $m = 1$ to $T$, compute the $m$-th hypothesis and its coefficients, $(h_m, \\alpha_m) \\leftarrow \\arg \\min\\limits_{h \\in \\mathcal{H},\\alpha} \\sum\\limits_{n=1}^N \\mathcal{L}(H_{m-1}(x_n) + \\alpha h(x_n), t_n)$\n","    3. add it to the additive model, $H_m = H_{m-1} + \\alpha_m h_m$;\n"," - consider a exponential loss function $\\mathcal{L}(y,t) = \\mathrm{e}^{-ty}$;\n"," - propagating...\n"," - we recover the AdaBoost algorithm:\n"," - $\\begin{align} h_m &\\leftarrow \\arg \\min\\limits_{h \\in \\mathcal{H}} \\sum\\limits_{n=1}^N w_n^{(m)} 1_{h(x_n) \\neq t_i} \\\\ \\alpha &= \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)\\\\ \\epsilon_m &= \\frac{\\sum\\limits_{n=1}^N w_n^{(m)} 1_{h(x_n) \\neq t_i} }{\\sum\\limits_{n=1}^N w_n^{(m)}} \\\\ w_n^{(m+1)} &= w_n^{(m)} \\exp\\left(-\\alpha_m h_m(x_n) t_n \\right) \\end{align}$\n"," - thus, AdaBoost is just an additive model with stagewise training\n"," and an exponential loss function."],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 10 - Neural Networks (Part I)\n","\n"," Consider a *unit* that computes\n"," the equation $y = \\phi(w^\\mathsf{T} x + b)$:\n"," - $x$ is the input vector;\n"," - $y$ is the scalar output of the unit;\n"," - $w$ are the weights associated with the input units;\n"," - $b$ are the associated biases.\n"," - similar to the expression for logistic regression\n"," $y = \\sigma(w^\\mathsf{T}x + b)$.\n","\n","\n"," Connect several units together\n"," into a *directed acyclic graph* to\n"," give a *feed-forward neural network*:\n"," - *recurrent neural networks* can have cycles;\n"," - structure: an input layer, many hidden layers, an output layer;\n"," - each layer connects $N$ input units to $M$ output layers;\n"," - *fully connected* = all input units are connected to all output units;\n"," - a fully connected multilayer network is called a *multilayer perceptron*;\n"," - $y = \\phi(W x + b)$\n"," - examples of $phi(z)$:\n","    1. linear ($y = z$),\n","    2. rectified linear unit (ReLU, $y = \\max(0,z)$),\n","    3. soft-ReLU ($y = \\ln(1 + \\mathrm{e}^z)$),\n","    4. hard-threshold/heaviside/unit-step ($y = H(z)$),\n","    5. logistic ($y = \\frac{1}{1 + \\mathrm{e}^z}$),\n","    6. tanh ($y = \\tanh(z) $);\n","\n","\n"," Expressive power:\n"," - any sequence of linear layers is equivalent to a single linear layer;\n"," - thus, deep *linear* networks are no more expressive than linear regression;\n"," - however, a multilayer feed-forward network with\n"," *nonlinear* activation functions are *universal function approximators*;\n"," - universality $\\Rightarrow$ risk of overfitting;\n","\n","\n"," The *backpropagation* algorithm:\n","\n"," 1. forward pass by computing $z, y = \\phi(z), \\mathcal{L}, \\mathcal{R}, \\mathcal{L}_\\textrm{reg}$\n"," in order;\n"," 2. backward pass by computing partial derivatives of $\\mathcal{L}_\\textrm{reg}$\n"," with chain rule.\n","\n","\n"," Example:\n"," - $\\begin{align} z &= W^{(1)}x + b^{(1)} \\\\ h &= \\sigma(z) \\\\ y &= W^{(2)}x + b^{(2)} \\\\ \\mathcal{L} &= \\frac{1}{2}|| t-y||^2 \\end{align}$\n"," - $\\begin{align} \\frac{\\partial L}{\\partial y} &= y - t \\\\ \\frac{\\partial L}{\\partial W^{(2)}} &= \\frac{\\partial L}{\\partial y} h^\\mathsf{T} \\\\ \\ldots \\end{align}$\n","\n","\n"," Computational cost:\n"," - forward pass: one add-multiply operation per weight;\n"," - backward pass: two add-multiply operation per weight;\n"," - cost is linear in number of layers, quadratic in the number of units per layer."],"metadata":{}},{"cell_type":"markdown","source":[" ## Lecture 11 - Neural Networks (Part II)\n","\n"," Problem:\n"," fully connected multilayer networks, necessary for real-life object recognition,\n"," are prohibitively costly.\n","\n"," Solution:\n"," locally connected layers.\n","\n"," *Convolutional network*:\n"," - convolution layer: convolve a *kernel* or *filter* across the inputs\n"," - basically force a local set of inputs to share\n"," the same weights and biases over the entire input vector;\n"," - conv. hyperparameters:\n"," $w \\times h$ size of filters,\n"," number of filters (depth of output volume),\n"," stride length (step size of convolution).\n"," - pooling layer:\n"," combine the filter responses and downsample\n"," (e.g. *max* or *average* pooling);\n"," - pooling hyperparameters:\n"," spatial extent (size of pooling area), stride length;"],"metadata":{}},{"source":["      "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ## Lecture 12 - Principal Component Analysis\n","\n"," PCA is an unsupervised learning algorithm\n"," and a linear model with a closed-form solution.\n","\n"," Projection onto a subspace $\\mathcal{S} \\in \\mathbb{R}^K$:\n"," - $\\mathbf{z} = \\mathbf{U}^\\mathsf{T} (\\mathbf{x} - \\mathbf{\\mu})$;\n"," - $\\mathbf{x}^\\prime = \\mathbf{U}\\mathbf{z} + \\mathbf{\\mu}$;\n"," - the columns of $\\mathbf{U}$ forms an orthonormal basis of $\\mathcal{S}$;\n"," - $\\mathbf{\\mu}$ is the origin of $\\mathcal{S}$;\n"," - $\\mathbf{x}^\\prime \\in \\mathbb{R}^N$ is the *reconstruction* of $\\mathbf{x} \\in \\mathbb{R}^L$\n"," (the point in $\\mathcal{S}$ closest to $\\mathbf{x}$);\n"," - $\\mathbf{z} \\in \\mathbb{R}^K$ is the *(latent) representation* (or *code*) of $\\mathbf{x}$.\n","\n","\n"," When $K \\ll L$, this mapping is called *dimensionality reduction*;\n"," learning such a mapping is called *representation learning*.\n","\n"," Choosing a good subspace $\\mathcal{S}$:\n"," - set $\\boldsymbol{\\mu} = \\frac{1}{N}\\sum\\limits_{n=1}^N \\mathbf{x}_n$;\n"," - two equivalent criteria:\n","    1. minimize *reconstruction error*,\n"," $\\min \\frac{1}{N}\\sum\\limits_{n=1}^N ||\\mathbf{x}_n - \\mathbf{x}^\\prime_n ||^2$;\n","    2. maximize variance of the code vectors,\n"," $\\max \\sum\\limits_{i=1}^K \\mathrm{var}[z_i] = \\ldots = \\frac{1}{N} \\sum\\limits_{n=1}^N ||\\mathbf{z}_n||^2$;\n"," - both are equivalent since\n"," $\\frac{1}{N}\\sum\\limits_{n=1}^N ||\\mathbf{x}_n - \\mathbf{x}^\\prime_n ||^2 + \\frac{1}{N}\\sum\\limits_{n=1}^N ||\\mathbf{x}^\\prime_n - \\boldsymbol{\\mu}||^2 = \\frac{1}{N}\\sum\\limits_{n=1}^N ||\\mathbf{x}_n - \\boldsymbol{\\mu}||^2 = \\textrm{const}$.\n","\n","\n"," Recall *spectral decomposition*:\n"," - $\\mathbf{A}$ is a symmetry matrix with a full set of eigenvectors and eigenvalues $\\lambda_i$;\n"," - $\\mathbf{A} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^\\mathsf{T}$;\n"," - $\\mathbf{Q}$ is orthogonal with columns $p$ as the eigenvectors of $A$;\n"," - $\\mathbf{D} = \\mathrm{diag}({\\lambda_i})$;\n"," - $\\mathbf{A}$ is positive semidefinite iff $\\lambda_i \\geq 0 \\; \\forall \\; i$;\n","\n","\n"," Define the *empirical/sample covariance matrix*:\n"," - $\\mathbf{\\Sigma} = \\frac{1}{N} \\sum\\limits_{n=1}^N (\\mathbf{x}_n - \\boldsymbol{\\mu})(\\mathbf{x}_n - \\boldsymbol{\\mu})^\\mathsf{T}$;\n"," - $\\mathbf{\\Sigma}$ is symmetric and positive semidefinite;\n"," - the optimal PCA subspace is spanned by the top $K$ eigenvectors of $\\mathbf{\\Sigma}$;\n"," - they are the *principal components*;\n"," - see the *Courant-Fischer min-max theorem*.\n"," - note that $\\mathrm{cov}[\\mathbf{z}] = \\ldots = [\\mathbf{I} \\: \\mathbf{0}] D [\\mathbf{I} \\: \\mathbf{0}]^\\mathsf{T}$\n"," - i.e. the basis vectors of $\\mathbf{z}$ are de-correlated.\n","\n","\n"," *Autoencoders*:\n"," - they are feed-forward neural networks that predict $\\mathbf{x}$ given $\\mathbf{x}$;\n"," - non-trivial example:\n"," add a *bottleneck layer* whose dimension (number of units in layer)\n"," is much smaller than that of the inputs;\n"," - reasons:\n","    1. map high-dim. data to lower dimensions for visualization;\n","    2. learn abstract features in an unsupervised way;\n"," - linear autoencoder:\n","    1. a $L$-d input layer, $K$-d hidden layer; $L$-d output layer ($\\mathbf{x} \\xrightarrow{\\mathbf{W}_1} \\mathbf{z} \\xrightarrow{\\mathbf{W}_2} \\mathbf{y}$);\n","    2. linear activations with squared-error loss $\\mathcal{L} = ||\\mathbf{x} - \\mathbf{y}||^2$;\n","    3. $\\mathbf{y} = \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{x}$;\n","    4. $\\mathbf{W}_1$ encodes while $\\mathbf{W}_2$ decodes;\n","    5. optimal mapping is just PCA ($\\mathbf{W}_1 = \\mathbf{U}^\\mathsf{T}$, $\\mathbf{W}_2 = \\mathbf{U}$);\n"," - nonlinear autoencoder:\n","    1. projection onto a nonlinear manifold;\n","    2. i.e. nonlinear dimensional reduction;\n","    3. better than linear autoencoders;"],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}