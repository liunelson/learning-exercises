{"cells":[{"cell_type":"markdown","source":[" # Notes and exercises from [Nielsen2015](http://neuralnetworksanddeeplearning.com/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 1.1 - Perceptrons\n","\n"," $ y =\n","   \\begin{cases}\n","       0 & \\text{if } \\sum_i w_i x_i \\geq \\text{threshold} \\\\\n","       1 & \\text{if } \\sum_i w_i x_i < \\text{threshold}\n","   \\end{cases} $\n","\n"," where $y = \\text{output}$, $x_i = \\text{input}$, and $w_i = \\text{weights}$.\n","\n"," Rewrite conditions as $\\mathbf{w} \\cdot \\mathbf{x} + b \\leq 0$, $> 0$\n"," where $b = -\\text{threshold}$ is the *bias*.\n","\n"," Perceptrons can be combined to give $\\text{AND}, \\text{OR}, \\text{NAND}$, etc.\n"," and perform any basic computation."],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 1.2 - Logistic/Sigmoid Neurons\n","\n"," To optimize $w_i$, the output $y$ should be defined by\n"," a differentiable *transfer* or *activation* function.\n","\n"," Let's replace it with the *logistic* or *sigmoid* function\n"," $$f(z) = \\frac{1}{1 + \\mathbf{e}^{-z}}$$\n"," where $z = \\mathbf{w} \\cdot \\mathbf{x} + b$.\n","\n"," Thus,\n"," $$\\Delta y \\approx \\sum_i \\frac{\\partial y}{\\partial w_i} \\Delta w_i + \\frac{\\partial y}{\\partial b} \\Delta b$$"],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 1.3 - Architecture of Neural Networks\n","\n"," Layers: *input*, *hidden*, and *output*.\n","\n"," *Feedforward* neural networks: no loop back.\n"," *Recurrent* neural networks: feedback loops allowed."],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 1.4 - Simple Network to Classify Handwritten Digits\n","\n"," Read handwritten digits = *segmentation* + *classification*.\n","\n"," Exercise: classify greyscale images of handwritten digits ($28 \\times 28$ pixels) in\n"," [MNIST](http://yann.lecun.com/exdb/mnist/) dataset\n"," (60,000 labeled training images, 10,000 labeled test images)\n","\n"," | Filename | Content | Size |\n"," | --- | --- | --- |\n"," | `train-images-idx3-ubyte.gz` | training set images | $9912422$ bytes |\n"," | `train-labels-idx1-ubyte.gz` | training set labels | $28881$ bytes |\n"," | `t10k-images-idx3-ubyte.gz` | test set images | $1648877$ bytes |\n"," | `t10k-labels-idx1-ubyte.gz` | test set labels | $4542$ bytes |\n"," Unzip and read test labels"],"metadata":{}},{"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Convert binary data to integer array\n","print(int.from_bytes(t10k_lab_data[0:4], byteorder = 'big', signed = False))\n","t10k_num = int.from_bytes(t10k_lab_data[4:8], byteorder = 'big', signed = False)\n","\n","t10k_lab = -1*np.ones((t10k_num, 1))\n","for i in range(t10k_num): \n","    t10k_lab[i] = t10k_lab_data[i + 8]\n","\n","# Delete bin data\n","t10k_lab_data = None\n","del t10k_lab_data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Unzip and read test images\n","with gzip.open('E:/Documents/Projects/learning-exercises/datasets/MNIST/t10k-images-idx3-ubyte.gz', 'rb') as hFile:\n","    t10k_img_data = hFile.read()\n","hFile.close()\n","hFile = None\n","del hFile \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# As before...\n","print(int.from_bytes(t10k_lab_data[0:4], byteorder = 'big', signed = False))\n","print(int.from_bytes(t10k_lab_data[4:(4+4)], byteorder = 'big', signed = False))\n","numRow = int.from_bytes(t10k_img_data[8:(8+4)], byteorder = 'big', signed = False)\n","numCol = int.from_bytes(t10k_img_data[12:(12+4)], byteorder = 'big', signed = False)\n","\n","# Image data\n","numPix = numRow*numCol\n","t10k_img = -1*np.ones((t10k_num, numPix))\n","k = 0\n","for i in range(t10k_num):\n","    for j in range(numPix):\n","        k = k + 1\n","        t10k_img[i, j] = t10k_img_data[i*(numPix) + j + 16]\n","\n","# Delete bin data\n","t10k_img_data = None\n","del t10k_img_data\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["fig = plt.figure(1)\n","fig.suptitle('MNIST t10k')\n","for i in range(18):\n","    ax = fig.add_subplot(3, 6, i + 1)\n","    ax.imshow(np.reshape(t10k_img[i, 0:numPix], (numRow, numCol)))\n","    ax.set_aspect(aspect = 1)\n","    ax.tick_params(axis = 'x', bottom = False, labelbottom = False)\n","    ax.tick_params(axis = 'y', left = False, labelleft = False)\n","    ax.set_title('%(index)d (%(label)d)' % {'index': i, 'label': t10k_lab[i]})\n","\n","ax = None\n","del ax\n","fig = None\n","del fig\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Approach: 784 nodes in input layer, $n = 15$ nodes in single hidden layer,\n"," 10 nodes in output layer ($0 \\ldots 9$).\n","\n"," $x = (784 \\times 1)$ vector, $y = (10 \\times 1)$ vector\n","\n"," Define a *cost* (*loss* or *objective*) function:\n"," $$C(\\mathbf{w}, b) = \\frac{1}{2 n} \\sum_\\mathbf{x} || \\mathbf{y}(\\mathbf{x}) - \\mathbf{a} ||^2 $$\n"," where $n$ is the number of training inputs and\n"," $\\mathbf{a} = \\mathbf{a}(\\mathbf{w}, \\mathbf{x}, b)$ is the network output given $\\mathbf{x}$.\n"," This $C$ is a *quadratic* cost function or the *mean square error* (MSE).\n","\n"," Let's consider how to minimize $C$ over $\\mathbf{w}, b$\n"," such that $\\mathbf{a} \\rightarrow \\mathbf{y} \\ \\forall \\ \\mathbf{x}$ in the training set."],"metadata":{}},{"cell_type":"markdown","source":[" One technique is *gradient descent*.\n"," $$\\Delta C \\approx \\sum_i \\frac{\\partial C}{\\partial w_i} \\Delta w_i = \\nabla C \\cdot \\Delta w $$\n"," where $\\Delta w$ is chosen to be $-\\eta \\nabla C$ (for some $\\eta > 0$)\n"," to ensure that $\\Delta C = - \\eta |\\nabla C |^2 \\leq 0 $.\n","\n"," Thus, $\\mathbf{w} \\rightarrow \\mathbf{w} - \\eta \\nabla C$\n"," would allow us to iteratively approach a (local) minimum of $C$,\n"," where $\\eta = \\frac{\\epsilon}{| \\nabla C |}$ and $\\epsilon$ is some small value.\n","\n"," Problem: Computation of $C$ becomes slow if there are many $\\mathbf{x}$.\n","\n"," Solution: *Stochastic gradient descent*\n"," $$ \\nabla C \\approx \\sum_{\\{\\mathbf{x}\\}} \\nabla C_\\mathbf{x}$$\n"," where $\\{ \\mathbf{x} \\}$ is some small, randomly chosen subset of all $\\mathbf{x}$."],"metadata":{}},{"cell_type":"markdown","source":[" # Implementing network to classify the MNIST digits"],"metadata":{}},{"source":["# Import data\n","import gzip \n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","with gzip.open('E:/Documents/Projects/learning-exercises/datasets/MNIST/train-labels-idx1-ubyte.gz', 'rb') as hFile:\n","    train_lab_data = hFile.read()\n","hFile.close()\n","with gzip.open('E:/Documents/Projects/learning-exercises/datasets/MNIST/train-images-idx3-ubyte.gz', 'rb') as hFile:\n","    train_img_data = hFile.read()\n","hFile.close()\n","\n","hFile = None\n","del hFile \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Convert binary data to integer array\n","\n","# Label data\n","print(int.from_bytes(train_lab_data[0:4], byteorder = 'big', signed = False))\n","train_num = int.from_bytes(train_lab_data[4:8], byteorder = 'big', signed = False)\n","\n","train_lab = -1*np.ones((train_num, 1))\n","for i in range(train_num): \n","    train_lab[i] = train_lab_data[i + 8]\n","train_lab_data = None\n","del train_lab_data\n","\n","# Image data\n","print(int.from_bytes(train_img_data[0:4], byteorder = 'big', signed = False))\n","print(int.from_bytes(train_img_data[4:(4+4)], byteorder = 'big', signed = False))\n","numRow = int.from_bytes(train_img_data[8:(8+4)], byteorder = 'big', signed = False)\n","numCol = int.from_bytes(train_img_data[12:(12+4)], byteorder = 'big', signed = False)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Split 60k training set into smaller 50k training set\n"," and 10k *validation* set.\n"," Image data\n"," Label data"],"metadata":{}},{"source":["fig = plt.figure(1)\n","fig.suptitle('MNIST train50k')\n","for i in range(18):\n","    ax = fig.add_subplot(3, 6, i + 1)\n","    ax.imshow(np.reshape(train_img[i, 0:numPix], (numRow, numCol)))\n","    ax.set_aspect(aspect = 1)\n","    ax.tick_params(axis = 'x', bottom = False, labelbottom = False)\n","    ax.tick_params(axis = 'y', left = False, labelleft = False)\n","    ax.set_title('%(index)d (%(label)d)' % {'index': i, 'label': train_lab[i]})\n","\n","fig = plt.figure(2)\n","fig.suptitle('MNIST valid10k')\n","for i in range(18):\n","    ax = fig.add_subplot(3, 6, i + 1)\n","    ax.imshow(np.reshape(valid_img[i, 0:numPix], (numRow, numCol)))\n","    ax.set_aspect(aspect = 1)\n","    ax.tick_params(axis = 'x', bottom = False, labelbottom = False)\n","    ax.tick_params(axis = 'y', left = False, labelleft = False)\n","    ax.set_title('%(index)d (%(label)d)' % {'index': i, 'label': valid_lab[i]})\n","\n","ax = None\n","del ax\n","fig = None\n","del fig\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Define `Network` class with list `sizes`,\n"," where a neural network with 2, 3, 1 neurons in the 1st, 2nd, and 3rd layers\n"," is specified by `sizes = [2, 3, 1]`.\n"," The weights and biases are initialized to random values (Gaussian, mean $0$ and std dev $1$).\n","\n"," `net.weights[0]` specifies the weights $\\mathbf{w}$ connecting the neurons of the 1st and 2nd layers;\n"," `net.biases[0]`, the biases $\\mathbf{b}$ of the neurons in the 2nd layer."],"metadata":{}},{"cell_type":"markdown","source":[" Let's use $w_{jk}$ to denote the weight for the connection\n"," between the $k$th neuron of the 2nd layer and the $j$th neuron of the 3rd layer.\n"," Then, the vector of activations of the 3rd layer is given by\n"," $$ \\mathbf{a}' = \\sigma(\\mathbf{W} \\mathbf{a} + \\mathbf{b})$$\n"," where $\\mathbf{a}, \\mathbf{b}$ is the vector of activations and biases of the 2nd layer,\n"," $(\\mathbf{W})_{jk} = w_{jk}$ is the weight matrix, and $\\sigma(z)$ is the activation function.\n","\n"," Let's then add a `feedforward` method to the `Network` class to compute the output when\n"," given an input.\n","Return the output of the network if \"a\" is input."],"metadata":{}},{"cell_type":"markdown","source":[" Now add an stochastic gradient descent (`SGD`) method to `Network`\n"," for it to learn.\n","    Train the neural network using mini-batch stochastic gradient descent (SGD). \n","\n","    train_data -- a list of tuples (x, y) representing the training inputs and the desired outputs. \n","    epochs -- number of epochs to train for\n","    mini_batch_size -- size of mini-batches to use for sampling\n","    eta -- learning rate\n","\n","    If ``test_data`` is provided, then it will be evaluated by the network \n","    after each epoch and partial progress will be printed out \n","    (useful for tracking progress but slow)."],"metadata":{}},{"cell_type":"markdown","source":[" A function for updating the network weights and biases after each iteration of SGD\n","    Update the network weights and biases after iteration of stochastic gradient descent (SGD). \n","    \n","    mini_batch --- list of tuples\n","    eta --- learning rate\n"," Different data definition"],"metadata":{}},{"cell_type":"markdown","source":[" `backprop` refers to the *backpropagation* algorithm.\n","\n"," Let's add the other helper functions:\n"," Backpropagation\n","    Return a tuple (nabla_a, nabla_w) representing the gradient of the cost function C_x, \n","    where nabla_a and nabla_w are layer-by-layer lists of numpy arrays \n","    (like self.biases and self.weights).\n"," Feedforward\n"," Backward pass\n"," l = 1 -- last layer of neurons\n"," l = 2 -- etc.\n"," Test function\n","    Return the number of test inputs for which the network outputs the correct results \n","    (output = index of node with high activation).\n","test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n"," Different data definition\n","    Return the vector of partial derivatives (partial C_x / partial a) \n","    for the output activations.\n"," Derivative of sigmoid function\n","Derivative of the `sigmoid` function."],"metadata":{}},{"cell_type":"markdown","source":[" Reshape train_lab to fit function -> list containing N 2-tuples (x, y)\n"," where x = 784-d ndarray and y = 10-d ndarray for the label."],"metadata":{}},{"cell_type":"markdown","source":[" Training:"],"metadata":{}},{"cell_type":"markdown","source":[" ---\n"," Try with code from [MichalDanielDobrzanski](https://github.com/MichalDanielDobrzanski/DeepLearningPython35)"],"metadata":{}},{"source":["import network\n","net = network.Network([784, 30, 10])\n","net.SGD(train_data, 5, 10, 3.0, test_data = test_data)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}