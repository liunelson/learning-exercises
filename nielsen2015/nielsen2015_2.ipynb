{"cells":[{"cell_type":"markdown","source":[" # Notes and exercises from [Nielsen2015](http://neuralnetworksanddeeplearning.com/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 2 Backpropagation\n","\n"," ## Sec. 2.1 Matrix Notation\n","\n"," $w_{ij}^{k}$: *weight* for the connection to the $i^\\mathrm{th}$ neuron\n"," in the $k^\\mathrm{th}$ layer from the $j^\\mathrm{th}$ neuron\n"," in the $(k-1)^\\mathrm{th}$ layer.\n","\n"," $b_i^k, z_i^k, a_i^k$: *bias*, *weighted output*, *activation*\n"," for the $i^\\mathrm{th}$ neuron in the $k^\\mathrm{th}$ layer.\n","\n"," Therefore:\n","\n"," $ \\begin{equation}\n","   z_i^k = \\sum\\limits_j w_{ij}^k a_j^{k-1} + b_i^k \\\\\n","   a_i^k = \\sigma (z_i^k)\n"," \\end{equation} $\n","\n"," or in matrix notation:\n","\n"," $ \\begin{equation}\n","   \\mathbf{z}_k = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k \\\\\n","   \\mathbf{a}_k = \\sigma (\\mathbf{z}_k)\n"," \\end{equation} $\n","\n"," where $(\\mathbf{a}_k)_i = a_i^k$ etc.\n"," and $\\mathbf{W}_k$ is the $N_k \\times N_{k-1}$ weight matrix."],"metadata":{}},{"cell_type":"markdown","source":[" ## Sec. 2.2 Cost Function\n","\n"," Assume a *cost function* of the form:\n","\n"," $C = \\frac{1}{n} \\sum\\limits_x C_x $\n"," and $C_x = \\frac{1}{2}| \\mathbf{y}(\\mathbf{x}) - \\mathbf{a}_N (x)|^2 $\n","\n"," where $(\\mathbf{x}, \\mathbf{y})$ is one of $n$ training example pairs\n"," and $\\mathbf{a}_N(\\mathbf{x})$ is the activation of the last or $N^\\mathrm{th}$ layer of the network."],"metadata":{}},{"cell_type":"markdown","source":[" ## Sec. 2.3 The Hadamard Product\n","\n"," Define $\\odot$ as the Hadamard/Schur/element-wise matrix product:\n","\n"," ($\\mathbf{u} \\odot \\mathbf{v})_i = u_i v_i$"],"metadata":{}},{"cell_type":"markdown","source":[" ## Sec. 2.4 Equations of Backpropagation\n","\n"," Define $\\delta_i^k = \\frac{\\partial C}{\\partial {z_i^k}}$ as *error* in the $i^\\mathrm{th}$ neuron\n"," of the $k^\\mathrm{th}$ layer.\n"," ---\n"," **Equation 1**:\n","\n"," $ \\begin{equation}\n","   \\delta_i^N = \\frac{\\partial C}{\\partial a_i^N} \\sigma^\\prime(z_i^N) \\\\\n","   \\boldsymbol{\\delta}_N = \\nabla_{a_N} C \\odot \\sigma^\\prime (\\mathbf{z}_N)\n"," \\end{equation} $\n","\n"," where $\\nabla_{\\mathbf{a}_N} C = \\mathbf{a}_N - \\mathbf{y} $ and $\\sigma^\\prime(z) = \\frac{\\mathrm{d} \\sigma}{\\mathrm{d} z}$.\n"," ---\n"," **Equation 2**:\n","\n"," $ \\begin{equation}\n","   \\boldsymbol{\\delta}_k = \\left( W_{k+1}^\\mathsf{T} \\boldsymbol{\\delta}_{k+1} \\right)\n","       \\odot \\sigma^\\prime(z_i^N) \\\\\n"," \\end{equation} $\n","\n"," where the error propagates backwards from the output layer.\n"," ---\n"," **Equation 3**:\n","\n"," $ \\begin{equation}\n","   \\frac{\\partial C}{\\partial b_i^k} = \\delta_i^k\n"," \\end{equation} $\n"," ---\n"," **Equation 4**:\n","\n"," $ \\begin{equation}\n","   \\frac{\\partial C}{\\partial w_{ij}^k} = \\delta_i^k a_j^{k-1}\n"," \\end{equation} $\n"," ---\n"," Errors can be calculated by propagating Eq. 1 backwards with Eq. 2.\n","\n"," Since $\\sigma^\\prime(z) \\sim \\mathrm{e}^{-z^2}$,\n"," Eq. 1 means that errors vary little # when $|z| \\gg 0$\n"," or the activation of the output neurons is near $0$ or $1$.\n"," Thus, biases (Eq. 3) learn slowly when the output neuron is saturated\n"," and weights (Eq. 4) learn slowly when the output neuron is saturated\n"," or the input neuron is low-activated."],"metadata":{}},{"cell_type":"markdown","source":[" ## Sec. 2.5 The Backpropagation Algorithm\n","\n"," 1. Input $\\mathbf{x}$ and set $\\mathbf{a}_1 = \\sigma(\\mathbf{x})$\n"," 2. Feedforward: $\\mathbf{z}_k = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k$ and $\\mathbf{a}_k = \\sigma(\\mathbf{z}_k)$\n"," 3. Output error $\\mathbf{\\delta}_N = \\nabla_{\\mathbf{a}_N} C \\odot \\sigma^\\prime(\\mathbf{z}_N)$\n"," 4. Backpropagate: $\\mathbf{\\delta}_k = \\left( W_{k+1}^\\mathsf{T} \\mathbf{\\delta}_{k+1} \\right) \\odot \\sigma^\\prime(\\mathbf{z}_k)$\n"," 5. Output: $\\frac{\\partial C}{\\partial w_{ij}^k} = \\delta_i^k a_j^{k-1}$\n","   and $\\frac{\\partial C}{\\partial b_i^k} = \\delta_i^k$\n","\n","\n"," Combine this with stochastic gradient descent:\n","\n"," 1. Input is a mini-batch of M training examples: $\\mathbf{X} = \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_M \\}$\n"," 2. For each $\\mathbf{x}_m$, apply the backprop. algorithm (steps 1 to 5)\n"," 3. Update weights and biases:\n","   $\\mathbf{W}_k \\rightarrow \\mathbf{W}_k - \\frac{\\eta}{M} \\sum\\limits_m \\mathbf{\\delta}_k(\\mathbf{x}_m) \\otimes \\mathbf{a}_{k-1}(\\mathbf{x}_m)$\n","   and\n","   $\\mathbf{b}_k \\rightarrow \\mathbf{b}_k - \\frac{\\eta}{M} \\sum\\limits_m \\mathbf{\\delta}_k (\\mathbf{x}_m)$\n","\n"," Note that one use matrix notation to do all the $\\mathbf{x}_m$ at once:\n","\n"," $ \\begin{equation}\n","   \\mathbf{X} = [ \\mathbf{x}_1 \\ldots \\mathbf{x}_M] \\\\\n","   \\mathbf{A}_1 = \\sigma(\\mathbf{X}) \\\\\n","   \\mathbf{Z}_k = \\mathbf{W}_k \\mathbf{A}_{k-1} + \\mathbf{B}_k \\\\\n","   \\mathbf{\\Sigma}_N = \\nabla_{\\mathbf{a}_N}C(\\mathbf{A}_N) \\odot \\sigma^\\prime(\\mathbf{Z}_N)\n"," \\end{equation} $"],"metadata":{}},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}