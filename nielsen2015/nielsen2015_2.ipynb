{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Notes and exercises from [Nielsen2015](http://neuralnetworksanddeeplearning.com/)\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Ch. 2 Backpropagation\n",
    "\n",
    " ## Sec. 2.1 Matrix Notation\n",
    "\n",
    " $w_{ij}^{k}$: *weight* for the connection to the $i^\\mathrm{th}$ neuron\n",
    " in the $k^\\mathrm{th}$ layer from the $j^\\mathrm{th}$ neuron\n",
    " in the $(k-1)^\\mathrm{th}$ layer.\n",
    "\n",
    " $b_i^k, z_i^k, a_i^k$: *bias*, *weighted output*, *activation*\n",
    " for the $i^\\mathrm{th}$ neuron in the $k^\\mathrm{th}$ layer.\n",
    "\n",
    " Therefore:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   z_i^k = \\sum\\limits_j w_{ij}^k a_j^{k-1} + b_i^k \\\\\n",
    "   a_i^k = \\sigma (z_i^k)\n",
    " \\end{equation} $\n",
    "\n",
    " or in matrix notation:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\mathbf{z}_k = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k \\\\\n",
    "   \\mathbf{a}_k = \\sigma (\\mathbf{z}_k)\n",
    " \\end{equation} $\n",
    "\n",
    " where $(\\mathbf{a}_k)_i = a_i^k$ etc.\n",
    " and $\\mathbf{W}_k$ is the $N_k \\times N_{k-1}$ weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sec. 2.2 Cost Function\n",
    "\n",
    " Assume a *cost function* of the form:\n",
    "\n",
    " $C = \\frac{1}{n} \\sum\\limits_x C_x $\n",
    " and $C_x = \\frac{1}{2}| \\mathbf{y}(\\mathbf{x}) - \\mathbf{a}_N (x)|^2 $\n",
    "\n",
    " where $(\\mathbf{x}, \\mathbf{y})$ is one of $n$ training example pairs\n",
    " and $\\mathbf{a}_N(\\mathbf{x})$ is the activation of the last or $N^\\mathrm{th}$ layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sec. 2.3 The Hadamard Product\n",
    "\n",
    " Define $\\odot$ as the Hadamard/Schur/element-wise matrix product:\n",
    "\n",
    " ($\\mathbf{u} \\odot \\mathbf{v})_i = u_i v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sec. 2.4 Equations of Backpropagation\n",
    "\n",
    " Define $\\delta_i^k = \\frac{\\partial C}{\\partial {z_i^k}}$ as *error* in the $i^\\mathrm{th}$ neuron\n",
    " of the $k^\\mathrm{th}$ layer.\n",
    "\n",
    " ---\n",
    "\n",
    " **Equation 1**:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\delta_i^N = \\frac{\\partial C}{\\partial a_i^N} \\sigma^\\prime(z_i^N) \\\\\n",
    "   \\boldsymbol{\\delta}_N = \\nabla_{a_N} C \\odot \\sigma^\\prime (\\mathbf{z}_N)\n",
    " \\end{equation} $\n",
    "\n",
    " where $\\nabla_{\\mathbf{a}_N} C = \\mathbf{a}_N - \\mathbf{y} $ and $\\sigma^\\prime(z) = \\frac{\\mathrm{d} \\sigma}{\\mathrm{d} z}$.\n",
    "\n",
    " ---\n",
    "\n",
    " **Equation 2**:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\boldsymbol{\\delta}_k = \\left( W_{k+1}^\\mathsf{T} \\boldsymbol{\\delta}_{k+1} \\right)\n",
    "       \\odot \\sigma^\\prime(z_i^N) \\\\\n",
    " \\end{equation} $\n",
    "\n",
    " where the error propagates backwards from the output layer.\n",
    "\n",
    " ---\n",
    "\n",
    " **Equation 3**:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\frac{\\partial C}{\\partial b_i^k} = \\delta_i^k\n",
    " \\end{equation} $\n",
    "\n",
    " ---\n",
    "\n",
    " **Equation 4**:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\frac{\\partial C}{\\partial w_{ij}^k} = \\delta_i^k a_j^{k-1}\n",
    " \\end{equation} $\n",
    "\n",
    " ---\n",
    "\n",
    " Errors can be calculated by propagating Eq. 1 backwards with Eq. 2.\n",
    "\n",
    " Since $\\sigma^\\prime(z) \\sim \\mathrm{e}^{-z^2}$,\n",
    " Eq. 1 means that errors vary little # when $|z| \\gg 0$\n",
    " or the activation of the output neurons is near $0$ or $1$.\n",
    " Thus, biases (Eq. 3) learn slowly when the output neuron is saturated\n",
    " and weights (Eq. 4) learn slowly when the output neuron is saturated\n",
    " or the input neuron is low-activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sec. 2.5 The Backpropagation Algorithm\n",
    "\n",
    " 1. Input $\\mathbf{x}$ and set $\\mathbf{a}_1 = \\sigma(\\mathbf{x})$\n",
    " 2. Feedforward: $\\mathbf{z}_k = \\mathbf{W}_k \\mathbf{a}_{k-1} + \\mathbf{b}_k$ and $\\mathbf{a}_k = \\sigma(\\mathbf{z}_k)$\n",
    " 3. Output error $\\mathbf{\\delta}_N = \\nabla_{\\mathbf{a}_N} C \\odot \\sigma^\\prime(\\mathbf{z}_N)$\n",
    " 4. Backpropagate: $\\mathbf{\\delta}_k = \\left( W_{k+1}^\\mathsf{T} \\mathbf{\\delta}_{k+1} \\right) \\odot \\sigma^\\prime(\\mathbf{z}_k)$\n",
    " 5. Output: $\\frac{\\partial C}{\\partial w_{ij}^k} = \\delta_i^k a_j^{k-1}$\n",
    "   and $\\frac{\\partial C}{\\partial b_i^k} = \\delta_i^k$\n",
    "\n",
    "\n",
    " Combine this with stochastic gradient descent:\n",
    "\n",
    " 1. Input is a mini-batch of M training examples: $\\mathbf{X} = \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_M \\}$\n",
    " 2. For each $\\mathbf{x}_m$, apply the backprop. algorithm (steps 1 to 5)\n",
    " 3. Update weights and biases:\n",
    "   $\\mathbf{W}_k \\rightarrow \\mathbf{W}_k - \\frac{\\eta}{M} \\sum\\limits_m \\mathbf{\\delta}_k(\\mathbf{x}_m) \\otimes \\mathbf{a}_{k-1}(\\mathbf{x}_m)$\n",
    "   and\n",
    "   $\\mathbf{b}_k \\rightarrow \\mathbf{b}_k - \\frac{\\eta}{M} \\sum\\limits_m \\mathbf{\\delta}_k (\\mathbf{x}_m)$\n",
    "\n",
    " Note that one use matrix notation to do all the $\\mathbf{x}_m$ at once:\n",
    "\n",
    " $ \\begin{equation}\n",
    "   \\mathbf{X} = [ \\mathbf{x}_1 \\ldots \\mathbf{x}_M] \\\\\n",
    "   \\mathbf{A}_1 = \\sigma(\\mathbf{X}) \\\\\n",
    "   \\mathbf{Z}_k = \\mathbf{W}_k \\mathbf{A}_{k-1} + \\mathbf{B}_k \\\\\n",
    "   \\mathbf{\\Sigma}_N = \\nabla_{\\mathbf{a}_N}C(\\mathbf{A}_N) \\odot \\sigma^\\prime(\\mathbf{Z}_N)\n",
    " \\end{equation} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
