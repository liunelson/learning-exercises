{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes and exercises from [Nielsen2015](http://neuralnetworksanddeeplearning.com/)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Ch. 6 Deep Learning\n","\n"," Deep neural networks are hard to train due to inherent instability of\n"," the gradient-descent algorithm by backpropagation.\n","\n"," Let's consider ways to fix this problem."],"metadata":{}},{"cell_type":"markdown","source":[" ### Sec. 6.1 Convolutional Neural Networks\n","\n"," Previously, a fully connected neural network was used\n"," for the MNIST handwritten digit classification problem.\n","\n"," However, it is not very physical since it treated nearby pixels\n"," the same as far-apart pixels.\n","\n"," Solution: a *convolutional neural network* to account spatial structure.\n","\n"," - *Local receptive fields*:\n"," local subsets of units to which a single hidden unit is connected;\n"," it is defined by the *kernel size* (side length of subregion),\n"," *padding size* (length of padding outside), and\n"," *stride size* (step size of convolution).\n","\n"," e.g. a MNIST image has $N_\\textrm{in} \\times N_\\textrm{in}$ pixels;\n"," assuming a local receptive field with kernel size $K$, padding size $P = 0$,\n"," and stride size $S = 1$,\n"," then, the first hidden layer has $N_\\textrm{out} \\times N_\\textrm{out}$ units.\n","\n"," $N_\\textrm{out} = \\left \\lfloor \\frac{N_\\textrm{in} + 2P - K}{S} \\right \\rfloor + 1$\n","\n"," - *Shared weights and biases*:\n"," All the hidden units use the same weights and biases.\n","\n"," The output of the $(j,k)$-th hidden unit is\n"," $\\sigma\\left( \\sum\\limits_{l,m = 0}^K w_{l,m} \\: a_{j+l,k+m} + b\\right)$\n","\n"," The map from the input layer to the hidden layer is known as\n"," a *feature map* with some *kernel* or *filter* defined by *shared weights* $w$\n"," and *shared bias* $b$.\n"," There may be multiple feature maps defined by sets of $w$ and $b$,\n"," each detecting different kinds of features (that are translationally invariant).\n","\n"," Overall, the number of parameters ($w$, $b$) are greatly reduced.\n"," For each feature map, there are only $N_\\textrm{out}^2 + 1$.\n"," A fully connected network would have $(N_\\textrm{in}^2 + 1) N_1$,\n"," where $N_1$ is the number of hidden units.\n","\n"," *Convolutional* because activation can be written as\n"," $a^1 = \\sigma(b + w \\astar a^0)$.\n","\n"," - *Pooling layers*:\n"," Layer that comes after the convolutional layer.\n","\n"," A pooling layer downsamples the previous output,\n"," mapping several nearby hidden units to a single pooling unit,\n"," as a mean to reduce the number of parameters for later layers.\n","\n"," A common technique is *max-pooling*,\n"," wherein the output is simply the maximum activation value\n"," in a given subregion (e.g. $2 \\times 2$).\n","\n"," Another technique is *$L^2$ pooling*,\n"," wherein the $L^2$ norm is taken.\n","\n"," Two hyperparameters: pool size (side length of the pooled region)\n"," and stride size (step size).\n"," If pool size $>$ stride size, it is called *overlapping* pooling.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" An example of a convolutional neural network for the MNIST problem:\n","\n"," 1. Input layer with $28 \\times 28$ units, one for each image pixel\n"," 2. Convolutional layer ($5 \\times 5$ kernel, three feature maps) with $24 \\times 24 \\times 3$ outputs\n"," 3. Pooling layer ($2 \\times 2$ kernel) with $12 \\times 12 \\times 3$ units\n"," 4. Output layer (fully connected, $12 \\times 12 \\times 3 \\times 10$ weights and $10$ biases) with 10 units\n","\n"," Training is done as previously, using stochastic gradient descent\n"," and backpropagation.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### Sec. 6.2 Suggestions\n","\n"," *Rectifield Linear Units*:\n"," Use ReLUs ($\\sigma(z) = \\max(0, z)$) instead of\n"," sigmoid (or tanh) activation units.\n"," It is empirically shown to improve performance of CNNs.\n","\n"," *Expanded Training Dataset*:\n"," Generate additional training examples by distorting existing ones,\n"," through translation, rotation, skewing, and *elastic distortion*\n"," (see [Simard, SteinKraus, and Platt](https://ieeexplore.ieee.org/document/1227801)).\n","\n"," *Dropout*:\n"," Randomly drop out activation units in the fully-connected layer\n"," to prevent co-adaption amongst all units.\n"," Not necessary for the convolutional layers.\n","\n"," *Ensemble of Networks*:\n"," Train multiple neural networks which have different random seeds\n"," (in the initialization and dropout steps)\n"," and use the ensemble average output.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### Sec. 6.3 Other Approaches\n","\n"," *Recurrent Neural Networks* (RNNs):\n"," An unit's activation may depend on both the activation\n"," of units in the previous layer and its own activation\n"," at an earlier time.\n","\n"," *Long Short-Term Memory Units* (LSTMs):\n"," Deep RNNs are difficult to train due to the 'unstable gradient' problem\n"," (backpropagation through layers *and* time);\n"," a solution is to include LSTMs into the network.\n","\n"," *Deep Belief Networks* (DBNs):\n"," DBNs are an example of a *generative* model,\n"," wherein an output is specified and an input is generated;\n"," they can also do unsupervised and semi-supervised learning;\n"," currently unpopular.\n",""],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}