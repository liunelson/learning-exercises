{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes and exercises from [Statlect](https://www.statlect.com)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" # Part 4 - [Asymptotic Theory](https://www.statlect.com/asymptotic-theory/)"],"metadata":{}},{"cell_type":"markdown","source":[" ## Sequences of Random Variables\n","\n"," Let $\\{X_n\\}$ be a sequence of random variables.\n","\n"," Terminology:\n"," - The sequence $\\{x_n\\}$ is a *realization* of it $\\{X_n\\}$\n"," if $x_n$ is a realization of $X_n$.\n","\n"," - It is a sequence *on* sample space $\\Omega$\n"," if all $X_n$ are functions $\\Omega \\rightarrow \\mathbb{R}$.\n","\n"," - It is an *independent* sequence (or a sequence of independent random variables)\n"," if every finite subset of it is a set of mutually independent random variables.\n","\n"," - It is an *identically distributed* sequence if\n"," any two elements $X_i, X_j$ have the same CDF.\n","\n"," - It is an *IID* sequence if it has both of the above properties.\n","\n"," - It is *stationary* (or strictly stationary) if\n"," two random vectors $[X_{n+1}, \\ldots, X_{n+q}]$ and $[X_{n+k+1}, \\ldots, X_{n+kq}]$\n"," have the same joint CDF. Corollary: IID $\\Rightarrow$ stationary.\n","\n"," - It is *covariance stationary* (or weakly stationary) if\n"," all $X_n$ have the same mean $\\mathrm{E}[X_n] = \\mu$ and variance $\\mathrm{var}[X_n] = \\sigma^2$;\n"," the latter condition means $\\forall \\: j, \\: \\exists \\: \\gamma_i \\in \\mathbb{R} \\: : \\: \\mathrm{cov}[X_n, X_{n-j}] = \\gamma_j \\: \\forall \\: n < j$.\n","\n"," - It is *mixing* (or strongly mixing) if\n"," two groups of terms that are far apart are asymptotically independent,\n"," i.e. $\\lim\\limits_{k\\rightarrow \\infty} \\left(\\mathrm{E}[f(\\boldsymbol{Y}) g(\\boldsymbol{Z})] - \\mathrm{E}[f(\\boldsymbol{Y})]\\mathrm{E}[g(\\boldsymbol{Z})] \\right) = 0$\n"," for $Y = [X_{n+1}, \\ldots, X_{n+q}]$, $Z = [X_{n+k+1}, \\ldots, X_{n+k+q}]$\n"," and any function $f, g$ and any $n, q \\in \\mathbb{N}$.\n"," Corollary: independent $\\Rightarrow$ mixing.\n","\n"," - It is *ergodic* if either $P(\\{X_n\\} \\in A) = 0$ or $P(\\{X_n\\} \\in A) = 1$\n"," where $A$ is a *shift-invariant* set. 'Ergodic' is weaker than 'mixing'.\n","\n"," - A subset $A$ is shift invariant  if $\\{x_n\\} in A \\; \\Rightarrow \\; \\{x_n\\}_{n>1} in A$,\n"," where $\\{x_n\\}_{n>1} = \\{x_2, x_3, \\ldots\\}$.\n","\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Modes of Stochastic Convergence\n","\n"," ### Pointwise Convergence:\n"," A sequence of random variables $\\{X_n\\}$ defined on sample space $\\Omega$\n"," is *pointwise convergent* to a random variable $X$ on $\\Omega$\n"," if $\\{X_n(\\omega)\\}$ converges to $X(\\omega) \\; \\forall \\: \\omega \\in \\Omega$.\n"," $X$ is called the *pointwise limit* of the sequence.\n","\n"," Example:\n"," - Let $\\Omega = \\{\\omega_1, \\omega_2\\}$ be a sample space with sample points $\\omega_1, \\omega_2$.\n"," - Let $\\{X_n\\}$ be a sequence of random variables\n"," such that $X_n(\\omega) = \\begin{cases} \\frac{1}{n} & \\omega = \\omega_1 \\\\ 1 + \\frac{2}{n} & \\omega = \\omega_2 \\end{cases}$\n"," - Since $X_n(\\omega_1) \\rightarrow 0$ and $X_n(\\omega_2) \\rightarrow 1$,\n"," $\\{X_n\\}$ is pointwise convergent to $X$, where $X = \\begin{cases} 0 & \\omega = \\omega_1 \\\\  1 & \\omega = \\omega_2 \\end{cases}$\n","\n","\n"," For a sequence of random vectors, the standard criterion for convergence is just\n"," $\\lim\\limits_{n \\rightarrow \\infty} d(\\boldsymbol{X}_n(\\omega), \\boldsymbol{X}(\\omega)) = 0$\n"," where $d$ is the Euclidean distance.\n","\n"," ### Almost-Sure Convergence:\n"," A weakened version of pointwise convergence.\n"," $\\{X_n\\}$ is almost-surely convergent if $\\{X_n(\\omega)\\}$ converges for most $\\omega \\in \\Omega$\n"," except those belonging in some subset $\\subseteq E$, where $E$ is a zero-probability event.\n","\n"," ### Convergence in Probability:\n"," A sequence of random variable $\\{X_n\\}$ is *convergent in probability* to random variable $X$\n"," if there is a high probability that their difference is very small,\n"," i.e. $\\lim\\limits_{n \\rightarrow \\infty} P(|X_n - X| > \\epsilon) = 0$ for any $\\epsilon > 0$.\n","\n"," Example:\n"," - Let $X$ be a discrete random variable with support $R_X = \\{0,1\\}$\n"," and PMF $p_X(x) = \\begin{cases} \\frac{1}{3} & x = 1 \\\\ \\frac{2}{3} & x = 0 \\\\ 0 & \\textrm{otherwise} \\end{cases}$.\n"," - Consider the sequence $\\{X_n\\}$ with terms $X_n = \\left(1 + \\frac{1}{n} \\right) X$.\n"," - Since $|X_n - X| = \\frac{1}{n} X$, $|X_n - X| = 0$ for $X = 0$ (with probability $\\frac{2}{3}$)\n"," and $|X_n - X| = \\frac{1}{n}$ (with probability $\\frac{1}{3}$).\n"," - Thus,\n"," $P(|X_n - X| \\leq \\epsilon) = \\begin{cases} \\frac{2}{3} & \\frac{1}{n} < \\epsilon \\\\ 1 & \\textrm{otherwise} \\end{cases}$\n"," - Let's pick an arbitrarily small $\\epsilon$.\n"," - since $P(|X_n - X| > \\epsilon) = 1 - P(|X_n - X| \\leq \\epsilon)$, $P(|X_n - X| > \\epsilon) = 0 \\: \\forall \\: \\epsilon > 0$.\n","\n","\n"," ### Mean-Square Convergence:\n"," A sequence $\\{X_n\\}$ is *mean-square convergent* to $X$\n"," if $\\lim\\limits_{n \\rightarrow \\infty} d(X_n, X)  = 0$\n"," where $d(X_n, X) = \\mathrm{E}[(X_n - X)^2]$.\n","\n"," ### Convergence in Distribution:\n"," A sequence of random variable $\\{X_n\\}$ is *convergent in distribution* (or in law)\n"," to random variable $X$ if their CDFs are 'close',\n"," i.e. $\\lim\\limits_{n \\rightarrow \\infty} F_n(x) = F_X(x)$ for all points $x in \\mathbb{R}$.\n","\n"," Example:\n"," - Let $\\{X_n\\}$ be IID where each term has an uniform distribution on $[0,1]$.\n"," - Thus, $F_{X_n}(x) = \\begin{cases} 0 & x < 0 \\\\ x & 0 \\leq x < 1\\\\ 1 & x \\geq 1 \\end{cases}$.\n"," - Consider the sequence $\\{Y_n\\}$ wjere $Y_n = n \\left(1 - \\max\\limits_{1 \\leq i \\leq n}X_i \\right)$.\n"," - Is $\\{Y_n\\}$ convergent in distribution?\n"," - The CDF of $Y_n$ is:\n"," $\\begin{align} F_{Y_n}(y) &= P(Y_n \\leq y) \\\\ &= \\ldots \\\\ &= 1 - F_{X_n}\\left(1 - \\frac{y}{n}\\right)^n \\end{align}$\n"," - $F_{Y_n}(y) = \\begin{cases} 0 & y < 0 \\\\ 1 - \\left(1 - \\frac{y}{n} \\right)^n & 0 \\leq y < n\\\\ 1 & y \\geq n \\end{cases}$\n"," - $\\lim\\limits_{n \\rightarrow \\infty} F_{Y_n}(y) = F_Y(y) = \\begin{cases} 0 & y < 0 \\\\ 1 - \\mathrm{e}^{-y} & y \\geq 0 \\end{cases}$\n"," - $F_Y(y)$ can be shown to be a proper CDF (right-continuous, $-\\infty$ limit is 0, $+\\infty$ limit is 1).\n"," - Therefore, $\\{Y_n\\}$ converges in distribution to $Y$, an exponential random variable.\n","\n","\n"," ### Relations between the Modes of Convergence:\n"," Almost-sure convergence, mean-square convergence $\\Rightarrow$ Convergence in probability $\\Rightarrow$ Convergence in distribution.\n","\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Laws of Large Numbers (LLNs)\n","\n"," Let $\\{X_n\\}$ be a sequence of random variables\n"," and $\\bar{X}_n$ the sample mean of the first $n$ terms,\n"," $\\bar{X}_n = \\frac{1}{n} \\sum\\limits_{i = 1}^n X_i$\n","\n"," A LLN states sufficient conditions that guarantees convergence of $\\bar{X}_n$ to a constant as $n$ increases.\n","\n"," A LLN is *weak* if $\\bar{X}_n$ converges in probability,\n"," *strong* if $\\bar{X}_n$ converges almost surely.\n","\n"," ### Chebyshev's WLLN:\n"," - Let $\\{X_n\\}$ be uncorrelated and covariance stationary.\n"," - Then, $\\bar{X}_n$ converges in mean square and thus in probability to $\\mu$,\n"," i.e. $\\lim\\limits_{n \\rightarrow \\infty} P(|\\bar{X}_n - \\mu| > \\epsilon) = 0$\n","\n","\n"," ### Chebyshev's WLLN for correlated sequences:\n"," - Let $\\{X_n\\}$ be covariance stationary\n"," but $\\lim\\limits_{n \\rightarrow \\infty} \\frac{1}{n} \\sum\\limits_{i = 0}^n \\mathrm{cov}[X_n,X_{n-i}] = 0$.\n"," - Then, Chebyshev's WLLN applies.\n","\n","\n"," ### Kolmogorov's SLLN:\n"," - Let $\\{X_n\\}$ be IID with finite mean $\\mathrm{E}[X_n] = \\mu < \\infty \\: \\forall \\: n \\in \\mathbb{N}$.\n"," - Then, $\\bar{X}_n$ converges almost surely to $\\mu$.\n","\n","\n"," ### Ergodic Theory:\n"," - Let $\\{X_n\\}$ be just statioanry and ergodic with finite mean\n"," - Then, a SLLN applies to $\\bar{X}_n$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Central Limit Theorems (CLTs)\n","\n"," CLTs state conditions for the distribution of some function of the sample mean\n"," to converge to a standard normal distribution as the sample size increases.\n","\n"," ### Lindeberg-Levy CLT:\n"," - Let $\\{X_n\\}$ be IID such that finite expectation value $\\mu$ and variance $\\sigma^2 > 0$ for all n\n"," - Then, $\\sqrt{n} \\left(\\frac{\\bar{X}_n - \\mu}{\\sigma} \\right)$\n"," converges in distribution to some standard normal random variable $Z$.\n","\n","\n"," Example 1:\n"," - Let $\\{X_n\\}$ be a sequence of Bernoulli random variables with $p = \\frac{1}{2}$.\n"," - i.e. $R_{X_n} = \\{0,1\\}$ and $p_{X_n}(x) = \\begin{cases} p & x = 1 \\\\ 1-p & x = 0 \\\\ 0 & otherwise \\end{cases}$.\n"," - Use a CLT to find the distribution for the mean of the 1st 100 terms.\n","\n","\n"," - The sequence is IID and the mean of a generic term is $\\mathrm{E}[X_n] = \\sum\\limits_{x in R_{X_n}} x \\: p_{X_n}(x) = p < \\infty$.\n"," - The variance of a generic term is $\\mathrm{var}[X_n] = \\ldots = p - p^2 = \\frac{1}{4} < \\infty$.\n"," - Thus, the Lindeberg-Levy CLT applies.\n"," - $\\bar{X}_100 = \\frac{1}{100} \\sum\\limits_{i = 1}^{100} X_i$\n"," has a distribution that is normal with $\\mu = \\mathrm{E}[X_n] = \\frac{1}{2}$\n"," and $\\sigma = \\frac{\\mathrm{var}[X_n]}{n} = \\frac{1}{400}$.\n","\n","\n"," Example 2:\n"," - Let $Y$ be a binomial random variable with $n = 100, p = \\frac{1}{2}$.\n"," - Note that $Y = \\sum\\limits_{i = 1}^100 X_i$\n"," where $X_i$ are mutually independent Bernoulli random variables wih $p = \\frac{1}{2}$.\n"," Since $\\bar{X}_{100} \\sim N(\\mu = \\frac{1}{2},\\sigma^2 = \\frac{1}{400})$,\n"," $Y \\sim N(\\mu = \\frac{100}{2}, \\sigma^2 = \\frac{100^2}{400}) = N(\\mu = 50, \\sigma^2 = 25)$."],"metadata":{}},{"cell_type":"markdown","source":[" ## Continuous Mapping Theorem\n","\n"," Stochastic convergence of some sequence of random vectors ${\\boldsymbol{X}_n}$ to\n"," a random vector $\\boldsymbol{X}$ is preserved\n"," if function $g:\\mathbb{R}^K \\rightarrow \\mathbb{R}^L$ is a continuous function.\n","\n"," That is, $g(\\boldsymbol{X}_n)$ also converges to $g(\\boldsymbol{X})$\n"," in probability, in distribution or almost surely.\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" ## Empirical Distribution\n","\n"," Let $\\xi_n = [x_1 \\: \\ldots \\: x_n]$ be a sample of size $n$ of some variable $X$;\n"," $x_i$ is $i$-th observation from the sample.\n"," The *empirical* distribution of $\\Xi_n$ is the CDF of $X$ and it is defined as\n"," $F_n(x) = \\frac{1}{n}\\sum\\limits_{i=1}^n 1_{\\{x_i \\leq x \\}}$.\n","\n"," In effect, this assigns a probability $\\frac{1}{n}$ to each value $x_i$.\n","\n"," ### The Plug-In Principle:\n"," A feature of a given distribution can be approximated by the same feature of the empirical distribution\n"," of a sample of observations drawn from the given distribution.\n","\n"," ### The Monte Carlo Method:\n"," This is a computational method that uses a generated sample from a given probability distribution\n"," to produce a plug-in estimate of some of its features, e.g. a moment or a quantile.\n","\n"," - Let $X$ be a random variable with CDF $F_X(x)$.\n"," - Generate a sample $\\xi_n = [x_1 \\: \\ldots \\: x_n]$ of realizations.\n"," - Denote a feature of $F_X$ (mean, variance, etc.) by $T(F_X)$.\n"," - Denote the empirical distribution of $\\xi_n$ by $F_n(x)$.\n"," - Then, $T(F_n)$ is a *Monte Carlo* approximation of $T(F_X)$.\n","\n","\n"," Inverse Transformation Method:\n"," If $U$ is a pseudo-random number having an uniform distribution on $[0,1]$ and\n"," $F_X$ is an invertible CDF, then $X = F_X^{-1}(U)$ has CDF $F_X$.\n"," (see pseudo-random number sampling).\n","\n",""],"metadata":{}},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}