{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Notes and exercises from [Statlect](https://www.statlect.com)\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 5 - [Fundamentals of Statistics](https://www.statlect.com/fundamentals-of-statistics/)\n",
    "\n",
    " ## Statistical Inference\n",
    "\n",
    " The act of using observed data (the *sample*) to infer unknown features\n",
    " of the underlying probability distribution.\n",
    "\n",
    " A sample is the vector of realizations $x_1, \\ldots, x_n$\n",
    " of $n$ independent random variables $X_1, \\ldots, X_n$\n",
    " having a common distribution function $F_X(x)$.\n",
    "\n",
    " In other words, the sample $\\xi = [x_1 \\: \\ldots \\: x_n]$ is\n",
    " the realization of a random vector $\\Xi = [X_1 \\: \\ldots \\: X_n]$\n",
    " with joint CDF $F_\\Xi(\\xi) = F_X(x_1) \\ldots F_X(x_n)$.\n",
    "\n",
    " An individual realization $x_i$ is known as an *observation* from the sample.\n",
    "\n",
    " A *statistical model* (or *model specification* or just *model*)\n",
    " is a set of joint CDFs to which $F_\\Xi(\\xi)$ is assumed to belong,\n",
    " i.e. features that $F_\\Xi(\\xi)$ is assumed to have.\n",
    "\n",
    " Example: assuming that all $n$ random variables are mutually independent and have a common CDF,\n",
    " a model would be the subset of joint CDFs wherein all the marginal CDFs are equal\n",
    " and their product is equal to the underlying CDF.\n",
    "\n",
    " ### Parametric Models\n",
    " - Let $\\Psi$ be a model for $\\Xi$. is called *parametric* if the joint CDFs belonging to it\n",
    " - Let $\\Theta \\subseteq \\mathbb{p}$ be a set of $p$-dimensiona real vectors.\n",
    " - Let $\\gamma(\\theta)$ be a correspondence that associates a subset of $\\Psi$ to each $\\theta \\in \\Theta$.\n",
    " - The triple $(\\Phi,\\Theta,\\gamma)$ is a *parametric model* if $\\Psi = \\bigcup\\limits_{\\theta \\in \\Theta} \\gamma(\\theta)$.\n",
    " - $\\Theta$ is the *parameter space* and $\\theta$ is a *parameter.\n",
    " - If $\\gamma$ maps each parameter to an unique joint CDF,\n",
    " then $(\\Phi,\\Theta,\\gamma)$ is a *parametric family*.\n",
    " - If $\\gamma$ is one-to-one (i.e. each CDF is associated with just one parameter),\n",
    " then the parametric family is said to be *identifiable*.\n",
    " - Let $\\theta_0$ be the parameter that is associated with $F_\\Xi(\\xi)$,\n",
    " if it is unique, then it is called the *true parameter*.\n",
    "\n",
    "\n",
    " ### Statistical Inferences\n",
    " These are statements about the unknown distribution $F_\\Xi(\\xi)$\n",
    " based on the observed sample $\\xi$ and the statistical model $\\Psi$.\n",
    "\n",
    " They take the form of *model restrictions*.\n",
    "\n",
    " Given a subset of the original model $\\Psi_R \\subset \\Psi$,\n",
    " such restrictions can be either an inclusion restriction ($F_\\Xi \\in \\Psi_R$)\n",
    " or exclusion restriction ($F_\\Xi \\notin \\Psi_R$).\n",
    "\n",
    " Common statistical inferences:\n",
    " 1. In *hypothesis testing*, a restriction $F_\\Xi \\in \\Psi_R$ is proposed\n",
    " for either rejection or otherwise.\n",
    " 2. In *estimation*, some restriction must be chosen among a set of many.\n",
    " 3. In *Bayesian inference*, the observed sample $\\xi$ is used to update\n",
    " the subjective probability that the restriction is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Point Estimation\n",
    "\n",
    " *Point estimation* is the act of choosing a parameter $\\hat{\\theta} \\in \\Theta$\n",
    " to be the best guess of the unknown true parameter $\\theta_0$;\n",
    " $\\hat{\\theta}$ is called an *estimate* of $\\theta_0$.\n",
    "\n",
    " An *estimator* $\\hat{\\theta}(\\xi)$ is a function that produces\n",
    " a parameter estimate $\\hat{\\theta}$ from each sample $\\xi in \\Xi$.\n",
    "\n",
    " *Estimation error*:  $\\epsilon = \\hat{\\theta} - \\theta_0$\n",
    "\n",
    " A *loss function* $L(\\hat{\\theta}, \\theta_0)$ maps $\\Theta \\times \\Theta$ into $\\mathbb{R}$,\n",
    " quantifying the loss incurred by estimating $\\theta_0$ with $\\hat{\\theta}$.\n",
    "\n",
    " Examples:\n",
    "\n",
    " 1. *Absolute error*, $L(\\hat{\\theta}, \\theta_0) = | \\hat{\\theta} - \\theta_0 |$.\n",
    " 2. *Square error*, $L(\\hat{\\theta}, \\theta_0) = | \\hat{\\theta} - \\theta_0 |^2$.\n",
    "\n",
    "\n",
    " When an estimator is used,\n",
    " $L(\\hat{\\theta}(\\Xi), \\theta_0)$ can be considered to be a random variable.\n",
    " Its expected value is called the *statistical risk* and is denoted by\n",
    " $R(\\hat{\\theta}) = \\mathrm{E}[L(\\hat{\\theta}(\\Xi), \\theta_0)]$.\n",
    "\n",
    " *Mean absolute error*: $R(\\hat{\\theta}) = \\mathrm{E}[| \\hat{\\theta} - \\theta_0 |]$\n",
    "\n",
    " *Mean square error* (MSE): $R(\\hat{\\theta}) = \\mathrm{E}[| \\hat{\\theta} - \\theta_0 |^2]$\n",
    "\n",
    " *Root mean square error* (RMSE): $R(\\hat{\\theta}) = \\sqrt{\\mathrm{E}[| \\hat{\\theta} - \\theta_0 |^2]}$\n",
    "\n",
    "\n",
    " Criteria to evaluate estimators:\n",
    "\n",
    " 1. Unbiasedness, $\\hat{\\theta}$ is *unbiased* if $\\mathrm{E}[\\hat{\\theta}(\\Xi)] = \\theta_0$\n",
    " (estimator produces estimated values that are correct on average).\n",
    "\n",
    " 2. Consistency, $\\hat{\\theta}$ is *weakly/strongly consistent* if\n",
    " the sequence of estimators produced by a sequence of samples $\\{xi_n\\}$ converges\n",
    " in probability/almost surely to the true parameter $\\theta_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mean Estimation\n",
    "\n",
    " Consider the sample $\\xi_n = [x_1 \\: \\ldots \\: x_n]$ made up of $n$ independent draws\n",
    " from a probability distribution with unknown mean $\\mu$ and variance $\\sigma^2$.\n",
    " Therein is $n$ realizations $x_i$ of $n$ independent random variables $X_i$\n",
    " all having the same distribution.\n",
    "\n",
    " An estimator of $\\mu$ is the sample mean:\n",
    " $\\hat{\\mu} = \\bar{X}_n = \\frac{1}{n} \\sum\\limits_{i=1}^n X_i$.\n",
    "\n",
    " $\\hat{\\mu}$ is unbiased since\n",
    " $\\mathrm{E}[\\hat{\\mu}] = \\ldots = \\mu$.\n",
    "\n",
    " The variance of $\\hat{\\mu}$:\n",
    " $\\mathrm{var}[\\hat{\\mu}] = \\ldots = $\\frac{\\sigma^2}{n}$.\n",
    "\n",
    " The risk/MSE of $\\hat{\\mu}$:\n",
    " $\\mathrm{MSE}(\\hat{\\mu}) = \\ldots = \\mathrm{var}[\\hat{\\mu}]$.\n",
    "\n",
    " Since $\\{X_n\\}$ is an IID sequence with finite $\\mu$ and $\\sigma^2$,\n",
    " the sample mean $\\bar{X}_n$ is asymptotically normal.\n",
    "\n",
    " ## Variance Estimation\n",
    "\n",
    " An estimator of the variance:\n",
    " $\\widehat{\\sigma^2} = \\frac{1}{n}\\sum\\limits_{i=1}^n (X_i - \\mu)^2$.\n",
    "\n",
    " This estimator is unbiased:\n",
    " $\\mathrm{E}[\\widehat{\\sigma^2}] = \\ldots = \\sigma^2$.\n",
    "\n",
    " The variance of $\\widehat{\\sigma^2}$ goes to zero as $n$ approaches infinity:\n",
    " $\\mathrm{var}[\\widehat{\\sigma^2}] = \\ldots = $\\frac{2 \\sigma^4}{n}$.\n",
    "\n",
    " $\\widehat{\\sigma^2}$ has a gamma distribution with parameters $n$ and $\\sigma^2$.\n",
    "\n",
    " The risk/MSE of $\\widehat{\\sigma^2}$:\n",
    " $\\mathrm{MSE}(\\hat{\\mu}) = \\ldots = \\mathrm{var}[\\widehat{\\sigma^2}]$.\n",
    "\n",
    " If the mean is unknown, two other estimators can be used:\n",
    " $\\widehat{\\sigma^2} = \\begin{cases} S_n^2 = \\frac{1}{n} \\sum\\limits_{i=1}^n (X_i - \\bar{X}_n)^2 & \\textrm{unadjusted sample variance} \\\\ s_n^2 = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (X_i - \\bar{X}_n)^2 & \\textrm{adjusted sample variance} \\end{cases}$\n",
    "\n",
    " The *unadjusted sample variance* $S_n^2$ is a biased estimator of the true variance $\\sigma^2$:\n",
    " $\\mathrm{E}[S_n^2] = \\ldots = \\frac{n-1}{n} \\sigma^2$.\n",
    "\n",
    " The *adjusted sample variance* $s_n^2$ is unbiased though:\n",
    " $\\mathrm{E}[s_n^2] = \\ldots = \\sigma^2$.\n",
    "\n",
    " The sum of squared deviations from the true mean is always larger than that from the sample mean;\n",
    " the $n-1$ factor exactly corrects for this bias.\n",
    "\n",
    " $n-1$ is called the *number of degrees of freedom*;\n",
    " it is the number os sample points ($n$) minus the number of parameters to be estimated\n",
    " ($1$ for the true mean $\\mu$).\n",
    "\n",
    " The variance of these estimators is:\n",
    " $\\begin{align} \\mathrm{E}[S_n^2] &= \\frac{n-1}{n} \\frac{2\\sigma^4}{n} \\\\ \\mathrm{E}[s_n^2] &= \\frac{2\\sigma^4}{n-1} \\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set Estimation\n",
    "\n",
    " A *set estimation* is the act of choosing a subset $T$ of the parameter space $\\Theta$\n",
    " such that $T$ has some high probability (called *coverage probability* $C$)\n",
    " of containing the true and unknown parameter $\\theta_0$.\n",
    "\n",
    " Such subset $T$ is called a *set estimate* of (or *confidence set* for) $\\theta_0$.\n",
    "\n",
    " If $\\Theta \\subseteq \\mathbb{R}$,\n",
    " $T = [a, b]$ is called an *interval estimate* or *confidence interval*.\n",
    "\n",
    " If $T = T(\\xi)$, it is called a *set estimator*.\n",
    "\n",
    " The coverage probability is defined as $C(T,\\theta_0) = P_{\\theta_0}(\\theta_0 \\in T(\\Xi))$\n",
    " where the notation indicates that it is calculated using the CDF $F_\\Xi(\\xi; \\theta_0)$\n",
    " associated with $\\theta_0$.\n",
    "\n",
    " Since $C$ is rarely known, the *confidence coefficient* (or *level of confidence*)\n",
    " is calculated:\n",
    " $c(T) = \\mathrm{inf}\\limits_{\\theta \\in \\Theta} C(T,\\theta)$.\n",
    "\n",
    " The size of a confidence set is called its *measure*\n",
    " (as in Lebesgue measure, the generalization of volume to higher dimensions).\n",
    "\n",
    " ## Set Estimation of the Mean\n",
    "\n",
    " For a normal IID sample with *unknown mean* and *known variance*:\n",
    " - The estimator of the true mean is the sample mean, $\\hat{\\mu} = \\bar{X}_n$.\n",
    " - The interval estmator is then $T_n = \\left[\\bar{X}_n - \\sqrt{\\frac{\\sigma^2}{n}z}, \\bar{X}_n + \\sqrt{\\frac{\\sigma^2}{n}z} \\right]$\n",
    " where $z \\in \\mathbb{R}_{++}$ is some constant.\n",
    " - The coverage probability of $T_n$ is\n",
    " $C(T_n;\\mu) = P(-z \\leq Z \\leq z)$ where $Z$ is a standard normal random variable.\n",
    " - Since $C$ does not depend on the unknown $\\mu$, $c(T_n) = C(T_n;\\mu)$.\n",
    " - Size of $T_n$: $\\lambda(T_n) = 2 \\sqrt{\\frac{\\sigma^2}{n}}z$.\n",
    "\n",
    "\n",
    " If *unknown variance*:\n",
    " - Using the adjusted sample variance as the variance estimator,\n",
    " $T_n^{(a)} = \\left[\\bar{X}_n - \\sqrt{\\frac{s_n^2}{n}z}, \\bar{X}_n + \\sqrt{\\frac{s_n^2}{n}z} \\right]$\n",
    " - The coverage probability:\n",
    " $C(T_n^{(a)};\\mu,\\sigma^2) = P(-z \\leq Z_{n-1} \\leq z)$\n",
    " where $Z$ is a standard Student's t random variable.\n",
    " - The confidence coefficient:\n",
    " $c(T_n^{(a)}) = C(T_n^{(a)};\\mu, \\sigma^2)$\n",
    " - The size: $\\lambda(T_n^{(a)}) = 2 \\sqrt{\\frac{s_n^2}{n}}z$\n",
    " - The expected size: $\\mathrm{E}[\\lambda(T_n^{(a)})] = \\sqrt{\\frac{2}{n-1}} \\frac{\\Gamma(n/2)}{\\Gamma((n-1)/2)} 2 \\sqrt{\\frac{\\sigma^2}{n}}z$\n",
    "\n",
    "\n",
    " ## Set Estimation of the Variance\n",
    "\n",
    " For a normal IID sample with *known mean* and *known variance*:\n",
    " - The interval estimator: $T_n = [\\frac{n}{z_2}\\widehat{\\sigma_n^2}, \\frac{n}{z_1}\\widehat{\\sigma_n^2}]$\n",
    " where $\\widehat{\\sigma_n^2} = \\frac{1}{n}\\sum\\limits(X_i - \\mu)^2$\n",
    " and $z_1 < z_2$ are strictly positive constants.\n",
    " - Coverage probability:\n",
    " $C(T_n;\\sigma^2) = P(\\sigma \\in T_n) = P(z_1 \\leq Z \\leq z_2)$\n",
    " where $Z$ is a chi-square random variable with $n$ degrees of freedom.\n",
    " - Confidence coefficient:\n",
    " $c(T_n) = C(T_n;\\sigma^2)$\n",
    " - Size of the interval estimator:\n",
    " $\\lambda(T_n) = \\ldots = n \\left(\\frac{1}{z_1} - \\frac{1}{z_2} \\right) \\widehat{\\sigma_n^2}$.\n",
    " - Expected size: $\\mathrm{E}[\\lambda(T_n)] = \\ldots = n \\left(\\frac{1}{z_1} - \\frac{1}{z_2} \\right) \\sigma^2$.\n",
    "\n",
    "\n",
    " If *unknown mean*:\n",
    " - Using the adjusted sample variance as the variance estimator,\n",
    " $T_n = [\\frac{n-1}{z_2}\\widehat{s_n^2}, \\frac{n-1}{z_1}\\widehat{s_n^2}]$\n",
    " - Coverage probability:\n",
    " $C(T_n;\\mu,\\sigma^2) = P(\\sigma \\in T_n) = P(z_1 \\leq Z_{n-1} \\leq z_2)$\n",
    " where $Z_{n-1}$ is a chi-square random variable with $n-1$ degrees of freedom.\n",
    " - $\\ldots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hypothesis Testing\n",
    "\n",
    " Consider a random vector $\\Xi$ with support $R_\\Xi$\n",
    " and unknown joint CDF $F_\\Xi(\\xi)$,\n",
    " where $\\xi$ is a realization of $\\Xi$ (i.e. a sample)\n",
    " and $F_\\Xi$ is assumed to belong to a set $\\Phi$ (i.e. the statistical model).\n",
    "\n",
    " Null hypothesis:\n",
    " - Make a statement (statistical inference) about a model restriction $\\Phi_R \\subset \\Phi$.\n",
    " - Two options: (1) reject or (2) do not reject the restriction $F_\\Xi \\in \\Phi_R$.\n",
    " - If parametric model: (1) reject or (2) do not reject the restriction $\\theta_0 \\in \\Theta_R$.\n",
    " - The *null hypothesis* (denoted $H_0$) is that the restriction is true ($H_0: \\theta_0 \\in \\Theta_R$).\n",
    "\n",
    "\n",
    " *Alternative hypothesis*: $H_1: \\theta_0 \\in \\Theta_R^\\complement$\n",
    "\n",
    " Types of errors:\n",
    "\n",
    " 1. *Type I error*, rejecting $\\theta_0 \\in \\Theta_R$ when it is true.\n",
    "\n",
    " 2. *Type II error*, not rejecting $\\theta_0 \\in \\Theta_R$ when it is false.\n",
    "\n",
    "\n",
    " Critical region:\n",
    " - A test of hypothesis divides $R_\\Xi$ into two disjoint subsets: $C_\\Xi \\cup C_\\Xi^\\complement = R_\\Xi$.\n",
    " - The set of all $\\xi$ for which the null hypothesis is rejected,\n",
    " denoted $C_\\Xi$, is called the *critical region* (or *rejection region*),\n",
    " - $C_\\Xi = \\{\\xi \\in R_\\Xi \\: : \\: H_0 \\textrm{ is rejected whenever the sample is observed} \\}$.\n",
    "\n",
    "\n",
    " Test statistics:\n",
    " - A critical region can be implicitly defined in terms of a *test statistics*.\n",
    " - A test statistics is a random variable $S$ whose realization is a function of the sample $\\xi$,\n",
    " $S = s(\\Xi)$.\n",
    " - A critical region for $S$ is a subset $C_S \\subset \\mathbb{R}$ such that\n",
    " $s(\\xi) \\in C_S \\; \\Rightarrow \\; \\xi \\in C_\\Xi \\; \\Rightarrow \\; H_0 \\textrm{ is rejected}$.\n",
    " - Conversely, $s(\\xi) \\notin C_S \\; \\Rightarrow \\; \\xi \\notin C_\\Xi \\; \\Rightarrow \\; H_0 \\textrm{ is not rejected}$.\n",
    " - If $C_S^\\complement = [a, b]$, then $a, b$ are called *critical values* of the test.\n",
    " - A hypothesis test has a function known as its *power function* $\\pi(\\theta)$\n",
    " that associates the probability of rejecting $H_0$ to each parameter $\\theta \\in \\Theta$,\n",
    " i.e. $\\pi(\\theta) = P_\\theta(\\Xi \\in C_\\Xi)$.\n",
    " - When $\\theta \\in \\Theta_R$, $\\pi(\\theta)$ describes the probability of a Type I error.\n",
    " - The maximum such probability is the *size* or *level of significance*\n",
    " of the test (denoted $\\alpha$),\n",
    " $\\alpha = \\sup\\limits_{\\theta \\in \\Theta_R} \\pi(\\theta)$.\n",
    "\n",
    "\n",
    " The ideal test should have size $0$\n",
    " (no probability of rejecting the null hypothesis when it is true)\n",
    " and power $1$ when $\\theta_0 \\notin \\Theta_R$\n",
    " (guaranteed to reject the null hypothesis when it is false).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hypothesis Tests about the Mean\n",
    "\n",
    " Assuming normal IID samples\n",
    " (*unknown mean* $\\mu$ and *known variance* $\\sigma^2$).\n",
    "\n",
    " Let's test the null hypothesis $H_0$ that $\\mu = \\mu_0$\n",
    " for some specific value $\\mu_0 \\in \\mathbb{R}$.\n",
    "\n",
    " The alternative hypothesis: $H_1 : \\mu \\neq \\mu_0$.\n",
    "\n",
    " Consider the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum\\limits_{i=1}^n X_i$.\n",
    "\n",
    " A test statistics called *normal z-statistics* is defined as\n",
    " $Z_n = \\frac{\\bar{X}_n - \\mu_0}{\\sqrt{\\sigma^2/n}}$.\n",
    " The resulting test is called a *normal z-test*.\n",
    "\n",
    " Let $z \\in \\mathbb{R}_{++}$.\n",
    " Let's reject $H_0$ if $|Z_n| > z$,\n",
    " i.e. the critical region is $C_{Z_n} = (-\\infty,-z) \\cup (z,\\infty)$\n",
    " with critical values $\\pm z$.\n",
    "\n",
    " The power function of the test is\n",
    " $\\pi(\\mu) = P_\\mu(Z_n \\notin [-z, z]) = 1 - P\\left( -z + \\frac{\\mu_0 - \\mu}{\\sqrt{\\sigma^2/n}} \\leq Z \\leq z + \\frac{\\mu_0 - \\mu}{\\sqrt{\\sigma^2/n}} \\right)$\n",
    " where $Z$ is a standard normal random variable.\n",
    "\n",
    " The size of the test is just\n",
    " $\\alpha = \\pi(\\mu_0) = P_{\\mu_0}(Z_n \\notin [-z, z]) = 1 - P(-z \\leq Z \\leq z)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Let's assume now *unknown mean* $\\mu$\n",
    " and *unknown variance* $\\sigma^2$.\n",
    "\n",
    " Define a new test statistics $Z_n^{(a)}$ using the sample mean $\\bar{X}_n$\n",
    " and the adjusted sample variance:\n",
    " $Z_n^{(a)} = \\frac{\\bar{X}_n - \\mu_0}{\\sqrt{s_n^2/n}}.\n",
    "\n",
    " This is called *Student's t-statistics*\n",
    " and the resulting test is called a *Student's t-test*.\n",
    "\n",
    " As before,\n",
    " the critical region is $C_{Z_n^{(a)}} = (-\\infty,-z) \\cup (z,\\infty)$\n",
    " with critical values $\\pm z$.\n",
    "\n",
    " The power function of the test is\n",
    " $\\pi^{(a)}(\\mu) = P_\\mu(Z_n^{(a)} \\notin [-z, z]) = 1 - P\\left( -\\sqrt{\\frac{n-1}{n}} z \\leq W_{n-1} \\leq \\sqrt{\\frac{n-1}{n}} z \\right)$\n",
    " where $W_{n-1}$ is a non-central standard Student's t random variable\n",
    " with $n-1$ degrees of freedom and $c = \\frac{\\mu - \\mu_0}{\\sqrt{\\sigma^2/n}}$.\n",
    "\n",
    " The size of the test is just\n",
    " $\\alpha = \\pi^{(a)}(\\mu_0) = 1 - P(-z \\leq W_{n-1} \\leq z)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hypothesis Tests about the Mean\n",
    "\n",
    " Assuming normal IID samples\n",
    " (*known mean* $\\mu$ and *unknown variance* $\\sigma^2$).\n",
    "\n",
    " Null hypothesis $H_0: \\sigma^2 = \\sigma_0^2$.\n",
    "\n",
    " Set the test statistic to be $\\chi_n^2 = \\frac{n}{\\sigma_0^2} \\widehat{\\sigma_n^2}$\n",
    " where $\\widehat{\\sigma_n^2} = \\frac{1}{n}\\sum\\limits_{i=1}^n (X_i-\\mu)^2$.\n",
    "\n",
    " This test statistic is called *chi-square statistic*\n",
    " and the test is called a *chi-square test*.\n",
    "\n",
    " Define the critical region as $C_{\\chi_n^2} = [0,z_1) \\cup (z_2,\\infty]$\n",
    " where $z_1 < z_2$ and $z_i \\in \\mathbb{R}_{++}$ are the critical values.\n",
    "\n",
    " The power function of the test:\n",
    " $\\pi(\\sigma^2) = P_{\\sigma^2}(\\chi_n^2 \\notin [z_1,z_2]) = 1 - P \\left( \\frac{\\sigma_0^2}{\\sigma^2}z_1 \\leq \\kappa_n \\leq \\frac{\\sigma_0^2}{\\sigma^2}z_2 \\right)$.\n",
    "\n",
    " $\\kappa_n$ is a chi-square random variable with $n$ degrees of freedom.\n",
    "\n",
    " The size is just $\\alpha = \\pi(\\sigma_0^2) = 1 - P(z_1 \\leq \\kappa_n \\leq z_2)$.\n",
    "\n",
    "\n",
    " If *unknown mean* $\\mu$ and *unknown variance* $\\sigma^2$.\n",
    "\n",
    " The test statistic becomes $\\chi_n^2 = \\frac{n-1}{\\sigma_0^2} \\widehat{s_n^2}$\n",
    " where $s_n$ is the adjusted sample variance.\n",
    "\n",
    " Everything else is as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Estimation Methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
