{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\r\n# ms-python.python added\r\nimport os\r\ntry:\r\n\tos.chdir(os.path.join(os.getcwd(), '..'))\r\n\tprint(os.getcwd())\r\nexcept:\r\n\tpass\r\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Notes and exercises from [Statlect](https://www.statlect.com)\n"," ---"],"metadata":{}},{"cell_type":"markdown","source":[" # Part 5 - [Fundamentals of Statistics](https://www.statlect.com/fundamentals-of-statistics/)\n","\n"," ## Statistical Inference\n","\n"," The act of using observed data (the *sample*) to infer unknown features\n"," of the underlying probability distribution.\n","\n"," A sample is the vector of realizations $x_1, \\ldots, x_n$\n"," of $n$ independent random variables $X_1, \\ldots, X_n$\n"," having a common distribution function $F_X(x)$.\n","\n"," In other words, the sample $\\xi = [x_1 \\: \\ldots \\: x_n]$ is\n"," the realization of a random vector $\\Xi = [X_1 \\: \\ldots \\: X_n]$\n"," with joint CDF $F_\\Xi(\\xi) = F_X(x_1) \\ldots F_X(x_n)$.\n","\n"," An individual realization $x_i$ is known as an *observation* from the sample.\n","\n"," A *statistical model* (or *model specification* or just *model*)\n"," is a set of joint CDFs to which $F_\\Xi(\\xi)$ is assumed to belong,\n"," i.e. features that $F_\\Xi(\\xi)$ is assumed to have.\n","\n"," Example: assuming that all $n$ random variables are mutually independent and have a common CDF,\n"," a model would be the subset of joint CDFs wherein all the marginal CDFs are equal\n"," and their product is equal to the underlying CDF.\n","\n"," ### Parametric Models\n"," - Let $\\Psi$ be a model for $\\Xi$. is called *parametric* if the joint CDFs belonging to it\n"," - Let $\\Theta \\subseteq \\mathbb{p}$ be a set of $p$-dimensiona real vectors.\n"," - Let $\\gamma(\\theta)$ be a correspondence that associates a subset of $\\Psi$ to each $\\theta \\in \\Theta$.\n"," - The triple $(\\Phi,\\Theta,\\gamma)$ is a *parametric model* if $\\Psi = \\bigcup\\limits_{\\theta \\in \\Theta} \\gamma(\\theta)$.\n"," - $\\Theta$ is the *parameter space* and $\\theta$ is a *parameter.\n"," - If $\\gamma$ maps each parameter to an unique joint CDF,\n"," then $(\\Phi,\\Theta,\\gamma)$ is a *parametric family*.\n"," - If $\\gamma$ is one-to-one (i.e. each CDF is associated with just one parameter),\n"," then the parametric family is said to be *identifiable*.\n"," - Let $\\theta_0$ be the parameter that is associated with $F_\\Xi(\\xi)$,\n"," if it is unique, then it is called the *true parameter*.\n","\n","\n"," ### Statistical Inferences\n"," These are statements about the unknown distribution $F_\\Xi(\\xi)$\n"," based on the observed sample $\\xi$ and the statistical model $\\Psi$.\n","\n"," They take the form of *model restrictions*.\n","\n"," Given a subset of the original model $\\Psi_R \\subset \\Psi$,\n"," such restrictions can be either an inclusion restriction ($F_\\Xi \\in \\Psi_R$)\n"," or exclusion restriction ($F_\\Xi \\notin \\Psi_R$).\n","\n"," Common statistical inferences:\n"," 1. In *hypothesis testing*, a restriction $F_\\Xi \\in \\Psi_R$ is proposed\n"," for either rejection or otherwise.\n"," 2. In *estimation*, some restriction must be chosen among a set of many.\n"," 3. In *Bayesian inference*, the observed sample $\\xi$ is used to update\n"," the subjective probability that the restriction is true.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Point Estimation\n","\n"," *Point estimation* is the act of choosing a parameter $\\hat{\\theta} \\in \\Theta$\n"," to be the best guess of the unknown true parameter $\\theta_0$;\n"," $\\hat{\\theta}$ is called an *estimate* of $\\theta_0$.\n","\n"," An *estimator* $\\hat{\\theta}(\\xi)$ is a function that produces\n"," a parameter estimate $\\hat{\\theta}$ from each sample $\\xi in \\Xi$.\n","\n"," *Estimation error*:  $\\epsilon = \\hat{\\theta} - \\theta_0$\n","\n"," A *loss function* $L(\\hat{\\theta}, \\theta_0)$ maps $\\Theta \\times \\Theta$ into $\\mathbb{R}$,\n"," quantifying the loss incurred by estimating $\\theta_0$ with $\\hat{\\theta}$.\n","\n"," Examples:\n","\n"," 1. *Absolute error*, $L(\\hat{\\theta}, \\theta_0) = | \\hat{\\theta} - \\theta_0 |$.\n"," 2. *Square error*, $L(\\hat{\\theta}, \\theta_0) = | \\hat{\\theta} - \\theta_0 |^2$.\n","\n","\n"," When an estimator is used,\n"," $L(\\hat{\\theta}(\\Xi), \\theta_0)$ can be considered to be a random variable.\n"," Its expected value is called the *statistical risk* and is denoted by\n"," $R(\\hat{\\theta}) = \\mathrm{E}[L(\\hat{\\theta}(\\Xi), \\theta_0)]$.\n","\n"," *Mean absolute error*: $R(\\hat{\\theta}) = \\mathrm{E}[| \\hat{\\theta} - \\theta_0 |]$\n","\n"," *Mean square error* (MSE): $R(\\hat{\\theta}) = \\mathrm{E}[| \\hat{\\theta} - \\theta_0 |^2]$\n","\n"," *Root mean square error* (RMSE): $R(\\hat{\\theta}) = \\sqrt{\\mathrm{E}[| \\hat{\\theta} - \\theta_0 |^2]}$\n","\n"," Bias-variance decomposition:\n"," $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{tr}(\\mathrm{var}[\\hat{\\theta}]) + |\\mathrm{Bias}[\\hat{\\theta}]|^2$\n","\n"," Criteria to evaluate estimators:\n","\n"," 1. Unbiasedness, $\\hat{\\theta}$ is *unbiased* if $\\mathrm{E}[\\hat{\\theta}(\\Xi)] = \\theta_0$\n"," (estimator produces estimated values that are correct on average).\n","\n"," 2. Consistency, $\\hat{\\theta}$ is *weakly/strongly consistent* if\n"," the sequence of estimators produced by a sequence of samples $\\{xi_n\\}$ converges\n"," in probability/almost surely to the true parameter $\\theta_0$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Mean Estimation\n","\n"," Consider the sample $\\xi_n = [x_1 \\: \\ldots \\: x_n]$ made up of $n$ independent draws\n"," from a probability distribution with unknown mean $\\mu$ and variance $\\sigma^2$.\n"," Therein is $n$ realizations $x_i$ of $n$ independent random variables $X_i$\n"," all having the same distribution.\n","\n"," An estimator of $\\mu$ is the sample mean:\n"," $\\hat{\\mu} = \\bar{X}_n = \\frac{1}{n} \\sum\\limits_{i=1}^n X_i$.\n","\n"," $\\hat{\\mu}$ is unbiased since\n"," $\\mathrm{E}[\\hat{\\mu}] = \\ldots = \\mu$.\n","\n"," The variance of $\\hat{\\mu}$:\n"," $\\mathrm{var}[\\hat{\\mu}] = \\ldots = $\\frac{\\sigma^2}{n}$.\n","\n"," The risk/MSE of $\\hat{\\mu}$:\n"," $\\mathrm{MSE}(\\hat{\\mu}) = \\ldots = \\mathrm{var}[\\hat{\\mu}]$.\n","\n"," Since $\\{X_n\\}$ is an IID sequence with finite $\\mu$ and $\\sigma^2$,\n"," the sample mean $\\bar{X}_n$ is asymptotically normal.\n","\n"," ## Variance Estimation\n","\n"," An estimator of the variance:\n"," $\\widehat{\\sigma^2} = \\frac{1}{n}\\sum\\limits_{i=1}^n (X_i - \\mu)^2$.\n","\n"," This estimator is unbiased:\n"," $\\mathrm{E}[\\widehat{\\sigma^2}] = \\ldots = \\sigma^2$.\n","\n"," The variance of $\\widehat{\\sigma^2}$ goes to zero as $n$ approaches infinity:\n"," $\\mathrm{var}[\\widehat{\\sigma^2}] = \\ldots = $\\frac{2 \\sigma^4}{n}$.\n","\n"," $\\widehat{\\sigma^2}$ has a gamma distribution with parameters $n$ and $\\sigma^2$.\n","\n"," The risk/MSE of $\\widehat{\\sigma^2}$:\n"," $\\mathrm{MSE}(\\hat{\\mu}) = \\ldots = \\mathrm{var}[\\widehat{\\sigma^2}]$.\n","\n"," If the mean is unknown, two other estimators can be used:\n"," $\\widehat{\\sigma^2} = \\begin{cases} S_n^2 = \\frac{1}{n} \\sum\\limits_{i=1}^n (X_i - \\bar{X}_n)^2 & \\textrm{unadjusted sample variance} \\\\ s_n^2 = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (X_i - \\bar{X}_n)^2 & \\textrm{adjusted sample variance} \\end{cases}$\n","\n"," The *unadjusted sample variance* $S_n^2$ is a biased estimator of the true variance $\\sigma^2$:\n"," $\\mathrm{E}[S_n^2] = \\ldots = \\frac{n-1}{n} \\sigma^2$.\n","\n"," The *adjusted sample variance* $s_n^2$ is unbiased though:\n"," $\\mathrm{E}[s_n^2] = \\ldots = \\sigma^2$.\n","\n"," The sum of squared deviations from the true mean is always larger than that from the sample mean;\n"," the $n-1$ factor exactly corrects for this bias.\n","\n"," $n-1$ is called the *number of degrees of freedom*;\n"," it is the number os sample points ($n$) minus the number of parameters to be estimated\n"," ($1$ for the true mean $\\mu$).\n","\n"," The variance of these estimators is:\n"," $\\begin{align} \\mathrm{E}[S_n^2] &= \\frac{n-1}{n} \\frac{2\\sigma^4}{n} \\\\ \\mathrm{E}[s_n^2] &= \\frac{2\\sigma^4}{n-1} \\end{align}$\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Set Estimation\n","\n"," A *set estimation* is the act of choosing a subset $T$ of the parameter space $\\Theta$\n"," such that $T$ has some high probability (called *coverage probability* $C$)\n"," of containing the true and unknown parameter $\\theta_0$.\n","\n"," Such subset $T$ is called a *set estimate* of (or *confidence set* for) $\\theta_0$.\n","\n"," If $\\Theta \\subseteq \\mathbb{R}$,\n"," $T = [a, b]$ is called an *interval estimate* or *confidence interval*.\n","\n"," If $T = T(\\xi)$, it is called a *set estimator*.\n","\n"," The coverage probability is defined as $C(T,\\theta_0) = P_{\\theta_0}(\\theta_0 \\in T(\\Xi))$\n"," where the notation indicates that it is calculated using the CDF $F_\\Xi(\\xi; \\theta_0)$\n"," associated with $\\theta_0$.\n","\n"," Since $C$ is rarely known, the *confidence coefficient* (or *level of confidence*)\n"," is calculated:\n"," $c(T) = \\mathrm{inf}\\limits_{\\theta \\in \\Theta} C(T,\\theta)$.\n","\n"," The size of a confidence set is called its *measure*\n"," (as in Lebesgue measure, the generalization of volume to higher dimensions).\n","\n"," ## Set Estimation of the Mean\n","\n"," For a normal IID sample with *unknown mean* and *known variance*:\n"," - The estimator of the true mean is the sample mean, $\\hat{\\mu} = \\bar{X}_n$.\n"," - The interval estmator is then $T_n = \\left[\\bar{X}_n - \\sqrt{\\frac{\\sigma^2}{n}z}, \\bar{X}_n + \\sqrt{\\frac{\\sigma^2}{n}z} \\right]$\n"," where $z \\in \\mathbb{R}_{++}$ is some constant.\n"," - The coverage probability of $T_n$ is\n"," $C(T_n;\\mu) = P(-z \\leq Z \\leq z)$ where $Z$ is a standard normal random variable.\n"," - Since $C$ does not depend on the unknown $\\mu$, $c(T_n) = C(T_n;\\mu)$.\n"," - Size of $T_n$: $\\lambda(T_n) = 2 \\sqrt{\\frac{\\sigma^2}{n}}z$.\n","\n","\n"," If *unknown variance*:\n"," - Using the adjusted sample variance as the variance estimator,\n"," $T_n^{(a)} = \\left[\\bar{X}_n - \\sqrt{\\frac{s_n^2}{n}z}, \\bar{X}_n + \\sqrt{\\frac{s_n^2}{n}z} \\right]$\n"," - The coverage probability:\n"," $C(T_n^{(a)};\\mu,\\sigma^2) = P(-z \\leq Z_{n-1} \\leq z)$\n"," where $Z$ is a standard Student's t random variable.\n"," - The confidence coefficient:\n"," $c(T_n^{(a)}) = C(T_n^{(a)};\\mu, \\sigma^2)$\n"," - The size: $\\lambda(T_n^{(a)}) = 2 \\sqrt{\\frac{s_n^2}{n}}z$\n"," - The expected size: $\\mathrm{E}[\\lambda(T_n^{(a)})] = \\sqrt{\\frac{2}{n-1}} \\frac{\\Gamma(n/2)}{\\Gamma((n-1)/2)} 2 \\sqrt{\\frac{\\sigma^2}{n}}z$\n","\n","\n"," ## Set Estimation of the Variance\n","\n"," For a normal IID sample with *known mean* and *known variance*:\n"," - The interval estimator: $T_n = [\\frac{n}{z_2}\\widehat{\\sigma_n^2}, \\frac{n}{z_1}\\widehat{\\sigma_n^2}]$\n"," where $\\widehat{\\sigma_n^2} = \\frac{1}{n}\\sum\\limits(X_i - \\mu)^2$\n"," and $z_1 < z_2$ are strictly positive constants.\n"," - Coverage probability:\n"," $C(T_n;\\sigma^2) = P(\\sigma \\in T_n) = P(z_1 \\leq Z \\leq z_2)$\n"," where $Z$ is a chi-square random variable with $n$ degrees of freedom.\n"," - Confidence coefficient:\n"," $c(T_n) = C(T_n;\\sigma^2)$\n"," - Size of the interval estimator:\n"," $\\lambda(T_n) = \\ldots = n \\left(\\frac{1}{z_1} - \\frac{1}{z_2} \\right) \\widehat{\\sigma_n^2}$.\n"," - Expected size: $\\mathrm{E}[\\lambda(T_n)] = \\ldots = n \\left(\\frac{1}{z_1} - \\frac{1}{z_2} \\right) \\sigma^2$.\n","\n","\n"," If *unknown mean*:\n"," - Using the adjusted sample variance as the variance estimator,\n"," $T_n = [\\frac{n-1}{z_2}\\widehat{s_n^2}, \\frac{n-1}{z_1}\\widehat{s_n^2}]$\n"," - Coverage probability:\n"," $C(T_n;\\mu,\\sigma^2) = P(\\sigma \\in T_n) = P(z_1 \\leq Z_{n-1} \\leq z_2)$\n"," where $Z_{n-1}$ is a chi-square random variable with $n-1$ degrees of freedom.\n"," - $\\ldots$\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Hypothesis Testing\n","\n"," Consider a random vector $\\Xi$ with support $R_\\Xi$\n"," and unknown joint CDF $F_\\Xi(\\xi)$,\n"," where $\\xi$ is a realization of $\\Xi$ (i.e. a sample)\n"," and $F_\\Xi$ is assumed to belong to a set $\\Phi$ (i.e. the statistical model).\n","\n"," Null hypothesis:\n"," - Make a statement (statistical inference) about a model restriction $\\Phi_R \\subset \\Phi$.\n"," - Two options: (1) reject or (2) do not reject the restriction $F_\\Xi \\in \\Phi_R$.\n"," - If parametric model: (1) reject or (2) do not reject the restriction $\\theta_0 \\in \\Theta_R$.\n"," - The *null hypothesis* (denoted $H_0$) is that the restriction is true ($H_0: \\theta_0 \\in \\Theta_R$).\n","\n","\n"," *Alternative hypothesis*: $H_1: \\theta_0 \\in \\Theta_R^\\complement$\n","\n"," Types of errors:\n","\n"," 1. *Type I error*, rejecting $\\theta_0 \\in \\Theta_R$ when it is true.\n","\n"," 2. *Type II error*, not rejecting $\\theta_0 \\in \\Theta_R$ when it is false.\n","\n","\n"," Critical region:\n"," - A test of hypothesis divides $R_\\Xi$ into two disjoint subsets: $C_\\Xi \\cup C_\\Xi^\\complement = R_\\Xi$.\n"," - The set of all $\\xi$ for which the null hypothesis is rejected,\n"," denoted $C_\\Xi$, is called the *critical region* (or *rejection region*),\n"," - $C_\\Xi = \\{\\xi \\in R_\\Xi \\: : \\: H_0 \\textrm{ is rejected whenever the sample is observed} \\}$.\n","\n","\n"," Test statistics:\n"," - A critical region can be implicitly defined in terms of a *test statistics*.\n"," - A test statistics is a random variable $S$ whose realization is a function of the sample $\\xi$,\n"," $S = s(\\Xi)$.\n"," - A critical region for $S$ is a subset $C_S \\subset \\mathbb{R}$ such that\n"," $s(\\xi) \\in C_S \\; \\Rightarrow \\; \\xi \\in C_\\Xi \\; \\Rightarrow \\; H_0 \\textrm{ is rejected}$.\n"," - Conversely, $s(\\xi) \\notin C_S \\; \\Rightarrow \\; \\xi \\notin C_\\Xi \\; \\Rightarrow \\; H_0 \\textrm{ is not rejected}$.\n"," - If $C_S^\\complement = [a, b]$, then $a, b$ are called *critical values* of the test.\n"," - A hypothesis test has a function known as its *power function* $\\pi(\\theta)$\n"," that associates the probability of rejecting $H_0$ to each parameter $\\theta \\in \\Theta$,\n"," i.e. $\\pi(\\theta) = P_\\theta(\\Xi \\in C_\\Xi)$.\n"," - When $\\theta \\in \\Theta_R$, $\\pi(\\theta)$ describes the probability of a Type I error.\n"," - The maximum such probability is the *size* or *level of significance*\n"," of the test (denoted $\\alpha$),\n"," $\\alpha = \\sup\\limits_{\\theta \\in \\Theta_R} \\pi(\\theta)$.\n","\n","\n"," The ideal test should have size $0$\n"," (no probability of rejecting the null hypothesis when it is true)\n"," and power $1$ when $\\theta_0 \\notin \\Theta_R$\n"," (guaranteed to reject the null hypothesis when it is false).\n","\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Hypothesis Tests about the Mean\n","\n"," Assuming normal IID samples\n"," (*unknown mean* $\\mu$ and *known variance* $\\sigma^2$).\n","\n"," Let's test the null hypothesis $H_0$ that $\\mu = \\mu_0$\n"," for some specific value $\\mu_0 \\in \\mathbb{R}$.\n","\n"," The alternative hypothesis: $H_1 : \\mu \\neq \\mu_0$.\n","\n"," Consider the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum\\limits_{i=1}^n X_i$.\n","\n"," A test statistics called *normal z-statistics* is defined as\n"," $Z_n = \\frac{\\bar{X}_n - \\mu_0}{\\sqrt{\\sigma^2/n}}$.\n"," The resulting test is called a *normal z-test*.\n","\n"," Let $z \\in \\mathbb{R}_{++}$.\n"," Let's reject $H_0$ if $|Z_n| > z$,\n"," i.e. the critical region is $C_{Z_n} = (-\\infty,-z) \\cup (z,\\infty)$\n"," with critical values $\\pm z$.\n","\n"," The power function of the test is\n"," $\\pi(\\mu) = P_\\mu(Z_n \\notin [-z, z]) = 1 - P\\left( -z + \\frac{\\mu_0 - \\mu}{\\sqrt{\\sigma^2/n}} \\leq Z \\leq z + \\frac{\\mu_0 - \\mu}{\\sqrt{\\sigma^2/n}} \\right)$\n"," where $Z$ is a standard normal random variable.\n","\n"," The size of the test is just\n"," $\\alpha = \\pi(\\mu_0) = P_{\\mu_0}(Z_n \\notin [-z, z]) = 1 - P(-z \\leq Z \\leq z)$.\n","\n",""],"metadata":{}},{"cell_type":"markdown","source":["\n"," Let's assume now *unknown mean* $\\mu$\n"," and *unknown variance* $\\sigma^2$.\n","\n"," Define a new test statistics $Z_n^{(a)}$ using the sample mean $\\bar{X}_n$\n"," and the adjusted sample variance:\n"," $Z_n^{(a)} = \\frac{\\bar{X}_n - \\mu_0}{\\sqrt{s_n^2/n}}.\n","\n"," This is called *Student's t-statistics*\n"," and the resulting test is called a *Student's t-test*.\n","\n"," As before,\n"," the critical region is $C_{Z_n^{(a)}} = (-\\infty,-z) \\cup (z,\\infty)$\n"," with critical values $\\pm z$.\n","\n"," The power function of the test is\n"," $\\pi^{(a)}(\\mu) = P_\\mu(Z_n^{(a)} \\notin [-z, z]) = 1 - P\\left( -\\sqrt{\\frac{n-1}{n}} z \\leq W_{n-1} \\leq \\sqrt{\\frac{n-1}{n}} z \\right)$\n"," where $W_{n-1}$ is a non-central standard Student's t random variable\n"," with $n-1$ degrees of freedom and $c = \\frac{\\mu - \\mu_0}{\\sqrt{\\sigma^2/n}}$.\n","\n"," The size of the test is just\n"," $\\alpha = \\pi^{(a)}(\\mu_0) = 1 - P(-z \\leq W_{n-1} \\leq z)$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Hypothesis Tests about the Mean\n","\n"," Assuming normal IID samples\n"," (*known mean* $\\mu$ and *unknown variance* $\\sigma^2$).\n","\n"," Null hypothesis $H_0: \\sigma^2 = \\sigma_0^2$.\n","\n"," Set the test statistic to be $\\chi_n^2 = \\frac{n}{\\sigma_0^2} \\widehat{\\sigma_n^2}$\n"," where $\\widehat{\\sigma_n^2} = \\frac{1}{n}\\sum\\limits_{i=1}^n (X_i-\\mu)^2$.\n","\n"," This test statistic is called *chi-square statistic*\n"," and the test is called a *chi-square test*.\n","\n"," Define the critical region as $C_{\\chi_n^2} = [0,z_1) \\cup (z_2,\\infty]$\n"," where $z_1 < z_2$ and $z_i \\in \\mathbb{R}_{++}$ are the critical values.\n","\n"," The power function of the test:\n"," $\\pi(\\sigma^2) = P_{\\sigma^2}(\\chi_n^2 \\notin [z_1,z_2]) = 1 - P \\left( \\frac{\\sigma_0^2}{\\sigma^2}z_1 \\leq \\kappa_n \\leq \\frac{\\sigma_0^2}{\\sigma^2}z_2 \\right)$.\n","\n"," $\\kappa_n$ is a chi-square random variable with $n$ degrees of freedom.\n","\n"," The size is just $\\alpha = \\pi(\\sigma_0^2) = 1 - P(z_1 \\leq \\kappa_n \\leq z_2)$.\n","\n","\n"," If *unknown mean* $\\mu$ and *unknown variance* $\\sigma^2$.\n","\n"," The test statistic becomes $\\chi_n^2 = \\frac{n-1}{\\sigma_0^2} \\widehat{s_n^2}$\n"," where $s_n$ is the adjusted sample variance.\n","\n"," Everything else is as before.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Estimation Methods\n","\n"," Previously, the concept of estimators was defined.\n","\n"," Let's discuss methods to derive them.\n","\n"," A estimator $\\hat{\\theta}$ is an *extremum estimator*\n"," if it can be represented as a solution of the optimization problem\n"," $\\hat{\\theta} = \\arg \\max\\limits_{\\theta \\in \\Theta} Q(\\theta, \\xi)$\n"," where $Q$ is some function of parameter $\\theta$ and sample $\\xi$.\n","\n"," In *maximum likelihood* (ML) estimation,\n"," $\\hat{\\theta}$ is the ML estimator of $\\theta$\n"," and we maximize the likelihood of the sample,\n"," i.e. $Q(\\theta, \\xi) = L(\\theta; \\xi)$:\n","\n"," 1. If $\\Xi$ is discrete, $L(\\theta;\\xi) = p_\\Xi(\\xi;\\theta)$\n"," is the joint PMF of $\\xi$ associated with the distribution corresponding to $\\theta$ for fixed $\\xi$.\n","\n"," 2. If $\\Xi$ is continuous, $L(\\theta;\\xi) = f_\\Xi(\\xi;\\theta)$\n"," is the joint PDF of $\\Xi$ associated with the distribution corresponding to $\\theta$ for fixed $\\xi$.\n","\n","\n"," In *generalized method of moments* (GMM) estimation,\n"," $Q(\\theta, \\xi) = -d(G(\\theta; \\xi),0)$.\n","\n"," In *nonlinear least squares* (NLS) estimation,\n"," the sample is $n$ realizations $y_i$ of $Y_i$ and $x_i$ of $X_i$.\n","\n"," The NLS estimator is an extremum estimator since it can be written with\n"," $Q(\\theta, \\xi) = -\\sum\\limits_{i=1}^n \\left(y_i - g(x_i;\\theta) \\right)^2$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Maximum Likelihood\n","\n"," ML estimation allows the use of a sample to estimate the parameters of\n"," the probability distribution that generated the sample.\n","\n"," A ML estimator of $\\theta_0$ is\n"," $\\hat{\\theta} = \\arg \\max\\limits_{\\theta \\in \\Theta} L(\\theta;\\xi)$.\n","\n"," The same value of $\\hat{\\theta}$ can be obtained if\n"," $L(\\theta;\\xi)$ is replaced by $l(\\theta;\\xi) = \\ln(L(\\theta;\\xi))$,\n"," where $l$ is called the *log-likelihood*.\n","\n"," It can be shown that $\\hat{\\theta}$ is asymptotically normal around $\\theta_0$:\n"," $\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\to} \\mathcal{N} \\left( 0, \\frac{1}{\\mathrm{var}[\\left. \\nabla_\\theta \\: l(\\theta; X) \\right|_{\\theta = \\theta_0} ]} \\right)$.\n","\n"," i.e. $\\hat{\\theta}_n \\overset{d}{\\to} \\mathcal{N} \\left( \\theta_0, \\frac{1}{n} \\frac{1}{\\mathrm{var}[\\left. \\nabla_\\theta \\: l(\\theta; X) \\right|_{\\theta = \\theta_0} ]} \\right)$\n","\n"," *Information equality*:\n"," $\\mathrm{var}[\\left. \\nabla_\\theta \\: l(\\theta; X) \\right|_{\\theta = \\theta_0} ] = - \\mathrm{E}[\\left. \\nabla_{\\theta\\theta} \\: l(\\theta;X) \\right|_{\\theta = \\theta_0}]$\n","\n"," Note that:\n"," - Left is the covariance matrix of the *score vector* (*Fisher information matrix*).\n"," - Right is the negative of expected value of the Hessian of the log-likelihood.\n"," - The score vector is the gradient of the log-likelihood.\n"," - It can be proved that the expected value of the score is equal to $0$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### ML Estimation for a Poisson Distribution\n","\n"," Assume:\n"," - An IID sequence $\\{X_n\\}$ made up of $n$ independent draws from a Poisson distribution.\n"," - The PMF is $p_X(x_i) = \\begin{cases} \\mathrm{e}^{-\\lambda_0} \\frac{1}{x_i} \\lambda_0^{x_i} & x_i \\in R_X \\\\ 0 & \\textrm{otherwise} \\end{cases}$\n"," where $R_X = \\mathbb{Z}_+$.\n","\n","\n"," By definition, the likelihood and log-likelihood functions are:\n"," $\\begin{align} L(\\lambda;x_1,\\ldots,x_n) &= \\prod\\limits_{i=1}^n p_X(x_i) \\\\ l(\\lambda;x_1,\\ldots,x_n) &= \\ln(L(\\lambda;x_1,\\ldots,x_n)) \\\\ &= -n\\lambda - \\sum\\limits_{i=1}^n \\ln(x_i !) + \\ln(\\lambda) \\sum\\limits_{i=1}^n x_i \\end{align}$\n","\n"," The ML estimator of $\\lambda$ is obtained as follows:\n"," - By definition, $\\hat{\\lambda} = \\arg \\max\\limits_\\lambda l(\\lambda;x_1,\\ldots,x_n)$\n"," - A 1st-order condition for an extremum is $\\begin{align} \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} l(\\lambda;x_1,\\ldots,x_n) &= 0 \\\\ -n + \\frac{1}{\\lambda} \\sum\\limits_{i=1}^n x_i &= 0 \\\\ \\lambda &= \\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\end{align}$\n"," - Thus, $\\hat{\\lambda} = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i$.\n"," - This is just the sample mean $\\bar{x}_n$.\n","\n","\n"," Asymptotical behaviour:\n"," $\\hat{\\theta}_n \\overset{d}{\\to} \\mathcal{N} \\left(\\lambda_0, \\frac{\\lambda_0}{n} \\right)$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### ML Estimation for a Normal Distribution\n","\n"," Same as before: derive $\\hat{\\theta} = \\begin{bmatrix} \\hat{\\mu}_n \\\\ \\widehat{\\sigma^2} \\end{bmatrix}$\n"," from log-likelihood function and ML definition.\n","\n"," $\\hat{\\mu}_n = \\ldots = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i = \\bar{x}_n$, i.e. the sample mean.\n","\n"," $\\widehat{\\sigma^2}_n = \\ldots = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i - \\hat{\\mu}_n)^2 = S_n$, i.e. the unadjusted sample mean.\n","\n"," Asymptotical behaviour:\n"," $\\hat{\\theta}_n = \\begin{bmatrix} \\hat{\\mu}_n \\\\ \\widehat{\\sigma^2} \\end{bmatrix} \\overset{d}{\\to} \\mathcal{N} \\left( \\begin{bmatrix} \\lambda_0 \\\\ \\sigma_0^2 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2/n & 0 \\\\ 0 & 2 \\sigma_0^4/n \\end{bmatrix} \\right)$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### ML Estimation for a Normal Linear Regression Model\n","\n"," Objective: estimate the parameters of the linear regression model\n"," $y_i = x_i \\beta_0 + \\epsilon_i$\n"," where $x_i$ is the $1 \\times K$ vector of regressors,\n"," $\\beta_0$ is the $K \\times 1$ vector of regression coefficients\n"," and $\\epsilon_i$ is an unobservable error term.\n","\n"," In matrix form, $y = X \\beta_0 + \\epsilon$\n"," where $y$ is the $N \\times 1$ vector of observations,\n"," $N \\times K$ is the matrix of regressors,\n"," $\\epsilon$ is $N \\times 1$ vector of error terms.\n","\n"," Assume $\\epsilon$ has a multivariate normal distribution conditional on $X$ with mean $0$\n"," and covariance $\\sigma_0^2 I_N = \\mathrm{var}[\\epsilon_i | X] I_N$.\n","\n"," Thus, $f_Y(y_i|X) = \\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\mathrm{e}^{-(y_i - x_i \\beta_0)^2 /(2 \\sigma_0^2)}$\n","\n"," The parameters to be estimated are just $\\beta, \\sigma^2$.\n","\n"," The log-likelihood function:\n"," $l(\\beta,\\sigma^2; y, X) = \\ln(f_Y(y_i|X)) = \\ldots$\n","\n"," Solving the ML equation:\n"," - $\\hat{\\beta}_N = (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y$\n"," - $\\widehat{\\sigma^2}_N = \\frac{1}{N} \\sum\\limits_{i = 1}^N (y_i - x_i \\hat{\\beta}_N)^2$\n","\n","\n"," $\\hat{\\beta}_N$ is just the usual OLS estimator\n"," and $\\widehat{\\sigma^2}_N$ is the unadjusted sample variance of the residuals $e_i = y_i - x_i \\hat{\\beta}_N$.\n","\n"," Asymptotical behaviour:\n"," $\\hat{\\theta}_N = \\begin{bmatrix} \\hat{\\beta}_N \\\\ \\widehat{\\sigma^2}_N \\end{bmatrix} \\overset{d}{\\to} \\mathcal{N} \\left( \\begin{bmatrix} \\beta_0 \\\\ \\sigma_0^2 \\end{bmatrix}, \\begin{bmatrix} \\frac{\\sigma_0^2}{n} (\\mathrm{E}[x_i^\\mathsf{T} x_i])^{-1} & 0 \\\\ 0 & 2 \\frac{\\sigma_0^4}{n} \\end{bmatrix} \\right)$.\n",""],"metadata":{}},{"cell_type":"markdown","source":["## ML Estimation Algorithm\n","\n"," The ML optimization problem: $\\hat{\\theta} = \\arg \\max\\limits_{\\theta \\in \\Theta} \\ln \\left( L(\\theta; \\xi) \\right)$\n","\n"," where:\n"," - $\\Theta$ is the parameter space;\n"," - $\\xi$ is the sample and the realization of the random vector $\\Xi$;\n"," - $L(\\theta;\\xi)$ is the likelihood of the sample,\n"," i.e. the joint PMF $p_\\Xi(\\xi;\\theta)$ or PDF $f_\\Xi(\\xi;\\theta)$ where $\\xi$ is fixed;\n"," - $\\hat{\\theta}$ is the estimator and the value for which the log-likelihood is maximized.\n","\n","\n"," In some special cases, this problem could be solved analytically,\n"," i.e. $\\hat{\\theta}$ can be written as a function of $\\xi$.\n"," In others, numerical algorithms are necessary.\n","\n"," Avoid constrained optimization.\n"," One way is to reparameterize:\n"," - $\\theta \\in (0, \\infty)$ becomes\n"," $\\theta^\\prime = \\mathrm{e}^\\theta$.\n"," - $\\theta \\in (0, 1)$ becomes $\\theta^\\prime = \\frac{\\mathrm{e}^\\theta}{1 + \\mathrm{e}^\\theta}$.\n","\n","\n"," Another way is to use a *penalty function* $p$:\n"," - Given the constrained problem $\\hat{\\theta} = \\arg \\max\\limits_{\\theta \\in \\Theta} \\ln \\left( L(\\theta; \\xi) \\right)$,\n"," - solved the associated unconstrained problem $\\hat{\\theta} = \\arg \\max\\limits_{\\theta \\in \\mathbb{R}^k} \\left[ \\ln \\left( L(\\theta; \\xi) \\right) - p(\\theta) \\right]$,\n"," - where $p(\\theta) = \\begin{cases} 0 & \\theta \\in \\Theta \\\\ \\infty & \\textrm{otherwise} \\end{cases}$.\n","\n","\n"," ### ML Hypothesis Testing\n","\n"," Assume that some unknown parameter $\\theta \\in \\Theta$ ($\\Theta \\subseteq \\mathbb{R}^p$)\n"," with true value $\\theta_0$ has been estimated by ML methods.\n","\n"," We now want to test the null hypothesis $H_0: \\theta_0 \\in \\Theta_R$\n"," where $\\Theta_R \\subset \\Theta$.\n","\n"," Popular tests:\n","\n"," 1. *Wald test* (uses the unrestricted estimate $\\hat{\\theta}$)\n","\n"," 2. *Score test* (uses the restricted estimate $\\hat{\\theta}^R$)\n","\n"," 3. *Likelihood ratio test* (uses both estimates)\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### Wald Test\n","\n"," Let $\\hat{\\theta}_n$ be the estimator/estimate of a $p \\times 1$ parameter $\\theta_0$\n"," obtained by ML estimation\n"," ($\\hat{\\theta}_n = \\arg \\max\\limits_{\\theta \\in \\Theta} \\ln \\left( L(\\theta; \\xi_n) \\right)$).\n"," with sample size $n$.\n","\n"," The Wald statistic is\n"," $W_n = n\\: g(\\hat{\\theta}_n)^\\mathsf{T} \\: \\left[ J_g(\\hat{\\theta}_n) \\hat{V}_n J_g(\\hat{\\theta}_n)^\\mathsf{T} \\right]^{-1} g(\\hat{\\theta}_n)$.\n","\n"," $g: \\mathbb{R}^p \\rightarrow \\mathbb{R}^{r}$ is a vector-valued function such that\n"," $\\Theta_R = \\{ \\theta \\in \\Theta \\: : \\: g(\\theta) = 0 \\}$.\n","\n"," $J_g(\\theta)$ is the Jacobian of $g$ (the $r \\times p$ matrix of partial derivatives of $g$\n"," with respect to $\\theta$).\n","\n"," $\\hat{V}_n$ is a consistent estimate of the asymptotic covariance matrix of $\\hat{\\theta}_n$,\n","\n"," There are three estimates of $\\hat{V}_n$:\n","\n"," 1. *Outer product of gradients* (OPG) estimate,\n"," $\\hat{V}_n^\\textrm{O} = \\left(\\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla_\\theta \\ln( f_X(x_i;\\hat{\\theta}_n)) \\: \\nabla_\\theta \\ln( f_X(x_i;\\hat{\\theta}_n))^\\mathsf{T} \\right)^{-1}$\n"," 2. *Hessian* estimate,\n"," $\\hat{V}_n^\\textrm{H} = \\left(-\\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla_{\\theta,'theta}\\ln( f_X(x_i;\\hat{\\theta}_n)) \\right)^{-1}$\n"," 3. *Sandwich* estimate,\n"," $\\hat{V}_n^\\textrm{S} = \\hat{V}_n^\\textrm{H} \\left( \\hat{V}_n^\\textrm{O} \\right)^{-1} \\hat{V}_n^\\textrm{H}$\n","\n","\n"," Under the null hypothesis $H_0$, i.e. $g(\\theta_0) = 0$, $W_n$ converges in distribution\n"," to a chi-square distribution with $r$ DOFs.\n","\n"," The Wald test is performed by fixing some critical value $z$ and\n"," rejecting $H_0$ is $W_n > z$.\n","\n"," The size of test is $\\alpha = P(W_n > z) = 1 - P(W_n \\leq z) \\approx 1 - F(z)$,\n"," where $F$ is the CDF of a chi-square distribution with $n$ DOFs.\n","\n"," Alternatively, we set $z = F^{-1}(1 - \\alpha)$.\n","\n"," Example:\n"," - Let $\\Theta = \\mathbb{R}^2$ and sample size $n = 90$.\n"," - Suppose $\\hat{\\theta}_n = \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}$\n"," and $\\hat{V}_n = \\begin{bmatrix} 10 & 5 \\\\ 5 & 10 \\end{bmatrix}$.\n"," - Test restriction $\\theta_{0, 1} + \\theta_{0, 2} = 10$.\n"," - Then, $g : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$ is $g(\\theta) = g(\\theta_1, \\theta_2) = \\theta_1 + \\theta_2 - 10$.\n"," and $r = 1$.\n"," - The Jacobian of $g$ is $J_g(\\theta) = \\left[\\frac{\\partial g}{\\partial \\theta_1} \\: \\frac{\\partial g}{\\partial \\theta_2} \\right] = [1 \\: 1]$.\n"," - $g(\\hat{\\theta}_n) = 5 + 6 - 10 = 1$ and $J_g(\\hat{\\theta}_n) = [1 \\: 1]$.\n"," - The Wald statistic is thus $W_n = \\ldots = 3$.\n"," - Suppose we want size $\\alpha = 5%$.\n"," - Then, the critical value is $z = F^{-1}(1 - \\alpha) = 3.84$\n"," where $F(z)$ is the chi-square CDF with $1$ DOF.\n"," - Since $W_n < z$, the null hypothesis is not rejected."],"metadata":{}},{"cell_type":"markdown","source":[" ### Score Test\n","\n"," Define $\\hat{\\theta}_n^R$ as the estimate obtained by ML estimation\n"," over the restricted parameter space $\\Theta_R$.\n","\n"," The score or *Lagrange multiplier* (LM) statistic is\n"," $\\textrm{LM}_n = \\frac{1}{n} \\nabla_\\theta \\left( \\ln \\left[L(\\hat{\\theta}_n^R;\\xi_n )\\right] \\right)^\\mathsf{T} \\: \\hat{V}_n \\: \\nabla_\\theta \\left( \\ln \\left[L(\\hat{\\theta}_n^R;\\xi_n )\\right] \\right)$.\n","\n"," $\\nabla_\\theta \\left( \\ln \\left[L(\\hat{\\theta}_n^R;\\xi_n )\\right] \\right)$\n"," is the score, i.e. the gradient of the log-likelihood function.\n","\n"," Same convergence as Wald statistic.\n","\n"," The test is performed as in the Wald test."],"metadata":{}},{"cell_type":"markdown","source":[" ### Likelihood Ratio Test\n","\n"," Define $\\hat{\\theta}_n$ and $\\hat{\\theta}_n^R$ as before.\n","\n"," The likelihood ratio statistic is\n"," $\\textrm{LR}_n = \\ln \\left[ \\left( \\frac{L(\\hat{\\theta}_n;\\xi_n )}{L(\\hat{\\theta}_n^R;\\xi_n )} \\right)^2 \\right] = 2 \\left( \\ln \\left[L(\\hat{\\theta}_n;\\xi_n )\\right] - \\ln \\left[L(\\hat{\\theta}_n^R;\\xi_n )\\right] \\right)$\n","\n"," Same convergence as Wald statistic.\n","\n"," The test is performed as in the Wald test.\n",""],"metadata":{}},{"source":["  "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ### Model Selection Criteria\n","\n"," Consider, for some sample $\\xi_N$ enerated by unknown CDF $f$,\n"," $M = 2$ models:\n","\n"," 1. Normal distribution with joint CDF $f_1(\\xi_N;\\theta_1) = \\prod\\limits_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right) $\n","\n"," 2. Exponential distribution with joint CDF $f_2(\\xi_N;\\theta_2) = \\prod\\limits_{i=1}^n \\lambda \\exp(-\\lambda x_i) 1_{\\{ x_i > 0\\}}$.\n","\n","\n"," Then, the parameter vectors are $\\theta_1 = [\\mu \\; \\sigma^2]$ and $\\theta_2 = \\lambda$.\n","\n"," Then, the model parameters $\\theta_m$ are estimated by ML estimation\n"," to give estimators $\\hat{\\theta}_1, \\ldots, \\hat{\\theta}_M$.\n","\n"," Let $S$ be the index of the chosen model, i.e. $S \\in [1, \\ldots, M]$.\n","\n"," General criterion:\n"," - Measure dissimilarity between the $m$ model CDF with the true CDF by the KL divergence.\n"," - $D_\\textrm{KL}(f(\\xi_N), f_m(\\xi_N;\\hat{\\theta}_m)) = \\mathrm{E} \\left[ -\\ln \\left( \\frac{f_m(\\xi_N; \\hat{\\theta}_m)}{f(\\xi_N)} \\right) \\right]$.\n"," - The expected value is with respect to the true CDF $f$.\n"," - Ideally, $S = \\arg \\min\\limits_{m=1, \\ldots, M} \\mathrm{E}[D_\\textrm{KL}(f(\\xi_N), f_m(\\xi_N;\\hat{\\theta}_m))]$\n"," where this $\\mathrm{E}$ is over the sampling distribution of $\\hat{\\theta}_m$.\n"," - But $f$ and sampling distribution of $\\hat{\\theta}_m$ are unknown!\n","\n","\n"," Popular criteria:\n"," - *Akaike information criterion* (AIC)\n"," - *Corrected Akaike information criterion* (CAIC)\n"," - *Bayesian information criterion* (BIC)\n","\n","\n"," Akaike information criterion:\n"," - Let $S = \\arg \\min\\limits_{m=1, \\ldots, M} \\textrm{AIC}_m$,\n"," - where $\\textrm{AIC}_m = K_m - \\ln(f_m(\\xi_n;\\hat{\\theta}_m))$\n"," - where $K_m = \\mathrm{dim}(\\hat{\\theta}_m)$\n","\n","\n"," Corrected Akaike information criterion:\n"," - $\\textrm{CAIC}_m = \\textrm{AIC}_m + \\frac{2 K_m^2 + 2 K_m}{N - k_m - 1}$\n"," - This approximation is more precise for smaller samples.\n","\n","\n"," Bayesian information criterion:\n"," $\\textrm{BIC}_m = K_m \\ln(N) - 2 \\ln(f_m(\\xi_n;\\hat{\\theta}_m))$\n","\n"," All these criteria penalize the dimensionality/complexity $K_m$ of the model.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Conditional Models\n","\n"," Recall that a statistical model consists of:\n"," - a sample $\\xi$ that is a realization of a random vector $\\Xi$;\n"," - its unknown joint CDF $F_\\Xi(\\xi)$;\n"," - the sample is used to infer characteristics of $F_\\Xi(\\xi)$;\n"," - a model for $\\Xi$ is used to make such inferences\n"," - such model is just a set of joint CDFs to which $F_\\Xi(\\xi)$ is assumed to belong.\n","\n","\n"," In a conditional model, the sample is split into inputs $x$ and outputs $y$:\n"," $\\xi = [y \\: x]$.\n","\n"," The object of interest:\n"," the conditional CDF of the outputs given the inputs\n"," $F_{Y|X=x}(y)$.\n","\n"," The distribution of the inputs $x$ are ignored.\n","\n"," Terminology:\n"," - *Regression model*, a conditional model wherein the output is continuous.\n"," - *Classication model*, a conditional model wherein the output is discrete.\n"," - Inputs: predictors, independent/explanatory variables, features, regressors.\n"," - Outputs: predictands, dependent/response/target variables, regressands.\n","\n","\n"," Examples:\n"," - linear regression model;\n"," - logistic classication model."],"metadata":{}},{"cell_type":"markdown","source":[" ## Linear Regression\n","\n"," Assume a sample of realizations $(y_i, x_i)$ for $i = 1, \\ldots, N$;\n"," the outputs $y_i$ are scalars while the associated inputs are $1 \\times K$ vectors.\n","\n"," Postulate: $y_i = x_i \\beta + \\epsilon_i$.\n","\n"," $\\beta$ is a $K \\times 1$ vector of constants called *regression coefficients*;\n"," $\\epsilon_i$ is an unobservable error term.\n","\n"," Matrix notation: $y = X \\beta + \\epsilon$\n","\n"," $y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix}, X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_N \\end{bmatrix}, \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix}$\n","\n"," $X$ is a $N \\times K$ matrix often called the *design matrix*.\n","\n"," It is assumed that the first entry of $x_i$ (or first column of $X$) is equal to $1$;\n"," the corresponding $\\beta$ is called *intercept*.\n","\n"," Inference about this model is carried out as point and set estimation and hypothesis testing\n"," about $\\beta$ and $\\mathrm{var}[\\epsilon]$.\n","\n"," OLS estimation:\n"," - A common estimator of $\\beta$ is the *ordinary least squares* (OLS) estimator.\n"," - Definition: $\\hat{\\beta} = \\arg \\min\\limits_b \\sum\\limits_{i=1}^N (y_i - x_i b)^2$.\n","\n","\n"," A popular linear regression model is\n"," the *normal linear regression model* (NLRM) by assuming:\n"," - $X$ has full rank (i.e. $X^\\mathsf{T}X$ is invertible and $\\hat{\\beta}$ as defined)\n"," - $\\epsilon$ has a multivariate normal distribution\n"," conditional on $X$ and all $\\epsilon_i$ are mutually independent and have constant variance\n"," (i.e. $\\mathrm{var}[\\epsilon] = \\sigma^2 I$).\n","\n","\n"," Properties:\n"," - If $X$ has full rank, then $\\hat{\\beta} = (X^\\mathsf{T}X)^{-1} X^\\mathsf{T} y$.\n"," - $\\hat{\\beta}$ is unbiased, i.e. $\\mathrm{E}[\\hat{\\beta}] = \\mathrm{E}[\\hat{\\beta}|X] = \\beta$\n"," - $\\mathrm{var}[\\hat{\\beta}|X] = \\sigma^2 (X^\\mathsf{T} X)^{-1}$.\n"," - $\\hat{\\beta}$ is consistent, i.e. i.e. converges to $\\beta_0$ as $N \\rightarrow \\infty$.\n"," - $\\hat{\\beta}$ is asymptotically normal.\n"," - The OLS estimator of the error terms $\\sigma^2$ is\n"," the adjusted sample variance of the residuals ($e_i = y_i - x_i \\hat{\\beta}$),\n"," $\\widehat{\\sigma^2} = \\frac{1}{N - K} \\sum\\limits_{i = 1}^N e_i^2$.\n"," - $\\widehat{\\sigma^2}$ is unbiased,\n"," i.e. $\\mathrm{E}[\\widehat{\\sigma^2}] = \\mathrm{E}[\\widehat{\\sigma^2}|X] = \\sigma^2$.\n"," - $\\widehat{\\sigma^2}$ has a gamma distribution with parameters $N-K$ and $\\sigma^2$.\n","\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### R squared of a Linear Regression\n","\n"," The (unadjusted) *R squared* of a linear regression is denoted $R^2$\n"," and is defined as $R^2 = 1 - \\frac{S_e^2}{S_y^2}$.\n","\n"," $S_y^2$ is the unadjusted sample variance of the residuals,\n"," $S_y^2 = \\frac{1}{N} \\sum\\limits_{i=1}^N (y_i - \\bar{y})^2$\n"," where $\\bar{y} = \\frac{1}{N}\\sum\\limits_{i=1}^N y_i$ is the sample mean.\n","\n"," $S_e^2$ is the unadjusted sample variance of the outputs,\n"," $S_e^2 = \\frac{1}{N} \\sum\\limits_{i=1}^N (e_i - \\bar{e})^2$\n"," where $\\bar{e} = 0$.\n","\n"," $R^2$ is a goodness-of-fit measure; $R^2 = 1$ when perfect, $0<R^2<1$ otherwise.\n","\n"," The *adjusted R squared* is $r^2 = 1 - \\frac{N-1}{N-K} \\frac{S_e^2}{S_y^2} = 1 - \\frac{s_e^2}{s_y^2}$.$.\n"," where $s_y^2 = \\frac{1}{N-1} \\sum\\limits_{i=1}^N (y_i - \\bar{y})^2$\n"," $s_e^2 = \\frac{1}{N-K} \\sum\\limits_{i=1}^N (e_i - \\bar{e})^2$\n"," are the adjusted sample variances.\n","\n"," $\\frac{N-1}{N-K}$ is called a *degrees of freedom adjustment*.\n","\n","\n"," ### Hypothesis Testing in Linear Regression\n","\n"," A *t test*:\n"," - Test a restriction on a single coefficient.\n"," - Null hypothesis, $H_0: \\beta_k = q$.\n"," - Test statistic is $t = \\frac{\\hat{\\beta}_k - q}{\\sqrt{\\hat{\\sigma^2} S_{kk}}}$\n"," - $S_kk$ is the $k$-th diagonal entry of $(X^\\mathsf{T}X)^{-1}$.\n"," - $t$ has a standard Student's t distribution with $N-K$ DoFs.\n"," - $H_0$ is rejected if $t$ is outside the acceptance region.\n","\n","\n"," A *F test*:\n"," - Testing a set of linear restrictions.\n"," - Null hypothesis, $H_0: A\\beta = q$\n"," - where $A$ is a $L \\times K$ matrix, $q$ is a $L \\times 1$ vector,\n"," and $L$ is the number of restrictions.\n"," - Test statistic is $F = \\frac{1}{L} (A \\hat{\\beta} - q)^\\mathsf{T} \\left[\\widehat{\\sigma^2} A \\: (X^\\mathsf{T} X)^{-1} A^\\mathsf{T} \\right] (A \\hat{\\beta} - q)$.\n"," - $F$ has an F distribution with $L$ and $N-K$ DoFs.\n","\n","\n"," Since the ML estimator of $\\beta$ is equal to the OLS estimator,\n"," the usual ML-based tests can be used on $\\beta$.\n","\n"," If the OLS estimator is asymptotically normal:\n"," - *z test*: like the t test but using $z_N = \\frac{\\hat{\\beta}_k - q}{\\sqrt{\\frac{\\hat{V}_{kk}}{N}}}$\n"," where $\\hat{V}$ is the asymptotic covariance matrix of $\\beta$.\n"," - $z_N$ has a standard normal distribution.\n"," - *chi-square test*: like the F test but using $\\chi_N^2 = (A \\hat{\\beta} - q)^\\mathsf{T} \\left[\\frac{1}{N} A \\: \\hat{V} A^\\mathsf{T} \\right]^{-1} (A \\hat{\\beta} - q) $.\n"," - $\\chi_N^2$ has a chi-square distribution with $L$ DOFs.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### Gauss-Markov Theorem\n","\n"," Under certain conditions, the OLS estimator of $\\beta$ of a linear regression model,\n"," $\\hat{\\beta} = (X^\\mathsf{T}X)^{-1} X^\\mathsf{T} y$,\n"," is the *best linear unbiased estimator* (BLUE),\n"," i.e. the one with the smallest variance.\n","\n"," ### Generalized Least Squares\n","\n"," The *generalized least squares* (GLS) estimator of $\\beta$\n"," is a generalization of the OLS estimator.\n","\n"," The GLS estimator is used when the OLS one is not BLUE.\n"," The Gauss-Markov theorem does not apply\n"," when no homoskedasticity ($\\epsilon_i$'s have different variances)\n"," or presence of serial correlation (covariance between $\\epsilon_i$'s is not zero).\n","\n"," GLS estimator:\n"," - $\\mathrm{var}[\\epsilon|X] = V$\n"," where $V$ is a $N \\times N$ symmetric positive definite matrix.\n"," - Write $V = \\Sigma \\Sigma^\\mathsf{T}$\n"," - Write $\\Sigma^{-1} y = \\Sigma^{-1} X \\beta + \\Sigma^{-1}\\epsilon \\: \\rightarrow \\: \\tilde{y} = \\tilde{X} \\beta + \\tilde{epsilon}$.\n"," - Then, $\\hat{\\beta}_\\textrm{GLS} = (\\tilde{X}^\\mathsf{T}\\tilde{X})^{-1} \\tilde{X}^\\mathsf{T} \\tilde{y} = (X^\\mathsf{T} V^{-1} X)^{-1} X^\\mathsf{T} V^{-1} y$.\n"," - $\\hat{\\beta}_\\textrm{GLS}$ is BLUE.\n","\n","\n"," The OLS problem was to solve $\\hat{\\beta}_\\textrm{OLS} = \\arg \\min\\limits_b (y - X \\; b)^\\mathsf{T} (y - X \\; b)$.\n","\n"," The GLS problem is instead $\\hat{\\beta}_\\textrm{GLS} = \\arg \\min\\limits_b (y - X \\; b)^\\mathsf{T} V^{-1} (y - X \\; b)$.\n","\n"," When $V$ is diagonal (i.e. uncorrelated $\\epsilon_i$'s),\n"," the GLS estimator is called the *weighted least squares* (WLS) estimator,\n"," $\\hat{\\beta}_\\textrm{GLS} = \\arg \\min\\limits_b (y - X \\; b)^\\mathsf{T} V^{-1} (y - X \\; b) = \\sum\\limits_{i=1}^N V_{ii}^{-1} (y_i - X_i b)^2$.\n","\n"," $V$ is usually unknown.\n"," In *feasible* GLS,\n"," it is replaced by its estimate $\\hat{V}$:\n"," $\\hat{\\beta}_\\textrm{FGLS} = (X^\\mathsf{T} \\hat{V}^{-1} X)^{-1} X^\\mathsf{T} \\hat{V}^{-1} y$\n","\n"," However, there is no general method for estimating $V$.\n"," One way is to run OLS once,\n"," assume a diagonal $V$, and take $\\hat{V}_{ii} = \\alpha \\hat{V}_{i-1,i-1} + (1 - \\alpha) \\hat{\\epsilon}_i$\n"," where $\\epsilon_i = y_i - X_i \\hat{\\beta}_\\textrm{OLS}$.\n",""],"metadata":{}},{"source":["              "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ### Multicollinearity\n","\n"," *Multicollinearity* is a problem that occurs when one or more regressors are highly correlated.\n"," Even when $N \\gg 0$, $\\hat{\\beta}_\\textrm{OLS}$ still has high variance.\n","\n"," In such cases, the matrix $X^\\mathsf{T}X$ is\n"," either not full rank or close to be rank deficient.\n","\n"," A measure of multicollinearity is the *variance inflation factor* (VIF):\n"," $\\mathrm{VIF}_k = \\frac{1}{1 - R_k^2}$\n"," where $R_k^2$ is the R squared of a regression\n"," in which $X_{\\cdot k}$ is the dependent variable\n"," and $X_{\\cdot j}$ ($j \\neq k$) are the dependent variables.\n","\n"," Watch out for when $\\mathrm{VIF}_k > 10$\n"," or the *condition number* of $X^\\mathsf{T}X$ is $>20$.\n","\n"," Remedies:\n"," - Increase sample size $N$.\n"," - Drop the regressors with high $\\mathrm{VIF}$.\n"," - Replace high-$\\mathrm{VIF}$ regressors with a linear combination of them.\n"," - Use regularization methods like *ridge* (add $\\lambda |b|^2$ to the OLS loss function),\n"," *lasso* (*least absolute shrinkage and selection operator*,\n"," add $\\lambda |b|$ to the OLS loss function), and *elastic net* (both).\n"," - Use *Bayesian regression*.\n","\n","\n","\n","\n"," ### Ridge Regression\n","\n"," *Ridge regression* is a linear regression model whose coefficients $\\beta$\n"," are not estimated by OLS but by the ridge estimator $\\hat{\\beta}_\\lambda$\n"," where $\\lambda \\leq 0$ (it is biased but has smaller variance).\n","\n"," $\\hat{\\beta}_\\lambda = \\arg \\min\\limits_b \\sum\\limits_{i=1}^N (y_i - x_i b) + \\lambda \\sum\\limits_{k=1}^K b_k^2$\n","\n"," Solution: $\\hat{\\beta}_\\lambda = (X^\\mathsf{T} X + \\lambda I)^{-1} X^\\mathsf{T} y$.\n","\n"," Assuming $\\mathrm{E}[\\epsilon|X] = 0$ and $\\mathrm{var}[\\epsilon|X] = \\sigma^2 I$,\n"," - the bias is $\\mathrm{E}[\\hat{\\beta}_\\lambda|X] - \\beta = [(X^\\mathsf{T} X + \\lambda I)^{-1} - (X^\\mathsf{T} X)^{-1}] X^\\mathsf{T} X \\: \\beta$.\n"," - the variance is $\\mathrm{var}[\\hat{\\beta}_\\lambda|X] = \\sigma^2 (X^\\mathsf{T} X + \\lambda I)^{-1} X^\\mathsf{T} X (X^\\mathsf{T} X + \\lambda I)^{-1}$.\n","\n","\n"," Ridge regression is so-called because the $\\lambda I$ term is like adding a diagonal *ridge*\n"," to the matrix $X^\\mathsf{T} X$.\n","\n"," From the bias-variance decomposition of the mean-squared error:\n"," - $\\mathrm{MSE}(\\hat{\\beta}_\\lambda|X) = \\mathrm{E}[|\\hat{\\beta}_\\lambda|^2 \\; |X] = \\mathrm{tr}(\\mathrm{var}[\\hat{\\beta}_\\lambda | X]) + |\\mathrm{Bias}[\\hat{\\beta}_\\lambda | X]|^2$.\n"," - For OLS,\n"," $\\mathrm{MSE}(\\hat{\\beta}_\\textrm{OLS}|X) = \\mathrm{tr}(\\mathrm{var}[\\hat{\\beta}_\\textrm{OLS} | X])$\n"," - It can be shown that $\\mathrm{MSE}(\\hat{\\beta}_\\lambda|X) < \\mathrm{MSE}(\\hat{\\beta}_\\textrm{OLS}|X)$\n"," for some value of $\\lambda$.\n","\n","\n"," A way to find $\\lambda$ (a *hyperparameter*) is to use\n"," k-fold cross-validation:\n"," - split sample into two sets (training set and test set);\n"," - split training set into $k$ subsets;\n"," - for $i = 1,\\ldots,k$,\n"," drop the $i$-th subset to merge the remainder into a training subset for the model\n"," (find $\\hat{\\beta}_{\\lambda_i}$) and use the $i$-th subset as the *validation set*\n"," (calculate the resulting MSE);\n"," - find the value of $\\lambda$ that minimizes the MSE\n"," and apply it to the test set for performance metric.\n","\n","\n"," The ridge estimator is *not* scale-invariant;\n"," always use *standardized variables*\n"," (substract each variable by its mean and divide by its standard deviation;\n"," drop the intercept $\\beta_i = 1$ since it is not standardizable).\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Classification Models\n","\n"," A *classification model* is a conditional model\n"," wherein the output variable has a *discrete* probability distribution\n","\n"," Two types:\n","\n"," 1. *binary* classification models\n"," wherein the output has a Bernoulli distribution conditional on the inputs;\n"," 2. *multinomial* classification models\n"," wherein the output has a multinoulli distribution conditional on the inputs.\n","\n","\n"," Let $X = [X_1 \\: \\ldots \\: X_K]$ be a $K \\times 1$ random vector with support\n"," $R_X = \\left\\{ x \\in \\{0,1\\}^K \\: : \\: \\sum\\limits_{i=1}^K x_i = 1 \\right\\}$.\n"," $X$ has a *multinoulli* distribution with probabilities $p_1,\\ldots,p_K$\n"," if its joint PMF is $p_X(x_1,\\ldots,x_K) = \\begin{cases} \\prod\\limits_{i=1}^K p_i^{x_i} & (x_1,\\ldots,x_K) \\in R_X \\\\ 0 & \\textrm{otherwise} \\end{cases}$.\n","\n"," Example:\n"," - Suppose an output variable can take on one of three values (`red`,`green`,`blue`).\n"," - Then, it can be represented as a multinoulli random vector\n"," with realizations $\\begin{cases} [1\\:0\\:0] & \\textrm{red} \\\\ [0\\:1\\:0] & \\textrm{green} \\\\ [0\\:0\\:1] & \\textrm{blue} \\end{cases}$.\n","\n","\n"," Assumptions:\n"," - A sample of data $(y_i,x_i)$ for $i = 1,\\ldots,N$ is observed.\n"," - Each input $x_i$ is a $1 \\times K$ vector.\n"," - Each output $y_i$ can be any $1 \\times J$ vector whose entries are $c_j = \\delta_{ij}$.\n"," - There exists $J$ functions $f_j$ such that $P(y_i = c_j|x_i) = f_j(x_i;\\theta)$ for all $i,j$.\n","\n","\n"," *Binary logistic* (or *logit*) classification model\n"," - The conditional PMF of $y_i$ is $p_{Y_i|X=x_i}(y_i) = \\begin{cases} \\mathrm{sigm}(x_i\\beta) & y_i = 1 \\\\ 1 - \\mathrm{sigm}(x_i\\beta) & y_i = 0 \\\\ 0 & \\textrm{otherwise} \\end{cases}$.\n"," - $\\beta$ is a $K \\times 1$ vector of coefficients and $\\mathrm{sigm}(t) = \\frac{1}{1 + \\mathrm{e}^{-t}}$.\n"," - Thus, $J = 2$, $f_1(x_i;\\theta) = P(y_i = 1|x_i) = \\mathrm{sigm}(x_i\\beta)$, and $f_2(x_i;\\theta) = P(y_i = 0|x_i) = 1 - \\mathrm{sigm}(x_i\\beta)$.\n","\n","\n"," *Multinomial logistic* classification model (also called *softwax model*):\n"," - $J \\leq 2$.\n"," - The conditional PMF of $y_i$ is $f_j(x_i;\\theta) = P(y_i = c_j|x_i) = \\frac{\\mathrm{e}^{x_i \\beta_j}}{\\sum\\limits_{k=1}^K \\mathrm{e}^{x_i\\beta_k}}$.\n"," - Each class $j$ corresponds to a $K \\times 1$ coefficient vector $\\beta_j$.\n"," - The vector of parameters is just $\\theta = [\\beta_1 \\: \\ldots \\: \\beta_J]$.\n","\n","\n"," The parameters of a multinomial logistic classification model\n"," can be estimated by the ML estimation method.\n","\n"," The likelihood of a sample point $(y_i,x_i)$ is $L(\\theta;y_i,x_i) = \\prod\\limits_{j=1}^J f_j(x_i;\\theta)^{y_{ij}}$.\n"," where $y_{ij} = \\delta_{ij}$ is the $i$-th component of the multinoulli vector $y_i$.\n","\n"," For the whole sample, $L(\\theta;y,x) = \\prod\\limits_{i=1}^N L(\\theta;y_i,x_i) = \\prod\\limits_{i=1}^N \\prod\\limits_{j=1}^J f_j(x_i;\\theta)^{y_{ij}}$.\n","\n"," The log-likelihood is just $l(\\theta;y,x) = \\ln(L(\\theta;y,x))$.\n","\n"," The ML estimator of the parameter $\\theta$ is\n"," then the solution of the optimization problem\n"," $\\hat{\\theta} = \\arg \\max_\\limits{\\theta} l(\\theta;y,x)$.\n",""],"metadata":{}},{"cell_type":"markdown","source":["\n"," ### Binary Logistic Classification Model\n","\n"," The logistic function $\\mathrm{sigm}(t)$ is used to tackle the classification problem\n"," as in a linear regression model (as linear combination of $x_i \\beta$)\n"," while ensuring the terms $x_i \\beta$ are between $0$ and $1$.\n","\n"," Alternatively, consider $z_i = x_i \\beta + \\epsilon$ such that\n"," $y_i = \\begin{cases} 1 & z_i \\leq 0 \\\\ 0 & z_i < 0 \\end{cases}$.\n","\n"," Then,\n"," $\\begin{align} P(y_i = 1|x_i) &= P(z_i \\leq 0|x_i)\\\\&= P(\\epsilon_i \\leq x_i\\beta|x_i)\\\\&=F(x_i\\beta)\\end{align}$.\n","\n"," Thus, the logit model is just the assumption that\n"," $\\epsilon_i$ has the specific CDF $F_{\\epsilon_i}(x_i \\beta) = \\frac{1}{1 - \\exp(x_i \\beta)}$.\n","\n"," If $\\epsilon_i$ has a standard normal CDF, we would have the so-called *probit* model.\n",""],"metadata":{}},{"source":["  "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":["\n"," ## Markov Chain\n","\n"," A *Markov chain* is a sequence of random variables/vectors $\\{X_n\\}$\n"," that possess the *Markov property*:\n"," any given term $X_n$ is conditionally independent\n"," of all terms preceding $X_{n-1}$,\n"," i.e. $F(x_n|x_{n-1}, x_{n-2}, \\ldots, x_1) = F(x_n|x_{n-1})$.\n","\n"," The *state space* $S$ of a Markov chain $\\{X_n\\}$ is\n"," the set of all possible realizations of terms of the chain,\n"," i.e. the support of any term $X_n$ is included in $S$.\n","\n"," $S$ is finite:\n"," - $S = \\{s_1, \\ldots, s_J\\}$\n"," - Specify an initial probabiliy distribution for $X_1$, $P(X_1 = s_j) = \\pi_{1j}$.\n"," - Choose a $J \\times J$ *transition probability matrix $P$, $P_{ij} = P(X_n = s_j|X_{n-1} = s_i) \\: \\forall \\; n,i,j$.\n"," - If $P$ is equal for all $n$, we have *time-homogeneity*.\n"," - $\\pi, P$ completely determine the distribution of all terms of the chain, $P(x_n = s_j) = \\pi_{nj}, \\: \\pi_n = \\pi_1 P^n$.\n"," - *Stationary distribution*, an initial distribution $\\pi$ such that $\\pi_n = \\pi$ (thus, $\\pi = \\pi P$).\n"," - *Detailed balance* is satisfied iff $\\pi_{bi}P_{ij} = \\pi_{bj}P_{ji}$ for any $i,j \\leq J$.\n"," - A chain is *irreducible* iff\n"," every state $x$ leads to itself and every other state $y$\n"," in finite time $\\tau_{x,y} = \\min \\{n > 1 \\: : \\: X_n = y\\}$.\n"," - A state $y \\in S$ is *recurrent* iff $P_y(\\tau_{yy} < \\infty) = 1$, otherwise *transient*.\n"," - A chain is recurrent iff all states in $S$ are recurrent.\n"," - Period of a state $y$ is the minimum time it takes to return to itself,\n"," $d_y = \\mathrm{gcd} \\{n > 1 \\: : \\: P_x(X_n = x) > 0\\}$.\n"," - If a chain is irreducible, all $d_y$ are equal to some $d$.\n"," - A chain is *aperiodic* iff its period $d = 1$,\n"," - If a Markov chain with a finite state space $S$ is irreducible,\n"," then it has an unique stationary distribution $\\pi$.\n"," - Also: $\\frac{1}{n} \\sum\\limits_{i=1}^N f(X_i) \\rightarrow \\sum\\limits_{j=1}^J \\pi_j f(s_j)$\n"," for any bounded function $f$ (*ergodic theorem*).\n"," - If also aperiodic, $\\lim\\limits_{n \\rightarrow \\infty} \\pi_n = \\pi$ regardless of $\\pi$.\n","\n","\n"," $S$ is infinite and countable:\n"," - A state/chain is *positive recurrent* iff $\\mathrm{E}_y[\\tau_{yy}] < \\infty$.\n"," - If a Markov chain with a finite state space $S$ is irreducible and positive recurrent,\n"," then it has an unique stationary distribution.\n"," - Thus, equivalent ergodic theorem also applies.\n"," - If also aperiodic, then convergence to the stationary distribution.\n","\n","\n"," $S$ is infinite and uncountable: ...\n",""],"metadata":{}},{"source":["  \n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":["\n"," ## Autocorrelation\n","\n"," The autocorrelation coefficient between two terms $X_i, X_j$ of a sequence $\\{X_n\\}$ is\n","\n"," $\\rho(i, j) = \\frac{\\mathrm{cov}[X_i, X_j]}{\\sqrt{\\mathrm{var}[X_i] \\mathrm{var}[X_j]}}$\n","\n"," This is just the linear correlation coefficient between two random variables\n"," of the same sequence.\n","\n"," The sequence $\\{X_n\\}$ is said to be *covariance stationary* (or *weakly stationary*) iff\n","\n"," - all terms have same mean, $\\exists \\; \\mu \\in \\mathbb{R} \\: : \\: \\mathrm{E}[X_n] = \\mu, \\; \\forall \\: n \\in \\mathbb{N}$;\n"," - the covariance between any two terms depends only on their distance, $\\forall \\; j \\geq 0, \\; \\exists \\; \\gamma_j \\in \\mathbb{R} \\; : \\; \\mathrm{cov}[X_n, X_{n-j}] = \\gamma_j, \\; \\forall \\; n > j$.\n","\n","\n"," Corollay:\n","\n"," - all terms have the same variance,\n"," $\\exists \\; \\gamma_0 \\in \\mathbb{R} \\: : \\: \\mathrm{var}[X_n] = \\gamma_0, \\; \\forall \\: n \\in \\mathbb{N}$;\n"," - the autocorrelation coefficient between any two terms depends only on their distance\n"," (i.e. autocorrelation at *lag* $k$),\n"," $\\rho(i, j + k) = \\rho_k = \\frac{\\gamma_k}{\\gamma_0}$.\n","\n","\n"," Sample autocorrelation (given the first $N$ realizations of $\\{X_n\\}$):\n"," $\\hat{\\rho_k} = \\frac{\\frac{1}{N-k} \\sum\\limits_{n=1}^{N-k} (X_n - \\hat{\\mu})(X_{n+k} - \\hat{\\mu})}{\\frac{1}{N} \\sum\\limits_{n=1}^N (X_n - \\hat{\\mu})^2}$\n","\n"," If the sequence is covariance stationary,\n"," then this expression is a consistent estimator of $\\rho_k$.\n","\n"," Treating $k$ as a variable, $\\rho_k$ can be called the *autocorrelation function* (ACF).\n",""],"metadata":{}},{"cell_type":"markdown","source":["\n"," ## Markov Chain Monte Carlo (MCMC)\n","\n"," Monte Carlo methods give approximations of\n"," some feature like the mean of a probability distribution.\n","\n"," In a MCMC method:\n"," - a sample $\\xi_n = [x_1 \\: \\ldots \\: x_n]$ is also generated by computer\n"," - but it is such that the sequence $\\{X_n\\}$ is a Markov chain (i.e. not independent)\n"," converging to the stationary distribution $F_X(x)$.\n"," - A plug-in estimate $T(F_n)$ is made using the empirical distribution $F_n(x)$\n"," (each $x_i$ is assigned a probability of $\\frac{1}{n}$).\n"," - By an ergodic theorem, $T(F_n)$ (e.g. $= \\hat{\\mu}) converges to $T(F_x)$ (e.g. $=\\mu$).\n","\n","\n"," Popular examples:\n","\n"," 1. Metropolis-Hastings algorithm;\n"," 2. Gibbs sampling algorithm.\n",""],"metadata":{}},{"cell_type":"markdown","source":["\n"," ## Bayesian Inference\n","\n"," Recall that in a statistical inference problem:\n"," - observations form a sample in the form of a vector $x$;\n"," - $x$ is a realization of a random vector $X$;\n"," - the probability distribution $F_X$ is unknown;\n"," - define a statistical model, i.e. a set $\\Phi$ of possible $F_X$'s;\n"," - parameterize the model with $\\theta$ (optional);\n"," - use the sample $x$ and model $\\Phi$ to make a statement about $F_X$ or $\\theta$.\n","\n","\n"," Steps:\n"," 1. Define the *likelihood* $p(x|\\theta)$.\n","   - it is the PDF of $x$ when the parameter of $F_X$ is $\\theta$.\n","   - e.g. suppose $x = [x_1 \\; \\ldots \\; x_n]$ where $x_i$ is drawn from a normal distribution\n","     ($\\mu unknown, $\\sigma^2$ known).\n","   - $\\begin{align} p(x_i|\\mu) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right) \\\\ p(x|\\mu) &= \\prod\\limits_{i=1}^n p(x_i|\\mu) \\end{align}$\n"," 2. Define the *prior* $p(\\theta)$.\n","   - it is the subjective PDF assigned to $\\theta$.\n","   - e.g. assuming $\\mu$ is likely to be near some value $\\mu_0$, $p(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2 \\tau^2} \\right)$.\n"," 3. Define the *prior predictive distribution* $p(x)$.\n","   - With the prior and the likelihood, derive the marginal density of $x$.\n","   - Recall that a joint PDF can be written as a product of a conditional PDF and a marginal PDF.\n","   - $p(x) = \\int_\\theta p(x, \\theta) \\mathrm{d}\\theta = \\int_\\theta p(x|\\theta) p(\\theta) \\mathrm{d}\\theta$\n","   - e.g. $p(x) = \\int_{-\\infty}^\\infty p(x|\\mu) p(\\mu) \\mathrm{d}\\mu = \\ldots = (2 \\pi \\sigma^2)^{-n/2} \\left|\\mathrm{det}\\left(I_n + \\frac{\\tau^2}{\\sigma^2}ii^\\mathsf{T}\\right)\\right|\\exp \\left(-\\frac{1}{2\\sigma^2}(x - i\\mu_0)^\\mathsf{T} \\left(I_n + \\frac{\\tau^2}{\\sigma^2}ii^\\mathsf{T}\\right)^{-1}(x - i\\mu_0) \\right)$.\n","   - This is just a multivariate normal distribution with mean $i\\mu_0$ and covarance matrix $\\sigma^2 I_n + \\tau^2 ii^\\mathsf{T}$.\n","   - $i$ is the $n \\times 1$ vector of ones, $I_n$ is the $n \\times n$ identity matrix.\n"," 4. Calculate the *posterior* $p(\\theta|x)$.\n","   - This is the PDF of $\\theta$ conditional on $x$.\n","   - Use Bayes' rule, $p(\\theta|x) = \\frac{p(x|\\theta) p(\\theta)}{p(x)} = \\frac{p(x|\\theta) p(\\theta)}{\\int_\\theta p(x|\\theta) p(\\theta) \\mathrm{d}\\theta}$.\n","   - e.g. $p(\\mu|x) = \\ldots = (2 \\pi \\mu_n)^{-1/2} \\exp \\left(-\\frac{(\\mu - \\mu_n)^2}{2 \\sigma_n^2} \\right)$\n","   - where $\\mu_n = \\sigma_n^2 \\left[ \\frac{n}{\\sigma^2} \\left(\\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\right) + \\frac{\\mu_0}{\\tau^2} \\right]$\n","   - and $\\sigma_n^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1} $\n","   - As expected, when $n \\rightarrow \\infty$, all the weight is given to the information from the sample $x$.\n","\n","\n"," The *posterior predictive distribution*:\n"," - Suppose that a second sample $y$ is drawn from the same distribution as $x$.\n"," - Suppose also that the distribution of $y$ is independent on $x$, $p(y|\\theta,x) = p(y|\\theta)$.\n"," - Given $p(\\theta|x)$, we want to known the probability of $y$ given $x$ (the posterior predictive distribution).\n"," - $\\begin{align} p(y|x) &= \\int_\\theta p(y,\\theta|x) \\mathrm{d}\\theta \\\\ &= \\int_\\theta p(y|\\theta,x) \\; p(\\theta|x) \\mathrm{d}\\theta \\\\ &= \\int_\\theta p(y|\\theta) \\; p(\\theta|x) \\mathrm{d}\\theta \\end{align}$\n"," - e.g. let $y$ = \\{x_{n+1}\\} (a single new drawn),\n"," it can be shown that $p(x_{n+1}|x)$ is a normal distribution\n"," with mean $\\mean_n$ and variance $\\sigma^2 + \\sigma_n^2$.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ### Hierarchical Bayesian Models\n","\n"," *Hierarchical* Bayesian Models are those\n"," wherein the prior distribution of some of the parameters\n"," depends on other parameters.\n","\n"," Given observed data $x$:\n"," - the likelihood depends on two parameter vectors $\\theta, \\phi$, $p(x|\\theta,\\phi)$;\n"," - the prior is $p(\\phi, \\theta) = p(\\theta|\\phi) p(\\phi)$;\n"," - special case: the likelihood is required to be independent of $\\phi$,\n"," $p(x|\\theta,\\phi) \\rightarrow p(x|\\theta)$;\n"," - in such case, $\\phi$ is a *hyper-parameter* and $p(\\phi)$ is a *hyper-prior*;\n","\n","\n"," Example (random means):\n"," - Suppose a sample $x = [x_1 \\; \\ldots \\; x_n]$.\n"," - $x_i$ is drawn from a normal distribution with unknown $\\mu_i$\n"," and known common variance $\\sigma^2$.\n"," - $p(x_i|\\mu_i) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left(-\\frac{(x_i - \\mu_i)}{2 \\sigma^2} \\right)$.\n"," - Denote $\\mu = [\\mu_1 \\; \\ldots \\; \\mu_n]$.\n"," - Assuming the $x_i$'s are independent,\n"," $p(x|\\mu) = \\prod\\limits_{i=1}^n p(x_i|\\mu_i)$.\n"," - Assuming the $\\mu_i$'s are a sample of IID draws from a normal distribution of unknown means $m$ and known common variance $\\tau^2$,\n"," - $p(\\mu|m) = \\prod\\limits_{i=1}^n p(\\mu_i|m)$ = (2 \\pi \\sigma^2)^{-n/2} \\exp \\left(-\\frac{(\\mu_i - m)}{2 \\tau^2} \\right)$.\n"," - Assign a normal prior with known mean $m_0$ and variance $u^2$ to the hyper-parameter $m$,\n"," - $p(m) = (2 \\pi u^2)^{-1/2} \\exp \\left(-\\frac{(m - m_0)}{2 u^2} \\right)$\n"," - Such a model is a hierarchical Bayesian model.\n","\n","\n"," Example (normal-inverse gamma model):\n"," - Suppose now $x_i$ is drawn with unknown variance $\\sigma^2$ as well.\n"," - $p(x_i|\\mu, \\sigma) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left(-\\frac{(x_i - \\mu)}{2 \\sigma^2} \\right)$.\n"," - $p(x|\\mu,\\sigma) = \\prod\\limits_{i=1}^n p(x_i|\\mu, \\sigma)$.\n"," - Assume $\\mu$ is normal with known mean $m$ and variance $\\sigma^2/v$ ($\\nu$ is a known parameter).\n"," - $p(\\mu|\\sigma^2) = (2 \\pi \\frac{\\sigma^2}{\\nu})^{-1/2} \\exp \\left(-\\frac{\\nu (\\mu - m)}{2 \\sigma^2} \\right)$.\n"," - Assign an inverse-gamma (gamma distribution with precision $1/\\sigma^2$)\n"," prior to $\\sigma^2$.\n"," - $p(\\sigma^2) = \\frac{(k/h)^{k/2}}{2^{k/2} \\Gamma(k/2)} (1/sigma^2)^{k/2+1} \\exp(-\\frac{k}{2 h \\sigma^2})$.\n"," - $p(\\sigma^2 | x) = \\frac{p(x|\\mu,\\sigma^2)  \\left( p(\\mu|\\sigma^2) p(\\sigma^2) \\right)}{p(x)}\n","\n","\n"," To compute the posterior distribution:\n","\n"," 1. Conditional on $\\phi$,\n"," - the prior predictive distribution of $x$: $p(x|\\phi) = \\int p(x,\\theta|\\phi) \\mathrm{d}\\phi = \\int p(x|\\theta,\\phi) p(\\theta|\\phi) \\mathrm{d}\\theta$.\n"," - the posterior distribution of $\\theta$: $p(\\theta|x,\\phi) = \\frac{p(x|\\theta,\\phi) \\; p(\\theta|\\phi)}{p(x|\\phi)}$.\n"," 2. Using 1.,\n"," - the prior predictive distribution of $x$: $p(x) = \\int p(x,\\phi) \\mathrm{d}\\phi = \\int p(x|\\phi) p(\\phi) \\mathrm{d}\\phi$.\n"," - the posterior marginal distribution of $\\phi$: $p(\\phi|x) = \\frac{p(x|\\phi) \\; p(\\phi)}{p(x)}$.\n"," 3. The posterior joint distribution of $\\phi, \\theta$:\n"," $p(\\phi, \\theta | x) = p(\\theta|x,\\phi) p(\\phi|x)$.\n"," 4. The posterior marginal distribution of $\\theta$:\n"," $p(\\theta|x) = \\int p(\\phi\\theta|x) \\mathrm{d}\\phi$.\n",""],"metadata":{}},{"source":["  "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" ### Bayesian Estimation of the Parameters of a Normal Distribution\n","\n"," Case: *unknown* mean $\\mu$ and *known* variance $\\sigma^2$.\n"," - Suppose a sample $x = [x_1 \\; \\ldots \\; x_n]$.\n"," - $x_i$ are drawn IID from a normal distribution.\n"," - The likelihood is $p(x_i|\\mu) = N(\\mu, \\sigma^2)$\n"," and $p(x|\\mu) = \\prod\\limits_{i=1}^n p(x_i|\\mu)$.\n"," - The prior is $p(\\mu) = N(\\mu_0, \\tau_0^2)$.\n"," - To calculate the posterior is $p(\\mu|x)$,\n","   1. Write $p(x,\\mu) = p(x|\\mu) p(\\mu) = \\ldots = h(x) g(\\mu,x)$\n","   2. By factorization, $p(x) = h(x)$ and $p(\\mu|x) = g(\\mu, x)$.\n"," - Thus, the posterior $p(\\mu|x)$ is just $N(\\mu_n, \\sigma_n^2)$.\n"," - The prior predictive distribution $p(x)$ is\n"," $N(\\mu_0 i, \\sigma^2 I_n + \\tau_0^2 ii^\\mathsf{T})$.\n"," - The posterior predictive distribution $p(\\tilde{x}|x)$\n"," can be calculated too ($\\tilde{x} = [x_{n+1} \\; \\ldots \\; x_{n+m}]$).\n","\n","\n"," Case: *unknown* mean $\\mu$ and *unknown* variance $\\sigma^2$.\n"," - The prior is hierarchical.\n"," - See example in previous subsection."],"metadata":{}},{"cell_type":"markdown","source":[" ### Bayesian Linear Regression\n","\n"," Recall the normal linear regression model:\n"," - $y = X\\beta + \\epsilon$;\n"," - $y$ is the $N \\times 1$ vector of observations of the dependent variable;\n"," - $X$ is the $N \\times K$ matrix of regressors (full rank);\n"," - $\\beta$ is the $K \\times 1$ vector of regression coefficients;\n"," - $\\epsilon$ is the $N \\times 1$ vector of errors\n"," (multivariate normal distribution conditional on $X$ with mean $0$\n"," and covariance matrix $\\sigma^2 I_N$)\n","\n","\n"," Case: *unknown* $\\beta$ and *known* variance $\\sigma^2$.\n"," - Since $\\epsilon$ is multivariate normal and $y$ is a linear transformation of it,\n"," $y$ is also multivariate normal;\n"," - the likelihood is then $p(y|\\beta,X) = (2 \\pi)^{N/2} |\\mathrm{det}(\\sigma^2 I_N)|^{-1/2} \\exp \\left(-\\frac{1}{2} (y - X\\beta)^\\mathsf{T} (\\sigma^2 I_N)^{-1} (y - X \\beta) \\right)$\n"," - assume the prior on $\\beta$ to be multivariate normal;\n"," - with mean $\\beta_0$ and covariance $\\sigma^2 V_0$, $V_0$ is some $K\\times K$ symmetric positive definite matrix;\n"," - $p(\\beta) = (2 \\pi)^{N/2} |\\mathrm{det}(\\sigma^2 V_0)|^{-1/2} \\exp \\left(-\\frac{1}{2} (\\beta - \\beta_0)^\\mathsf{T} (\\sigma^2 V_0)^{-1} (\\beta - \\beta_0) \\right)$\n"," - Apply factorization to get $p(\\beta|y,X)$...\n"," - $p(\\beta|y,X) = \\ldots =  (2 \\pi)^{K/2} |\\mathrm{det}(\\sigma^2 V_N)|^{-1/2} \\exp \\left(-\\frac{1}{2} (\\beta - \\beta_N)^\\mathsf{T} (\\sigma^2 V_N)^{-1} (\\beta - \\beta_N) \\right)$\n"," - where $V_N = (V_0^{-1} + X^\\mathsf{T} X)^{-1}$;\n"," - where $\\beta_N = V_N [V_0^{-1} \\beta_0 + X^\\mathsf{T} y]$;\n"," - the posterior of $\\beta$ is multivariate normal with mean $\\beta_N$ and covariance $\\sigma^2 V_N$;\n"," - recall from OLS, $\\beta_\\textrm{OLS} = (X^\\mathsf{T} X)^{-1} X^\\mathsf{T} y$\n"," - then, $\\beta_N = \\ldots = V_N [V_0^{-1} \\beta_0 + X^\\mathsf{T} X \\beta_\\textrm{OLS}]$;\n"," - so, the posterior mean of $\\beta$ is the weighted average of\n","   1. the OLS estimate from $X, y$;\n","   2. the prior mean $\\beta_0$;\n"," - since $\\mathrm{var}[\\beta_\\textrm{OLS}] = \\sigma^2 (X^\\mathsf{T}X)^{-1}$\n"," - and $\\mathrm{var}[\\beta] = \\sigma^2 V_0$ by definition,\n"," - then, $V_N = (\\mathrm{var}[\\beta]^{-1} + \\mathrm{var}[\\beta_\\textrm{OLS}]^{-1})^{-1}$\n"," - and $\\beta_N =  (\\mathrm{var}[\\beta]^{-1} + \\mathrm{var}[\\beta_\\textrm{OLS}]^{-1})^{-1} [\\mathrm{var}[\\beta]^{-1} \\beta_0 + \\mathrm{var}[\\beta_\\textrm{OLS}]^{-1} \\beta_\\textrm{OLS}]$;\n"," - the weights of the weighted average are just the inverse variances (i.e. precision)\n"," of each source of information (the prior mean and the OLS estimator);\n"," - since $\\lim\\limits_{N \\rightarrow \\infty} X^\\mathsf{T}X = \\infty$,\n"," $\\beta_\\textrm{OLS}$ sensibly becomes more important;\n"," - therefore, Bayesian regression and frequentist (OLS) regression give the same result as large sample size.\n"," - the prior predictive distribution $p(y|X)$ is also calculated by factorization;\n"," - $p(y|X) = \\ldots$ (multivariate normal with mean $X\\beta_0$ and covariance $\\sigma^2 (X V_0X^\\mathsf{T} + I_N)$);\n"," - the posterior predictive distribution can be calculated for a new sample $(\\tilde{y},\\tilde{X})$ of size $M$;\n"," - i.e. predict $\\tilde{y}$ from $\\tilde{X}$ and previous sample;\n"," - the posterior $p(\\beta|y,X)$ is the new prior;\n"," - same likelihood $p(\\tilde{y}|\\tilde{X},y,X) = p(\\tilde{y}|\\tilde{X},\\beta$;\n"," - factorization, $p(\\beta|\\tilde{X},y,X) \\; p(\\tilde{y}|\\tilde{X},y,X) = p(\\tilde{y}|\\tilde{X},\\beta) \\; p(\\beta|y,X)$;\n"," - result: $\\tilde{y}$ is multivariate normal with mean $\\tilde{X}\\beta_N$ and covariance $\\tilde{X}V_N\\tilde{X}^\\mathsf{T} + I_M$;\n","\n","\n"," Case: *unknown* $\\beta$ and *unknown* variance $\\sigma^2$.\n"," - Similar as before but hierarchical...\n",""],"metadata":{}},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}